{"cli/docs/seldon_server_status": {"sections": {"seldon-server-status": "\nseldon server status\u00b6\nget status for server\n\nSynopsis\u00b6\nget the status for a server\nseldon server status [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon server\t - manage servers\n\n\n", "synopsis": "\nSynopsis\u00b6\nget the status for a server\nseldon server status [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon server\t - manage servers\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_server_status.html", "key": "cli/docs/seldon_server_status"}}, "cli/docs/seldon_server": {"sections": {"seldon-server": "\nseldon server\u00b6\nmanage servers\n\nSynopsis\u00b6\nget status for servers\nseldon server <subcomand> [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for server\n\n\n\n\nSEE ALSO\u00b6\n\nseldon\t -\nseldon server list\t - get list of servers\nseldon server status\t - get status for server\n\n\n", "synopsis": "\nSynopsis\u00b6\nget status for servers\nseldon server <subcomand> [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for server\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon\t -\nseldon server list\t - get list of servers\nseldon server status\t - get status for server\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_server.html", "key": "cli/docs/seldon_server"}}, "metrics/usage": {"sections": {"usage-metrics": "\nUsage Metrics\u00b6\nThere are various interesting system metrics about how Seldon Core v2 is used.\nThese metrics can be recorded anonymously and sent to Seldon by a lightweight, optional, stand-alone component called Hodometer.\nWhen provided, these metrics will be used to understand the adoption of Seldon Core v2 and how people interact with it.\nFor example, knowing how many clusters Seldon Core v2 is running on, if it is used in Kubernetes or for local development, and how many people are benefitting from features like multi-model serving.\n\nArchitecture\u00b6\n\nHodometer is not an integral part of Seldon Core v2, but rather an independent component which connects to the public APIs of the Seldon Core v2 scheduler.\nIf deployed in Kubernetes, it will also try to request some basic information from the Kubernetes API.\nRecorded metrics are sent to Seldon and, optionally, to any additional endpoints you define.\n\n\nPrivacy\u00b6\nHodometer was explicitly designed with privacy of user information and transparency of implementation in mind.\nIt does not record any sensitive or identifying information.\nFor example, it has no knowledge of IP addresses, model names, or user information.\nAll information sent to Seldon is anonymised with a completely random cluster identifier.\nHodometer supports different information levels, so you have full control over what metrics are provided to Seldon, if any.\nFor transparency, the implementation is fully open-source and designed to be easy to read.\nThe full source code is available here, with metrics defined in code here.\nSee below for an equivalent table of metrics.\n\n\nPerformance\u00b6\nMetrics are collected as periodic snapshots a few times per day.\nThey are lightweight to collect, coming mostly from the Seldon Core v2 scheduler, and are heavily aggregated.\nAs such, they should have minimal impact on CPU, memory, and network consumption.\nHodometer does not store anything it records, so does not have any persistent storage.\nAs a result, it should not be considered a replacement for tools like Prometheus.\n\n\nConfiguration\u00b6\n\nMetrics levels\u00b6\nHodometer supports 3 different metrics levels:\n\n\nLevel\nDescription\n\n\n\nCluster\nBasic information about the Seldon Core v2 installation\n\nResource\nHigh-level information about which Seldon Core v2 resources are used\n\nFeature\nMore detailed information about how resources are used and whether or not certain feature flags are enabled\n\n\n\nAlternatively, usage metrics can be completely disabled.\nTo do so, simply remove any existing deployment of Hodometer or disable it in the installation for your environment, discussed below.\n\n\nOptions\u00b6\nThe following environment variables control the behaviour of Hodometer, regardless of the environment it is installed in.\n\n\nFlag\nFormat\nExample\nDescription\n\n\n\nMETRICS_LEVEL\nstring\nfeature\nLevel of detail for recorded metrics; one of feature, resource, or cluster\n\nEXTRA_PUBLISH_URLS\ncomma-separated list of URLs\nhttp://my-endpoint-1:8000,http://my-endpoint-2:8000\nAdditional endpoints to publish metrics to\n\nSCHEDULER_HOST\nstring\nseldon-scheduler\nHostname for Seldon Core v2 scheduler\n\nSCHEDULER_PORT\ninteger\n9004\nPort for Seldon Core v2 scheduler\n\nLOG_LEVEL\nstring\ninfo\nLevel of detail for application logs\n\n\n\n\n\nKubernetes\u00b6\nHodometer is installed as a separate deployment, by default in the same namespace as the rest of the Seldon components.\n\nHelmIf you install Seldon Core v2 by Helm chart, there are values corresponding to the key environment variables discussed above.\nThese Helm values and their equivalents are provided below:\n\n\nHelm value\nEnvironment variable\n\n\n\nhodometer.metricsLevel\nMETRICS_LEVEL\n\nhodometer.extraPublishUrls\nEXTRA_PUBLISH_URLS\n\nhodometer.logLevel\nLOG_LEVEL\n\n\n\nIf you do not want usage metrics to be recorded, you can disable Hodometer via the hodometer.disable Helm value when installing the runtime Helm chart.\nThe following command disables collection of usage metrics in fresh installations and also serves to remove Hodometer from an existing installation:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n  --namespace seldon-mesh \\\n  --set hodometer.disable=true\n\n\n\nNote\nIt is a good practice to set Helm values in values file.\nThese can be applied by using the -f <filename> switch when running Helm.\n\n\n\n\nDocker Compose\u00b6\nThe Compose setup provides a pre-configured and opinionated, yet still flexible, approach to using Seldon Core v2.\nHodometer is defined as a service called hodometer in the Docker Compose manifest.\nIt is automatically enabled when running as per the installation instructions.\nYou can disable Hodometer in Docker Compose by removing the corresponding service from the base manifest.\nAlternatively, you can gate it behind a profile.\nIf the service is already running, you can stop it directly using docker-compose stop ....\nConfiguration can be provided by environment variables when running make or directly invoking docker-compose.\nThe available variables are defined in the Docker Compose environment file, prefixed with HODOMETER_.\n\n\nExtra publish URLs\u00b6\nHodometer can be instructed to publish metrics not only to Seldon, but also to any extra endpoints you specify.\nThis is controlled by the EXTRA_PUBLISH_URLS environment variable, which expects a comma-separated list of HTTP-compatible URLs.\nYou might choose to use this for your own usage monitoring.\nFor example, you could capture these metrics and expose them to Prometheus or another monitoring system using your own service.\nMetrics are recorded in MixPanel-compatible format, which employs a highly flexible JSON schema.\nFor an example of how to define your own metrics listener, see the receiver Go package in the hodometer sub-project.\n\n\n\nList of metrics\u00b6\n\n\nMetric name\nLevel\nFormat\nNotes\n\n\n\ncluster_id\ncluster\nUUID\nA random identifier for this cluster for de-duplication\n\nseldon_core_version\ncluster\nVersion number\nE.g. 1.2.3\n\nis_global_installation\ncluster\nBoolean\nWhether installation is global or namespaced\n\nis_kubernetes\ncluster\nBoolean\nWhether or not the installation is in Kubernetes\n\nkubernetes_version\ncluster\nVersion number\nKubernetes server version, if inside Kubernetes\n\nnode_count\ncluster\nInteger\nNumber of nodes in the cluster, if inside Kubernetes\n\nmodel_count\nresource\nInteger\nNumber of Model resources\n\npipeline_count\nresource\nInteger\nNumber of Pipeline resources\n\nexperiment_count\nresource\nInteger\nNumber of Experiment resources\n\nserver_count\nresource\nInteger\nNumber of Server resources\n\nserver_replica_count\nresource\nInteger\nTotal number of Server resource replicas\n\nmultimodel_enabled_count\nfeature\nInteger\nNumber of Server resources with multi-model serving enabled\n\novercommit_enabled_count\nfeature\nInteger\nNumber of Server resources with overcommitting enabled\n\ngpu_enabled_count\nfeature\nInteger\nNumber of Server resources with GPUs attached\n\ninference_server_name\nfeature\nString\nName of inference server, e.g. MLServer or Triton\n\nserver_cpu_cores_sum\nfeature\nFloat\nTotal of CPU limits across all Server resource replicas, in cores\n\nserver_memory_gb_sum\nfeature\nFloat\nTotal of memory limits across all Server resource replicas, in GiB\n\n\n\n\n", "architecture": "\nArchitecture\u00b6\n\nHodometer is not an integral part of Seldon Core v2, but rather an independent component which connects to the public APIs of the Seldon Core v2 scheduler.\nIf deployed in Kubernetes, it will also try to request some basic information from the Kubernetes API.\nRecorded metrics are sent to Seldon and, optionally, to any additional endpoints you define.\n", "privacy": "\nPrivacy\u00b6\nHodometer was explicitly designed with privacy of user information and transparency of implementation in mind.\nIt does not record any sensitive or identifying information.\nFor example, it has no knowledge of IP addresses, model names, or user information.\nAll information sent to Seldon is anonymised with a completely random cluster identifier.\nHodometer supports different information levels, so you have full control over what metrics are provided to Seldon, if any.\nFor transparency, the implementation is fully open-source and designed to be easy to read.\nThe full source code is available here, with metrics defined in code here.\nSee below for an equivalent table of metrics.\n", "performance": "\nPerformance\u00b6\nMetrics are collected as periodic snapshots a few times per day.\nThey are lightweight to collect, coming mostly from the Seldon Core v2 scheduler, and are heavily aggregated.\nAs such, they should have minimal impact on CPU, memory, and network consumption.\nHodometer does not store anything it records, so does not have any persistent storage.\nAs a result, it should not be considered a replacement for tools like Prometheus.\n", "configuration": "\nConfiguration\u00b6\n\nMetrics levels\u00b6\nHodometer supports 3 different metrics levels:\n\n\nLevel\nDescription\n\n\n\nCluster\nBasic information about the Seldon Core v2 installation\n\nResource\nHigh-level information about which Seldon Core v2 resources are used\n\nFeature\nMore detailed information about how resources are used and whether or not certain feature flags are enabled\n\n\n\nAlternatively, usage metrics can be completely disabled.\nTo do so, simply remove any existing deployment of Hodometer or disable it in the installation for your environment, discussed below.\n\n\nOptions\u00b6\nThe following environment variables control the behaviour of Hodometer, regardless of the environment it is installed in.\n\n\nFlag\nFormat\nExample\nDescription\n\n\n\nMETRICS_LEVEL\nstring\nfeature\nLevel of detail for recorded metrics; one of feature, resource, or cluster\n\nEXTRA_PUBLISH_URLS\ncomma-separated list of URLs\nhttp://my-endpoint-1:8000,http://my-endpoint-2:8000\nAdditional endpoints to publish metrics to\n\nSCHEDULER_HOST\nstring\nseldon-scheduler\nHostname for Seldon Core v2 scheduler\n\nSCHEDULER_PORT\ninteger\n9004\nPort for Seldon Core v2 scheduler\n\nLOG_LEVEL\nstring\ninfo\nLevel of detail for application logs\n\n\n\n\n\nKubernetes\u00b6\nHodometer is installed as a separate deployment, by default in the same namespace as the rest of the Seldon components.\n\nHelmIf you install Seldon Core v2 by Helm chart, there are values corresponding to the key environment variables discussed above.\nThese Helm values and their equivalents are provided below:\n\n\nHelm value\nEnvironment variable\n\n\n\nhodometer.metricsLevel\nMETRICS_LEVEL\n\nhodometer.extraPublishUrls\nEXTRA_PUBLISH_URLS\n\nhodometer.logLevel\nLOG_LEVEL\n\n\n\nIf you do not want usage metrics to be recorded, you can disable Hodometer via the hodometer.disable Helm value when installing the runtime Helm chart.\nThe following command disables collection of usage metrics in fresh installations and also serves to remove Hodometer from an existing installation:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n  --namespace seldon-mesh \\\n  --set hodometer.disable=true\n\n\n\nNote\nIt is a good practice to set Helm values in values file.\nThese can be applied by using the -f <filename> switch when running Helm.\n\n\n\n\nDocker Compose\u00b6\nThe Compose setup provides a pre-configured and opinionated, yet still flexible, approach to using Seldon Core v2.\nHodometer is defined as a service called hodometer in the Docker Compose manifest.\nIt is automatically enabled when running as per the installation instructions.\nYou can disable Hodometer in Docker Compose by removing the corresponding service from the base manifest.\nAlternatively, you can gate it behind a profile.\nIf the service is already running, you can stop it directly using docker-compose stop ....\nConfiguration can be provided by environment variables when running make or directly invoking docker-compose.\nThe available variables are defined in the Docker Compose environment file, prefixed with HODOMETER_.\n\n\nExtra publish URLs\u00b6\nHodometer can be instructed to publish metrics not only to Seldon, but also to any extra endpoints you specify.\nThis is controlled by the EXTRA_PUBLISH_URLS environment variable, which expects a comma-separated list of HTTP-compatible URLs.\nYou might choose to use this for your own usage monitoring.\nFor example, you could capture these metrics and expose them to Prometheus or another monitoring system using your own service.\nMetrics are recorded in MixPanel-compatible format, which employs a highly flexible JSON schema.\nFor an example of how to define your own metrics listener, see the receiver Go package in the hodometer sub-project.\n\n", "metrics-levels": "\nMetrics levels\u00b6\nHodometer supports 3 different metrics levels:\n\n\nLevel\nDescription\n\n\n\nCluster\nBasic information about the Seldon Core v2 installation\n\nResource\nHigh-level information about which Seldon Core v2 resources are used\n\nFeature\nMore detailed information about how resources are used and whether or not certain feature flags are enabled\n\n\n\nAlternatively, usage metrics can be completely disabled.\nTo do so, simply remove any existing deployment of Hodometer or disable it in the installation for your environment, discussed below.\n", "options": "\nOptions\u00b6\nThe following environment variables control the behaviour of Hodometer, regardless of the environment it is installed in.\n\n\nFlag\nFormat\nExample\nDescription\n\n\n\nMETRICS_LEVEL\nstring\nfeature\nLevel of detail for recorded metrics; one of feature, resource, or cluster\n\nEXTRA_PUBLISH_URLS\ncomma-separated list of URLs\nhttp://my-endpoint-1:8000,http://my-endpoint-2:8000\nAdditional endpoints to publish metrics to\n\nSCHEDULER_HOST\nstring\nseldon-scheduler\nHostname for Seldon Core v2 scheduler\n\nSCHEDULER_PORT\ninteger\n9004\nPort for Seldon Core v2 scheduler\n\nLOG_LEVEL\nstring\ninfo\nLevel of detail for application logs\n\n\n\n", "kubernetes": "\nKubernetes\u00b6\nHodometer is installed as a separate deployment, by default in the same namespace as the rest of the Seldon components.\n\nHelmIf you install Seldon Core v2 by Helm chart, there are values corresponding to the key environment variables discussed above.\nThese Helm values and their equivalents are provided below:\n\n\nHelm value\nEnvironment variable\n\n\n\nhodometer.metricsLevel\nMETRICS_LEVEL\n\nhodometer.extraPublishUrls\nEXTRA_PUBLISH_URLS\n\nhodometer.logLevel\nLOG_LEVEL\n\n\n\nIf you do not want usage metrics to be recorded, you can disable Hodometer via the hodometer.disable Helm value when installing the runtime Helm chart.\nThe following command disables collection of usage metrics in fresh installations and also serves to remove Hodometer from an existing installation:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n  --namespace seldon-mesh \\\n  --set hodometer.disable=true\n\n\n\nNote\nIt is a good practice to set Helm values in values file.\nThese can be applied by using the -f <filename> switch when running Helm.\n\n\n", "docker-compose": "\nDocker Compose\u00b6\nThe Compose setup provides a pre-configured and opinionated, yet still flexible, approach to using Seldon Core v2.\nHodometer is defined as a service called hodometer in the Docker Compose manifest.\nIt is automatically enabled when running as per the installation instructions.\nYou can disable Hodometer in Docker Compose by removing the corresponding service from the base manifest.\nAlternatively, you can gate it behind a profile.\nIf the service is already running, you can stop it directly using docker-compose stop ....\nConfiguration can be provided by environment variables when running make or directly invoking docker-compose.\nThe available variables are defined in the Docker Compose environment file, prefixed with HODOMETER_.\n", "extra-publish-urls": "\nExtra publish URLs\u00b6\nHodometer can be instructed to publish metrics not only to Seldon, but also to any extra endpoints you specify.\nThis is controlled by the EXTRA_PUBLISH_URLS environment variable, which expects a comma-separated list of HTTP-compatible URLs.\nYou might choose to use this for your own usage monitoring.\nFor example, you could capture these metrics and expose them to Prometheus or another monitoring system using your own service.\nMetrics are recorded in MixPanel-compatible format, which employs a highly flexible JSON schema.\nFor an example of how to define your own metrics listener, see the receiver Go package in the hodometer sub-project.\n", "list-of-metrics": "\nList of metrics\u00b6\n\n\nMetric name\nLevel\nFormat\nNotes\n\n\n\ncluster_id\ncluster\nUUID\nA random identifier for this cluster for de-duplication\n\nseldon_core_version\ncluster\nVersion number\nE.g. 1.2.3\n\nis_global_installation\ncluster\nBoolean\nWhether installation is global or namespaced\n\nis_kubernetes\ncluster\nBoolean\nWhether or not the installation is in Kubernetes\n\nkubernetes_version\ncluster\nVersion number\nKubernetes server version, if inside Kubernetes\n\nnode_count\ncluster\nInteger\nNumber of nodes in the cluster, if inside Kubernetes\n\nmodel_count\nresource\nInteger\nNumber of Model resources\n\npipeline_count\nresource\nInteger\nNumber of Pipeline resources\n\nexperiment_count\nresource\nInteger\nNumber of Experiment resources\n\nserver_count\nresource\nInteger\nNumber of Server resources\n\nserver_replica_count\nresource\nInteger\nTotal number of Server resource replicas\n\nmultimodel_enabled_count\nfeature\nInteger\nNumber of Server resources with multi-model serving enabled\n\novercommit_enabled_count\nfeature\nInteger\nNumber of Server resources with overcommitting enabled\n\ngpu_enabled_count\nfeature\nInteger\nNumber of Server resources with GPUs attached\n\ninference_server_name\nfeature\nString\nName of inference server, e.g. MLServer or Triton\n\nserver_cpu_cores_sum\nfeature\nFloat\nTotal of CPU limits across all Server resource replicas, in cores\n\nserver_memory_gb_sum\nfeature\nFloat\nTotal of memory limits across all Server resource replicas, in GiB\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/metrics/usage.html", "key": "metrics/usage"}}, "examples/cifar10": {"sections": {"production-image-classifier": "\nProduction Image Classifier\u00b6\nRun these examples from the samples/examples/image_classifier folder.\n\nCIFAR10 Image Classification Production Deployment\u00b6\n\nWe show an image classifier (CIFAR10) with associated outlier and drift detectors using a Pipeline.\n\nThe model is a tensorflow CIFAR10 image classfier\nThe outlier detector is created from the CIFAR10 VAE Outlier example.\nThe drift detector is created from the CIFAR10 KS Drift example\n\n\nModel Training (optional for notebook)\u00b6\nTo run local training run the training notebook.\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-06-30 15:39:28.732453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-30 15:39:28.732465: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\noutliers = []\nfor idx in range(0,X_train.shape[0]):\n    X_mask, mask = apply_mask(X_train[idx].reshape(1, 32, 32, 3),\n                                  mask_size=(14,14),\n                                  n_masks=1,\n                                  channels=[0,1,2],\n                                  mask_type='normal',\n                                  noise_distr=(0,1),\n                                  clip_rng=(0,1))\n    outliers.append(X_mask)\nX_outliers = np.vstack(outliers)\nX_outliers.shape\n\n\n(50000, 32, 32, 3)\n\n\n\ncorruption = ['brightness']\nX_corr, y_corr = fetch_cifar10c(corruption=corruption, severity=5, return_X_y=True)\nX_corr = X_corr.astype('float32') / 255\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\n\ndef infer(resourceName: str, batchSz: int, requestType: str):\n    if requestType == \"outlier\":\n        rows = X_outliers[0:0+batchSz]\n    elif requestType == \"drift\":\n        rows = X_corr[0:0+batchSz]\n    else:\n        rows = X_train[0:0+batchSz]\n    for i in range(batchSz):\n        show(rows[i])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [batchSz, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    print(response_raw)\n    print(response_raw.json())\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\nPipeline\u00b6\ncat ../../models/cifar10.yaml\necho \"---\"\ncat ../../models/cifar10-outlier-detect.yaml\necho \"---\"\ncat ../../models/cifar10-drift-detect.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/triton/tf_cifar10\"\n  requirements:\n  - tensorflow\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/cifar10.yaml\nseldon model load -f ../../models/cifar10-outlier-detect.yaml\nseldon model load -f ../../models/cifar10-drift-detect.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq .\nseldon model status cifar10-outlier -w ModelAvailable | jq .\nseldon model status cifar10-drift -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n\n\ncat ../../pipelines/cifar10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: cifar10-production\nspec:\n  steps:\n    - name: cifar10\n    - name: cifar10-outlier\n    - name: cifar10-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - cifar10\n    - cifar10-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/cifar10.yaml\n\n\nseldon pipeline status cifar10-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"cifar10-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"cifar10-production\",\n        \"uid\": \"cifeii2ufmbc73e5insg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"cifar10\"\n          },\n          {\n            \"name\": \"cifar10-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"cifar10-outlier\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"cifar10.outputs\",\n            \"cifar10-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:40:09.047429817Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ninfer(\"cifar10-production.pipeline\",20, \"normal\")\n\n\n![png](infer_files/infer_14_0.png)\n\n\n\n![png](infer_files/infer_14_1.png)\n\n\n\n![png](infer_files/infer_14_2.png)\n\n\n\n![png](infer_files/infer_14_3.png)\n\n\n\n![png](infer_files/infer_14_4.png)\n\n\n\n![png](infer_files/infer_14_5.png)\n\n\n\n![png](infer_files/infer_14_6.png)\n\n\n\n![png](infer_files/infer_14_7.png)\n\n\n\n![png](infer_files/infer_14_8.png)\n\n\n\n![png](infer_files/infer_14_9.png)\n\n\n\n![png](infer_files/infer_14_10.png)\n\n\n\n![png](infer_files/infer_14_11.png)\n\n\n\n![png](infer_files/infer_14_12.png)\n\n\n\n![png](infer_files/infer_14_13.png)\n\n\n\n![png](infer_files/infer_14_14.png)\n\n\n\n![png](infer_files/infer_14_15.png)\n\n\n\n![png](infer_files/infer_14_16.png)\n\n\n\n![png](infer_files/infer_14_17.png)\n\n\n\n![png](infer_files/infer_14_18.png)\n\n\n\n![png](infer_files/infer_14_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.437921e-08, 4.7317337e-09, 1.26449e-06, 4.8814868e-09, 1.5153439e-09, 8.490656e-09, 5.5131194e-10, 1.1617216e-09, 5.7729294e-10, 2.8839776e-07, 0.0006149016, 0.99938357, 0.888746, 2.5331951e-06, 0.00012967695, 0.10531583, 2.4284174e-05, 6.3332986e-06, 0.0016261435, 1.13079e-05, 0.0013286703, 0.0028091935, 2.0993439e-06, 3.680449e-08, 0.0013269952, 2.1766558e-05, 0.99841356, 0.00015300694, 6.9472035e-06, 1.3277059e-05, 6.1860555e-05, 3.4072806e-07, 1.1205097e-05, 0.99997175, 1.9948227e-07, 6.9880834e-08, 3.3387135e-08, 5.2603138e-08, 3.0352305e-07, 4.3738982e-08, 5.3243946e-07, 1.5870584e-05, 0.0006525102, 0.013322109, 1.480307e-06, 0.9766325, 4.9847167e-05, 0.00058075984, 0.008405659, 5.2234273e-06, 0.00023390084, 0.000116047224, 1.6682397e-06, 5.7737526e-10, 0.9975605, 6.45564e-05, 0.002371972, 1.0392675e-07, 9.747962e-08, 1.4484569e-07, 8.762438e-07, 2.4758325e-08, 5.028761e-09, 6.856381e-11, 5.9932094e-12, 4.921233e-10, 1.471166e-07, 2.7940719e-06, 3.4563383e-09, 0.99999714, 5.9420524e-10, 9.445026e-11, 4.1854888e-05, 5.041549e-08, 8.0302314e-08, 1.2119854e-07, 6.781646e-09, 1.2616152e-08, 1.1878505e-08, 1.628573e-09, 0.9999578, 3.281738e-08, 0.08930307, 1.4065135e-07, 4.1117343e-07, 0.90898305, 8.933351e-07, 0.0015637449, 0.00013868928, 9.092981e-06, 4.8759745e-07, 4.3976044e-07, 0.00016094849, 3.5653954e-07, 0.0760521, 0.8927447, 0.0011777573, 0.00265573, 0.027189083, 4.1892267e-06, 1.329405e-05, 1.8564688e-06, 1.3373891e-06, 1.0251247e-07, 8.651912e-09, 4.458202e-06, 1.4646349e-05, 1.260957e-06, 1.046087e-08, 0.9998946, 8.332438e-05, 3.900894e-07, 6.53852e-05, 3.012202e-08, 1.0247197e-07, 1.8824371e-06, 0.0004958526, 3.533475e-05, 2.739997e-07, 0.99939275, 4.840305e-06, 3.5346695e-06, 0.0005518078, 3.1597017e-07, 0.99902296, 0.00031509742, 8.07886e-07, 1.6366084e-06, 2.795575e-06, 6.112367e-06, 9.817249e-05, 2.602709e-07, 0.0004561966, 5.360607e-06, 2.8656412e-05, 0.000116040654, 6.881144e-05, 8.844774e-06, 4.4655946e-05, 3.5564542e-05, 0.006564381, 0.9926715, 0.007300911, 1.766928e-06, 3.0520596e-07, 0.026906287, 1.3769699e-06, 0.00027539674, 5.583593e-06, 3.792553e-06, 0.0003876767, 0.9651169, 0.18114138, 2.8360228e-05, 0.00019927241, 0.007685872, 0.00014663498, 3.9361137e-05, 5.941682e-05, 7.36174e-05, 0.79936546, 0.01126067, 2.3992783e-11, 7.6336457e-16, 1.4644799e-15, 1, 2.4652159e-14, 1.1786078e-10, 1.9402116e-13, 4.2408636e-15, 1.209294e-15, 2.9042784e-15, 1.5366902e-08, 1.2476195e-09, 1.3560152e-07, 0.999997, 4.3113017e-11, 2.8163534e-08, 2.4494727e-06, 1.3122828e-10, 3.8081083e-07, 2.1628158e-11, 0.0004926238, 6.9424555e-06, 2.827196e-05, 0.92534137, 9.500486e-06, 0.00036133997, 0.072713904, 1.2831057e-07, 0.0010457055, 2.8514464e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeij8fh5ss738i5bp0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",20, \"drift\")\n\n\n![png](infer_files/infer_16_0.png)\n\n\n\n![png](infer_files/infer_16_1.png)\n\n\n\n![png](infer_files/infer_16_2.png)\n\n\n\n![png](infer_files/infer_16_3.png)\n\n\n\n![png](infer_files/infer_16_4.png)\n\n\n\n![png](infer_files/infer_16_5.png)\n\n\n\n![png](infer_files/infer_16_6.png)\n\n\n\n![png](infer_files/infer_16_7.png)\n\n\n\n![png](infer_files/infer_16_8.png)\n\n\n\n![png](infer_files/infer_16_9.png)\n\n\n\n![png](infer_files/infer_16_10.png)\n\n\n\n![png](infer_files/infer_16_11.png)\n\n\n\n![png](infer_files/infer_16_12.png)\n\n\n\n![png](infer_files/infer_16_13.png)\n\n\n\n![png](infer_files/infer_16_14.png)\n\n\n\n![png](infer_files/infer_16_15.png)\n\n\n\n![png](infer_files/infer_16_16.png)\n\n\n\n![png](infer_files/infer_16_17.png)\n\n\n\n![png](infer_files/infer_16_18.png)\n\n\n\n![png](infer_files/infer_16_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [8.080701e-09, 2.3025173e-12, 2.2681688e-09, 1, 4.1828953e-11, 4.48467e-09, 3.216822e-08, 2.8404365e-13, 5.217064e-09, 3.3497323e-13, 0.96965235, 4.7030144e-06, 1.6964266e-07, 1.7355454e-05, 2.6667e-06, 1.9505828e-06, 1.1363079e-07, 3.3352034e-08, 0.030320557, 1.7086056e-07, 0.03725602, 6.8623276e-06, 7.5557014e-05, 0.00018132397, 2.2838503e-05, 0.000110639296, 2.3732607e-06, 2.1210687e-06, 0.9623351, 7.131072e-06, 0.999079, 4.207448e-09, 1.5788535e-08, 2.723756e-08, 2.6555508e-11, 2.1526697e-10, 2.7599315e-10, 2.0737433e-10, 0.0009210062, 3.0885383e-09, 6.665241e-07, 1.7765576e-09, 1.4911559e-07, 0.9765331, 1.9476123e-07, 2.8244015e-06, 0.023463126, 5.8030287e-09, 3.243206e-09, 1.12179785e-08, 4.4123663e-06, 4.7628927e-09, 1.1727273e-08, 0.9761534, 1.1409252e-08, 8.922882e-05, 0.023752932, 3.1563903e-08, 2.7916305e-09, 8.7746266e-10, 1.0166265e-05, 0.999703, 4.5408615e-05, 0.00022673907, 1.7365853e-07, 1.0147362e-06, 6.253448e-06, 2.9711526e-07, 7.811687e-07, 6.183683e-06, 0.86618125, 5.47548e-07, 0.00038408802, 0.013155022, 3.6916779e-06, 0.0006137024, 0.11965008, 3.6425424e-06, 6.7638084e-06, 1.2372367e-06, 1.9545263e-05, 1.1281859e-13, 1.6811868e-14, 0.9999777, 1.9805435e-11, 2.7563674e-06, 2.9651657e-09, 1.1363432e-12, 2.9902746e-13, 1.220973e-12, 2.9895918e-05, 3.4964305e-07, 1.1331837e-08, 1.7012125e-06, 3.6088227e-07, 3.035954e-08, 2.2102333e-06, 1.7414077e-08, 0.9999455, 1.9921794e-05, 0.9999999, 5.3446598e-11, 6.3188843e-10, 1.0956511e-07, 1.1538642e-10, 8.113561e-10, 4.7179572e-08, 1.4544753e-11, 5.490219e-08, 1.3347151e-10, 1.5363307e-07, 6.604881e-09, 2.424105e-10, 9.963063e-09, 3.9349533e-09, 1.5709017e-09, 7.705774e-10, 4.8085802e-08, 1.8885139e-05, 0.9999809, 7.147243e-08, 3.143131e-13, 2.1447092e-13, 0.00042652222, 6.945973e-12, 0.9995734, 6.174434e-09, 4.1128205e-11, 3.4031404e-13, 8.573159e-15, 1.2226405e-09, 2.3768018e-10, 2.822187e-07, 8.016278e-08, 4.0692296e-08, 6.8023346e-06, 2.3926754e-07, 0.9999925, 6.652648e-09, 7.743497e-09, 7.6360675e-06, 5.9386625e-09, 1.5675019e-09, 2.136716e-07, 1.3074002e-06, 3.700079e-10, 1.0984521e-09, 6.2138824e-08, 0.9609078, 0.03908287, 0.0008332255, 7.696685e-08, 2.4428939e-09, 7.186676e-05, 1.4520063e-09, 1.4521317e-08, 1.09093e-06, 1.2531165e-10, 0.9990938, 5.798501e-09, 5.785368e-05, 3.82365e-09, 7.404351e-08, 0.008338481, 8.048078e-10, 0.99157715, 1.1663455e-05, 1.4583546e-05, 8.3543476e-08, 3.274394e-08, 2.4682688e-05, 1.3951502e-09, 1.0260489e-08, 0.9998845, 1.9418138e-08, 8.667954e-07, 2.1851054e-07, 8.917964e-05, 4.4437223e-07, 1.1292918e-07, 4.5302792e-07, 5.631744e-08, 2.9086214e-08, 3.1013877e-07, 7.695681e-09, 2.1452344e-09, 1.1493902e-08, 6.1980093e-10, 0.99999917, 1.1436694e-08, 2.42685e-05, 8.557389e-08, 0.024081504, 0.0073837163, 4.8152968e-05, 5.128531e-07, 0.9684405, 9.630179e-08, 2.1060101e-05, 1.901065e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeimgfh5ss738i5bpg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"outlier\")\n\n\n![png](infer_files/infer_18_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [6.3606867e-06, 0.0006106364, 0.0054279356, 0.6536454, 1.4738829e-05, 2.6104701e-06, 0.3397848, 1.3538776e-05, 0.0004458526, 4.807229e-05], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [1], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"ok\")\n\n\n![png](infer_files/infer_19_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.43792e-08, 4.7317337e-09], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [0], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nUse the seldon CLI to look at the outputs from the CIFAR10 model. It will decode the Triton binary outputs for us.\nseldon pipeline inspect cifar10-production.cifar10.outputs\n\n\nseldon.default.model.cifar10.outputs\tcifeiq8fh5ss738i5bqg\t{\"modelName\":\"cifar10_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"fc10\", \"datatype\":\"FP32\", \"shape\":[\"1\", \"10\"], \"contents\":{\"fp32Contents\":[1.45001495e-8, 1.2525752e-9, 1.6298458e-7, 0.11529388, 1.7431412e-7, 0.0000061856604, 0.8846994, 6.0739285e-9, 7.43792e-8, 4.7317337e-9]}}]}\n\n\n\nseldon pipeline unload cifar10-production\n\n\nseldon model unload cifar10\nseldon model unload cifar10-outlier\nseldon model unload cifar10-drift\n\n\n\n\n\n\n\n", "cifar10-image-classification-production-deployment": "\nCIFAR10 Image Classification Production Deployment\u00b6\n\nWe show an image classifier (CIFAR10) with associated outlier and drift detectors using a Pipeline.\n\nThe model is a tensorflow CIFAR10 image classfier\nThe outlier detector is created from the CIFAR10 VAE Outlier example.\nThe drift detector is created from the CIFAR10 KS Drift example\n\n\nModel Training (optional for notebook)\u00b6\nTo run local training run the training notebook.\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-06-30 15:39:28.732453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-30 15:39:28.732465: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\noutliers = []\nfor idx in range(0,X_train.shape[0]):\n    X_mask, mask = apply_mask(X_train[idx].reshape(1, 32, 32, 3),\n                                  mask_size=(14,14),\n                                  n_masks=1,\n                                  channels=[0,1,2],\n                                  mask_type='normal',\n                                  noise_distr=(0,1),\n                                  clip_rng=(0,1))\n    outliers.append(X_mask)\nX_outliers = np.vstack(outliers)\nX_outliers.shape\n\n\n(50000, 32, 32, 3)\n\n\n\ncorruption = ['brightness']\nX_corr, y_corr = fetch_cifar10c(corruption=corruption, severity=5, return_X_y=True)\nX_corr = X_corr.astype('float32') / 255\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\n\ndef infer(resourceName: str, batchSz: int, requestType: str):\n    if requestType == \"outlier\":\n        rows = X_outliers[0:0+batchSz]\n    elif requestType == \"drift\":\n        rows = X_corr[0:0+batchSz]\n    else:\n        rows = X_train[0:0+batchSz]\n    for i in range(batchSz):\n        show(rows[i])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [batchSz, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    print(response_raw)\n    print(response_raw.json())\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\nPipeline\u00b6\ncat ../../models/cifar10.yaml\necho \"---\"\ncat ../../models/cifar10-outlier-detect.yaml\necho \"---\"\ncat ../../models/cifar10-drift-detect.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/triton/tf_cifar10\"\n  requirements:\n  - tensorflow\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/cifar10.yaml\nseldon model load -f ../../models/cifar10-outlier-detect.yaml\nseldon model load -f ../../models/cifar10-drift-detect.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq .\nseldon model status cifar10-outlier -w ModelAvailable | jq .\nseldon model status cifar10-drift -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n\n\ncat ../../pipelines/cifar10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: cifar10-production\nspec:\n  steps:\n    - name: cifar10\n    - name: cifar10-outlier\n    - name: cifar10-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - cifar10\n    - cifar10-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/cifar10.yaml\n\n\nseldon pipeline status cifar10-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"cifar10-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"cifar10-production\",\n        \"uid\": \"cifeii2ufmbc73e5insg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"cifar10\"\n          },\n          {\n            \"name\": \"cifar10-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"cifar10-outlier\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"cifar10.outputs\",\n            \"cifar10-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:40:09.047429817Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ninfer(\"cifar10-production.pipeline\",20, \"normal\")\n\n\n![png](infer_files/infer_14_0.png)\n\n\n\n![png](infer_files/infer_14_1.png)\n\n\n\n![png](infer_files/infer_14_2.png)\n\n\n\n![png](infer_files/infer_14_3.png)\n\n\n\n![png](infer_files/infer_14_4.png)\n\n\n\n![png](infer_files/infer_14_5.png)\n\n\n\n![png](infer_files/infer_14_6.png)\n\n\n\n![png](infer_files/infer_14_7.png)\n\n\n\n![png](infer_files/infer_14_8.png)\n\n\n\n![png](infer_files/infer_14_9.png)\n\n\n\n![png](infer_files/infer_14_10.png)\n\n\n\n![png](infer_files/infer_14_11.png)\n\n\n\n![png](infer_files/infer_14_12.png)\n\n\n\n![png](infer_files/infer_14_13.png)\n\n\n\n![png](infer_files/infer_14_14.png)\n\n\n\n![png](infer_files/infer_14_15.png)\n\n\n\n![png](infer_files/infer_14_16.png)\n\n\n\n![png](infer_files/infer_14_17.png)\n\n\n\n![png](infer_files/infer_14_18.png)\n\n\n\n![png](infer_files/infer_14_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.437921e-08, 4.7317337e-09, 1.26449e-06, 4.8814868e-09, 1.5153439e-09, 8.490656e-09, 5.5131194e-10, 1.1617216e-09, 5.7729294e-10, 2.8839776e-07, 0.0006149016, 0.99938357, 0.888746, 2.5331951e-06, 0.00012967695, 0.10531583, 2.4284174e-05, 6.3332986e-06, 0.0016261435, 1.13079e-05, 0.0013286703, 0.0028091935, 2.0993439e-06, 3.680449e-08, 0.0013269952, 2.1766558e-05, 0.99841356, 0.00015300694, 6.9472035e-06, 1.3277059e-05, 6.1860555e-05, 3.4072806e-07, 1.1205097e-05, 0.99997175, 1.9948227e-07, 6.9880834e-08, 3.3387135e-08, 5.2603138e-08, 3.0352305e-07, 4.3738982e-08, 5.3243946e-07, 1.5870584e-05, 0.0006525102, 0.013322109, 1.480307e-06, 0.9766325, 4.9847167e-05, 0.00058075984, 0.008405659, 5.2234273e-06, 0.00023390084, 0.000116047224, 1.6682397e-06, 5.7737526e-10, 0.9975605, 6.45564e-05, 0.002371972, 1.0392675e-07, 9.747962e-08, 1.4484569e-07, 8.762438e-07, 2.4758325e-08, 5.028761e-09, 6.856381e-11, 5.9932094e-12, 4.921233e-10, 1.471166e-07, 2.7940719e-06, 3.4563383e-09, 0.99999714, 5.9420524e-10, 9.445026e-11, 4.1854888e-05, 5.041549e-08, 8.0302314e-08, 1.2119854e-07, 6.781646e-09, 1.2616152e-08, 1.1878505e-08, 1.628573e-09, 0.9999578, 3.281738e-08, 0.08930307, 1.4065135e-07, 4.1117343e-07, 0.90898305, 8.933351e-07, 0.0015637449, 0.00013868928, 9.092981e-06, 4.8759745e-07, 4.3976044e-07, 0.00016094849, 3.5653954e-07, 0.0760521, 0.8927447, 0.0011777573, 0.00265573, 0.027189083, 4.1892267e-06, 1.329405e-05, 1.8564688e-06, 1.3373891e-06, 1.0251247e-07, 8.651912e-09, 4.458202e-06, 1.4646349e-05, 1.260957e-06, 1.046087e-08, 0.9998946, 8.332438e-05, 3.900894e-07, 6.53852e-05, 3.012202e-08, 1.0247197e-07, 1.8824371e-06, 0.0004958526, 3.533475e-05, 2.739997e-07, 0.99939275, 4.840305e-06, 3.5346695e-06, 0.0005518078, 3.1597017e-07, 0.99902296, 0.00031509742, 8.07886e-07, 1.6366084e-06, 2.795575e-06, 6.112367e-06, 9.817249e-05, 2.602709e-07, 0.0004561966, 5.360607e-06, 2.8656412e-05, 0.000116040654, 6.881144e-05, 8.844774e-06, 4.4655946e-05, 3.5564542e-05, 0.006564381, 0.9926715, 0.007300911, 1.766928e-06, 3.0520596e-07, 0.026906287, 1.3769699e-06, 0.00027539674, 5.583593e-06, 3.792553e-06, 0.0003876767, 0.9651169, 0.18114138, 2.8360228e-05, 0.00019927241, 0.007685872, 0.00014663498, 3.9361137e-05, 5.941682e-05, 7.36174e-05, 0.79936546, 0.01126067, 2.3992783e-11, 7.6336457e-16, 1.4644799e-15, 1, 2.4652159e-14, 1.1786078e-10, 1.9402116e-13, 4.2408636e-15, 1.209294e-15, 2.9042784e-15, 1.5366902e-08, 1.2476195e-09, 1.3560152e-07, 0.999997, 4.3113017e-11, 2.8163534e-08, 2.4494727e-06, 1.3122828e-10, 3.8081083e-07, 2.1628158e-11, 0.0004926238, 6.9424555e-06, 2.827196e-05, 0.92534137, 9.500486e-06, 0.00036133997, 0.072713904, 1.2831057e-07, 0.0010457055, 2.8514464e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeij8fh5ss738i5bp0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",20, \"drift\")\n\n\n![png](infer_files/infer_16_0.png)\n\n\n\n![png](infer_files/infer_16_1.png)\n\n\n\n![png](infer_files/infer_16_2.png)\n\n\n\n![png](infer_files/infer_16_3.png)\n\n\n\n![png](infer_files/infer_16_4.png)\n\n\n\n![png](infer_files/infer_16_5.png)\n\n\n\n![png](infer_files/infer_16_6.png)\n\n\n\n![png](infer_files/infer_16_7.png)\n\n\n\n![png](infer_files/infer_16_8.png)\n\n\n\n![png](infer_files/infer_16_9.png)\n\n\n\n![png](infer_files/infer_16_10.png)\n\n\n\n![png](infer_files/infer_16_11.png)\n\n\n\n![png](infer_files/infer_16_12.png)\n\n\n\n![png](infer_files/infer_16_13.png)\n\n\n\n![png](infer_files/infer_16_14.png)\n\n\n\n![png](infer_files/infer_16_15.png)\n\n\n\n![png](infer_files/infer_16_16.png)\n\n\n\n![png](infer_files/infer_16_17.png)\n\n\n\n![png](infer_files/infer_16_18.png)\n\n\n\n![png](infer_files/infer_16_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [8.080701e-09, 2.3025173e-12, 2.2681688e-09, 1, 4.1828953e-11, 4.48467e-09, 3.216822e-08, 2.8404365e-13, 5.217064e-09, 3.3497323e-13, 0.96965235, 4.7030144e-06, 1.6964266e-07, 1.7355454e-05, 2.6667e-06, 1.9505828e-06, 1.1363079e-07, 3.3352034e-08, 0.030320557, 1.7086056e-07, 0.03725602, 6.8623276e-06, 7.5557014e-05, 0.00018132397, 2.2838503e-05, 0.000110639296, 2.3732607e-06, 2.1210687e-06, 0.9623351, 7.131072e-06, 0.999079, 4.207448e-09, 1.5788535e-08, 2.723756e-08, 2.6555508e-11, 2.1526697e-10, 2.7599315e-10, 2.0737433e-10, 0.0009210062, 3.0885383e-09, 6.665241e-07, 1.7765576e-09, 1.4911559e-07, 0.9765331, 1.9476123e-07, 2.8244015e-06, 0.023463126, 5.8030287e-09, 3.243206e-09, 1.12179785e-08, 4.4123663e-06, 4.7628927e-09, 1.1727273e-08, 0.9761534, 1.1409252e-08, 8.922882e-05, 0.023752932, 3.1563903e-08, 2.7916305e-09, 8.7746266e-10, 1.0166265e-05, 0.999703, 4.5408615e-05, 0.00022673907, 1.7365853e-07, 1.0147362e-06, 6.253448e-06, 2.9711526e-07, 7.811687e-07, 6.183683e-06, 0.86618125, 5.47548e-07, 0.00038408802, 0.013155022, 3.6916779e-06, 0.0006137024, 0.11965008, 3.6425424e-06, 6.7638084e-06, 1.2372367e-06, 1.9545263e-05, 1.1281859e-13, 1.6811868e-14, 0.9999777, 1.9805435e-11, 2.7563674e-06, 2.9651657e-09, 1.1363432e-12, 2.9902746e-13, 1.220973e-12, 2.9895918e-05, 3.4964305e-07, 1.1331837e-08, 1.7012125e-06, 3.6088227e-07, 3.035954e-08, 2.2102333e-06, 1.7414077e-08, 0.9999455, 1.9921794e-05, 0.9999999, 5.3446598e-11, 6.3188843e-10, 1.0956511e-07, 1.1538642e-10, 8.113561e-10, 4.7179572e-08, 1.4544753e-11, 5.490219e-08, 1.3347151e-10, 1.5363307e-07, 6.604881e-09, 2.424105e-10, 9.963063e-09, 3.9349533e-09, 1.5709017e-09, 7.705774e-10, 4.8085802e-08, 1.8885139e-05, 0.9999809, 7.147243e-08, 3.143131e-13, 2.1447092e-13, 0.00042652222, 6.945973e-12, 0.9995734, 6.174434e-09, 4.1128205e-11, 3.4031404e-13, 8.573159e-15, 1.2226405e-09, 2.3768018e-10, 2.822187e-07, 8.016278e-08, 4.0692296e-08, 6.8023346e-06, 2.3926754e-07, 0.9999925, 6.652648e-09, 7.743497e-09, 7.6360675e-06, 5.9386625e-09, 1.5675019e-09, 2.136716e-07, 1.3074002e-06, 3.700079e-10, 1.0984521e-09, 6.2138824e-08, 0.9609078, 0.03908287, 0.0008332255, 7.696685e-08, 2.4428939e-09, 7.186676e-05, 1.4520063e-09, 1.4521317e-08, 1.09093e-06, 1.2531165e-10, 0.9990938, 5.798501e-09, 5.785368e-05, 3.82365e-09, 7.404351e-08, 0.008338481, 8.048078e-10, 0.99157715, 1.1663455e-05, 1.4583546e-05, 8.3543476e-08, 3.274394e-08, 2.4682688e-05, 1.3951502e-09, 1.0260489e-08, 0.9998845, 1.9418138e-08, 8.667954e-07, 2.1851054e-07, 8.917964e-05, 4.4437223e-07, 1.1292918e-07, 4.5302792e-07, 5.631744e-08, 2.9086214e-08, 3.1013877e-07, 7.695681e-09, 2.1452344e-09, 1.1493902e-08, 6.1980093e-10, 0.99999917, 1.1436694e-08, 2.42685e-05, 8.557389e-08, 0.024081504, 0.0073837163, 4.8152968e-05, 5.128531e-07, 0.9684405, 9.630179e-08, 2.1060101e-05, 1.901065e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeimgfh5ss738i5bpg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"outlier\")\n\n\n![png](infer_files/infer_18_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [6.3606867e-06, 0.0006106364, 0.0054279356, 0.6536454, 1.4738829e-05, 2.6104701e-06, 0.3397848, 1.3538776e-05, 0.0004458526, 4.807229e-05], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [1], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"ok\")\n\n\n![png](infer_files/infer_19_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.43792e-08, 4.7317337e-09], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [0], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nUse the seldon CLI to look at the outputs from the CIFAR10 model. It will decode the Triton binary outputs for us.\nseldon pipeline inspect cifar10-production.cifar10.outputs\n\n\nseldon.default.model.cifar10.outputs\tcifeiq8fh5ss738i5bqg\t{\"modelName\":\"cifar10_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"fc10\", \"datatype\":\"FP32\", \"shape\":[\"1\", \"10\"], \"contents\":{\"fp32Contents\":[1.45001495e-8, 1.2525752e-9, 1.6298458e-7, 0.11529388, 1.7431412e-7, 0.0000061856604, 0.8846994, 6.0739285e-9, 7.43792e-8, 4.7317337e-9]}}]}\n\n\n\nseldon pipeline unload cifar10-production\n\n\nseldon model unload cifar10\nseldon model unload cifar10-outlier\nseldon model unload cifar10-drift\n\n\n\n\n\n\n", "model-training-optional-for-notebook": "\nModel Training (optional for notebook)\u00b6\nTo run local training run the training notebook.\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-06-30 15:39:28.732453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-30 15:39:28.732465: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\noutliers = []\nfor idx in range(0,X_train.shape[0]):\n    X_mask, mask = apply_mask(X_train[idx].reshape(1, 32, 32, 3),\n                                  mask_size=(14,14),\n                                  n_masks=1,\n                                  channels=[0,1,2],\n                                  mask_type='normal',\n                                  noise_distr=(0,1),\n                                  clip_rng=(0,1))\n    outliers.append(X_mask)\nX_outliers = np.vstack(outliers)\nX_outliers.shape\n\n\n(50000, 32, 32, 3)\n\n\n\ncorruption = ['brightness']\nX_corr, y_corr = fetch_cifar10c(corruption=corruption, severity=5, return_X_y=True)\nX_corr = X_corr.astype('float32') / 255\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\n\ndef infer(resourceName: str, batchSz: int, requestType: str):\n    if requestType == \"outlier\":\n        rows = X_outliers[0:0+batchSz]\n    elif requestType == \"drift\":\n        rows = X_corr[0:0+batchSz]\n    else:\n        rows = X_train[0:0+batchSz]\n    for i in range(batchSz):\n        show(rows[i])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [batchSz, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    print(response_raw)\n    print(response_raw.json())\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n", "pipeline": "\nPipeline\u00b6\ncat ../../models/cifar10.yaml\necho \"---\"\ncat ../../models/cifar10-outlier-detect.yaml\necho \"---\"\ncat ../../models/cifar10-drift-detect.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/triton/tf_cifar10\"\n  requirements:\n  - tensorflow\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/cifar10.yaml\nseldon model load -f ../../models/cifar10-outlier-detect.yaml\nseldon model load -f ../../models/cifar10-drift-detect.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq .\nseldon model status cifar10-outlier -w ModelAvailable | jq .\nseldon model status cifar10-drift -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n\n\ncat ../../pipelines/cifar10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: cifar10-production\nspec:\n  steps:\n    - name: cifar10\n    - name: cifar10-outlier\n    - name: cifar10-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - cifar10\n    - cifar10-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/cifar10.yaml\n\n\nseldon pipeline status cifar10-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"cifar10-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"cifar10-production\",\n        \"uid\": \"cifeii2ufmbc73e5insg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"cifar10\"\n          },\n          {\n            \"name\": \"cifar10-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"cifar10-outlier\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"cifar10.outputs\",\n            \"cifar10-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:40:09.047429817Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ninfer(\"cifar10-production.pipeline\",20, \"normal\")\n\n\n![png](infer_files/infer_14_0.png)\n\n\n\n![png](infer_files/infer_14_1.png)\n\n\n\n![png](infer_files/infer_14_2.png)\n\n\n\n![png](infer_files/infer_14_3.png)\n\n\n\n![png](infer_files/infer_14_4.png)\n\n\n\n![png](infer_files/infer_14_5.png)\n\n\n\n![png](infer_files/infer_14_6.png)\n\n\n\n![png](infer_files/infer_14_7.png)\n\n\n\n![png](infer_files/infer_14_8.png)\n\n\n\n![png](infer_files/infer_14_9.png)\n\n\n\n![png](infer_files/infer_14_10.png)\n\n\n\n![png](infer_files/infer_14_11.png)\n\n\n\n![png](infer_files/infer_14_12.png)\n\n\n\n![png](infer_files/infer_14_13.png)\n\n\n\n![png](infer_files/infer_14_14.png)\n\n\n\n![png](infer_files/infer_14_15.png)\n\n\n\n![png](infer_files/infer_14_16.png)\n\n\n\n![png](infer_files/infer_14_17.png)\n\n\n\n![png](infer_files/infer_14_18.png)\n\n\n\n![png](infer_files/infer_14_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.437921e-08, 4.7317337e-09, 1.26449e-06, 4.8814868e-09, 1.5153439e-09, 8.490656e-09, 5.5131194e-10, 1.1617216e-09, 5.7729294e-10, 2.8839776e-07, 0.0006149016, 0.99938357, 0.888746, 2.5331951e-06, 0.00012967695, 0.10531583, 2.4284174e-05, 6.3332986e-06, 0.0016261435, 1.13079e-05, 0.0013286703, 0.0028091935, 2.0993439e-06, 3.680449e-08, 0.0013269952, 2.1766558e-05, 0.99841356, 0.00015300694, 6.9472035e-06, 1.3277059e-05, 6.1860555e-05, 3.4072806e-07, 1.1205097e-05, 0.99997175, 1.9948227e-07, 6.9880834e-08, 3.3387135e-08, 5.2603138e-08, 3.0352305e-07, 4.3738982e-08, 5.3243946e-07, 1.5870584e-05, 0.0006525102, 0.013322109, 1.480307e-06, 0.9766325, 4.9847167e-05, 0.00058075984, 0.008405659, 5.2234273e-06, 0.00023390084, 0.000116047224, 1.6682397e-06, 5.7737526e-10, 0.9975605, 6.45564e-05, 0.002371972, 1.0392675e-07, 9.747962e-08, 1.4484569e-07, 8.762438e-07, 2.4758325e-08, 5.028761e-09, 6.856381e-11, 5.9932094e-12, 4.921233e-10, 1.471166e-07, 2.7940719e-06, 3.4563383e-09, 0.99999714, 5.9420524e-10, 9.445026e-11, 4.1854888e-05, 5.041549e-08, 8.0302314e-08, 1.2119854e-07, 6.781646e-09, 1.2616152e-08, 1.1878505e-08, 1.628573e-09, 0.9999578, 3.281738e-08, 0.08930307, 1.4065135e-07, 4.1117343e-07, 0.90898305, 8.933351e-07, 0.0015637449, 0.00013868928, 9.092981e-06, 4.8759745e-07, 4.3976044e-07, 0.00016094849, 3.5653954e-07, 0.0760521, 0.8927447, 0.0011777573, 0.00265573, 0.027189083, 4.1892267e-06, 1.329405e-05, 1.8564688e-06, 1.3373891e-06, 1.0251247e-07, 8.651912e-09, 4.458202e-06, 1.4646349e-05, 1.260957e-06, 1.046087e-08, 0.9998946, 8.332438e-05, 3.900894e-07, 6.53852e-05, 3.012202e-08, 1.0247197e-07, 1.8824371e-06, 0.0004958526, 3.533475e-05, 2.739997e-07, 0.99939275, 4.840305e-06, 3.5346695e-06, 0.0005518078, 3.1597017e-07, 0.99902296, 0.00031509742, 8.07886e-07, 1.6366084e-06, 2.795575e-06, 6.112367e-06, 9.817249e-05, 2.602709e-07, 0.0004561966, 5.360607e-06, 2.8656412e-05, 0.000116040654, 6.881144e-05, 8.844774e-06, 4.4655946e-05, 3.5564542e-05, 0.006564381, 0.9926715, 0.007300911, 1.766928e-06, 3.0520596e-07, 0.026906287, 1.3769699e-06, 0.00027539674, 5.583593e-06, 3.792553e-06, 0.0003876767, 0.9651169, 0.18114138, 2.8360228e-05, 0.00019927241, 0.007685872, 0.00014663498, 3.9361137e-05, 5.941682e-05, 7.36174e-05, 0.79936546, 0.01126067, 2.3992783e-11, 7.6336457e-16, 1.4644799e-15, 1, 2.4652159e-14, 1.1786078e-10, 1.9402116e-13, 4.2408636e-15, 1.209294e-15, 2.9042784e-15, 1.5366902e-08, 1.2476195e-09, 1.3560152e-07, 0.999997, 4.3113017e-11, 2.8163534e-08, 2.4494727e-06, 1.3122828e-10, 3.8081083e-07, 2.1628158e-11, 0.0004926238, 6.9424555e-06, 2.827196e-05, 0.92534137, 9.500486e-06, 0.00036133997, 0.072713904, 1.2831057e-07, 0.0010457055, 2.8514464e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeij8fh5ss738i5bp0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",20, \"drift\")\n\n\n![png](infer_files/infer_16_0.png)\n\n\n\n![png](infer_files/infer_16_1.png)\n\n\n\n![png](infer_files/infer_16_2.png)\n\n\n\n![png](infer_files/infer_16_3.png)\n\n\n\n![png](infer_files/infer_16_4.png)\n\n\n\n![png](infer_files/infer_16_5.png)\n\n\n\n![png](infer_files/infer_16_6.png)\n\n\n\n![png](infer_files/infer_16_7.png)\n\n\n\n![png](infer_files/infer_16_8.png)\n\n\n\n![png](infer_files/infer_16_9.png)\n\n\n\n![png](infer_files/infer_16_10.png)\n\n\n\n![png](infer_files/infer_16_11.png)\n\n\n\n![png](infer_files/infer_16_12.png)\n\n\n\n![png](infer_files/infer_16_13.png)\n\n\n\n![png](infer_files/infer_16_14.png)\n\n\n\n![png](infer_files/infer_16_15.png)\n\n\n\n![png](infer_files/infer_16_16.png)\n\n\n\n![png](infer_files/infer_16_17.png)\n\n\n\n![png](infer_files/infer_16_18.png)\n\n\n\n![png](infer_files/infer_16_19.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [8.080701e-09, 2.3025173e-12, 2.2681688e-09, 1, 4.1828953e-11, 4.48467e-09, 3.216822e-08, 2.8404365e-13, 5.217064e-09, 3.3497323e-13, 0.96965235, 4.7030144e-06, 1.6964266e-07, 1.7355454e-05, 2.6667e-06, 1.9505828e-06, 1.1363079e-07, 3.3352034e-08, 0.030320557, 1.7086056e-07, 0.03725602, 6.8623276e-06, 7.5557014e-05, 0.00018132397, 2.2838503e-05, 0.000110639296, 2.3732607e-06, 2.1210687e-06, 0.9623351, 7.131072e-06, 0.999079, 4.207448e-09, 1.5788535e-08, 2.723756e-08, 2.6555508e-11, 2.1526697e-10, 2.7599315e-10, 2.0737433e-10, 0.0009210062, 3.0885383e-09, 6.665241e-07, 1.7765576e-09, 1.4911559e-07, 0.9765331, 1.9476123e-07, 2.8244015e-06, 0.023463126, 5.8030287e-09, 3.243206e-09, 1.12179785e-08, 4.4123663e-06, 4.7628927e-09, 1.1727273e-08, 0.9761534, 1.1409252e-08, 8.922882e-05, 0.023752932, 3.1563903e-08, 2.7916305e-09, 8.7746266e-10, 1.0166265e-05, 0.999703, 4.5408615e-05, 0.00022673907, 1.7365853e-07, 1.0147362e-06, 6.253448e-06, 2.9711526e-07, 7.811687e-07, 6.183683e-06, 0.86618125, 5.47548e-07, 0.00038408802, 0.013155022, 3.6916779e-06, 0.0006137024, 0.11965008, 3.6425424e-06, 6.7638084e-06, 1.2372367e-06, 1.9545263e-05, 1.1281859e-13, 1.6811868e-14, 0.9999777, 1.9805435e-11, 2.7563674e-06, 2.9651657e-09, 1.1363432e-12, 2.9902746e-13, 1.220973e-12, 2.9895918e-05, 3.4964305e-07, 1.1331837e-08, 1.7012125e-06, 3.6088227e-07, 3.035954e-08, 2.2102333e-06, 1.7414077e-08, 0.9999455, 1.9921794e-05, 0.9999999, 5.3446598e-11, 6.3188843e-10, 1.0956511e-07, 1.1538642e-10, 8.113561e-10, 4.7179572e-08, 1.4544753e-11, 5.490219e-08, 1.3347151e-10, 1.5363307e-07, 6.604881e-09, 2.424105e-10, 9.963063e-09, 3.9349533e-09, 1.5709017e-09, 7.705774e-10, 4.8085802e-08, 1.8885139e-05, 0.9999809, 7.147243e-08, 3.143131e-13, 2.1447092e-13, 0.00042652222, 6.945973e-12, 0.9995734, 6.174434e-09, 4.1128205e-11, 3.4031404e-13, 8.573159e-15, 1.2226405e-09, 2.3768018e-10, 2.822187e-07, 8.016278e-08, 4.0692296e-08, 6.8023346e-06, 2.3926754e-07, 0.9999925, 6.652648e-09, 7.743497e-09, 7.6360675e-06, 5.9386625e-09, 1.5675019e-09, 2.136716e-07, 1.3074002e-06, 3.700079e-10, 1.0984521e-09, 6.2138824e-08, 0.9609078, 0.03908287, 0.0008332255, 7.696685e-08, 2.4428939e-09, 7.186676e-05, 1.4520063e-09, 1.4521317e-08, 1.09093e-06, 1.2531165e-10, 0.9990938, 5.798501e-09, 5.785368e-05, 3.82365e-09, 7.404351e-08, 0.008338481, 8.048078e-10, 0.99157715, 1.1663455e-05, 1.4583546e-05, 8.3543476e-08, 3.274394e-08, 2.4682688e-05, 1.3951502e-09, 1.0260489e-08, 0.9998845, 1.9418138e-08, 8.667954e-07, 2.1851054e-07, 8.917964e-05, 4.4437223e-07, 1.1292918e-07, 4.5302792e-07, 5.631744e-08, 2.9086214e-08, 3.1013877e-07, 7.695681e-09, 2.1452344e-09, 1.1493902e-08, 6.1980093e-10, 0.99999917, 1.1436694e-08, 2.42685e-05, 8.557389e-08, 0.024081504, 0.0073837163, 4.8152968e-05, 5.128531e-07, 0.9684405, 9.630179e-08, 2.1060101e-05, 1.901065e-07], 'name': 'fc10', 'shape': [20, 10], 'datatype': 'FP32'}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect cifar10-production.cifar10-drift.outputs.is_drift\n\n\nseldon.default.model.cifar10-drift.outputs\tcifeimgfh5ss738i5bpg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"outlier\")\n\n\n![png](infer_files/infer_18_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [6.3606867e-06, 0.0006106364, 0.0054279356, 0.6536454, 1.4738829e-05, 2.6104701e-06, 0.3397848, 1.3538776e-05, 0.0004458526, 4.807229e-05], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [1], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\ninfer(\"cifar10-production.pipeline\",1, \"ok\")\n\n\n![png](infer_files/infer_19_0.png)\n\n\n\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [1.45001495e-08, 1.2525752e-09, 1.6298458e-07, 0.11529388, 1.7431412e-07, 6.1856604e-06, 0.8846994, 6.0739285e-09, 7.43792e-08, 4.7317337e-09], 'name': 'fc10', 'shape': [1, 10], 'datatype': 'FP32'}, {'data': [0], 'name': 'is_outlier', 'shape': [1, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nUse the seldon CLI to look at the outputs from the CIFAR10 model. It will decode the Triton binary outputs for us.\nseldon pipeline inspect cifar10-production.cifar10.outputs\n\n\nseldon.default.model.cifar10.outputs\tcifeiq8fh5ss738i5bqg\t{\"modelName\":\"cifar10_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"fc10\", \"datatype\":\"FP32\", \"shape\":[\"1\", \"10\"], \"contents\":{\"fp32Contents\":[1.45001495e-8, 1.2525752e-9, 1.6298458e-7, 0.11529388, 1.7431412e-7, 0.0000061856604, 0.8846994, 6.0739285e-9, 7.43792e-8, 4.7317337e-9]}}]}\n\n\n\nseldon pipeline unload cifar10-production\n\n\nseldon model unload cifar10\nseldon model unload cifar10-outlier\nseldon model unload cifar10-drift\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/cifar10.html", "key": "examples/cifar10"}}, "getting-started/kubernetes-installation/security/confluent-sasl": {"sections": {"confluent-cloud-sasl-example": "\nConfluent Cloud SASL Example\u00b6\n\nNew in Seldon Core 2.5.0\n\nSeldon Core v2 can integrate with Confluent Cloud managed Kafka.\nIn this example we use SASL security mechanism.\n\nCreate API Keys\u00b6\nIn your Confluent Cloud environment create new API keys.\nThe easiest way to obtain all required information is to head to Clients -> New client (choose e.g. Go) and generate new Kafka cluster API key from there.\nThis will generate for you:\n\nKey (we use it as username)\nSecret (we use it as password)\n\nDo not forget to also copy the bootstrap.servers from the example config.\nSee Confluent Cloud documentation in case of issues.\n\n\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic confluent-kafka-sasl -n seldon-mesh --from-literal password=\"<Confluent Cloud API Secret>\"\n\n\n\n\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: < Confluent Cloud Broker Endpoints >\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n  consumer:\n    messageMaxBytes: 8388608\n  producer:\n    messageMaxBytes: 8388608\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: \"PLAIN\"\n      client:\n        username: < username >\n        secret: confluent-kafka-sasl\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n\n\nTroubleshooting\u00b6\n\nFirst check Confluent Cloud documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n\n", "create-api-keys": "\nCreate API Keys\u00b6\nIn your Confluent Cloud environment create new API keys.\nThe easiest way to obtain all required information is to head to Clients -> New client (choose e.g. Go) and generate new Kafka cluster API key from there.\nThis will generate for you:\n\nKey (we use it as username)\nSecret (we use it as password)\n\nDo not forget to also copy the bootstrap.servers from the example config.\nSee Confluent Cloud documentation in case of issues.\n", "create-kubernetes-secret": "\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic confluent-kafka-sasl -n seldon-mesh --from-literal password=\"<Confluent Cloud API Secret>\"\n\n\n", "configure-seldon-core-v2": "\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: < Confluent Cloud Broker Endpoints >\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n  consumer:\n    messageMaxBytes: 8388608\n  producer:\n    messageMaxBytes: 8388608\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: \"PLAIN\"\n      client:\n        username: < username >\n        secret: confluent-kafka-sasl\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n", "troubleshooting": "\nTroubleshooting\u00b6\n\nFirst check Confluent Cloud documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/confluent-sasl.html", "key": "getting-started/kubernetes-installation/security/confluent-sasl"}}, "development/licenses": {"sections": {"rd-part-license-generation": "\n3rd part license generation\u00b6\nWe use the following tools and processes for generationg 3rd party licenses.\n\nGo Modules\u00b6\n\nInstall tools\u00b6\nFor Go modules we use a fork of Kubeflow\u2019s testing repo with some fixes.\nClone and update to this branch.\ngit clone https://github.com/SeldonIO/kubeflow-testing -b seldon-update\n\n\nGo to go-license-tools folder:\ncd kubeflow-testing/py/kubeflow/testing/go-license-tools/\n\n\nInstall\npython setup.py install\n\n\n\n\nGeneration\u00b6\nRun make update-3rd-party-licenses in top level folder.\n\n\n", "go-modules": "\nGo Modules\u00b6\n\nInstall tools\u00b6\nFor Go modules we use a fork of Kubeflow\u2019s testing repo with some fixes.\nClone and update to this branch.\ngit clone https://github.com/SeldonIO/kubeflow-testing -b seldon-update\n\n\nGo to go-license-tools folder:\ncd kubeflow-testing/py/kubeflow/testing/go-license-tools/\n\n\nInstall\npython setup.py install\n\n\n\n\nGeneration\u00b6\nRun make update-3rd-party-licenses in top level folder.\n\n", "install-tools": "\nInstall tools\u00b6\nFor Go modules we use a fork of Kubeflow\u2019s testing repo with some fixes.\nClone and update to this branch.\ngit clone https://github.com/SeldonIO/kubeflow-testing -b seldon-update\n\n\nGo to go-license-tools folder:\ncd kubeflow-testing/py/kubeflow/testing/go-license-tools/\n\n\nInstall\npython setup.py install\n\n\n", "generation": "\nGeneration\u00b6\nRun make update-3rd-party-licenses in top level folder.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/development/licenses.html", "key": "development/licenses"}}, "kubernetes/resources/model": {"sections": {"model": "\nModel\u00b6\nA Model is the core atomic building block. It specifies a machine learning artifact that will be loaded onto one of the running Servers. A model could be a standard machine learning inference component such as\n\na Tensorflow model, PyTorch model or SKLearn model.\nan inference transformation component such as a SKLearn pipeline or a piece of custom python logic.\na monitoring component such as an outlier detector or drift detector.\nAn alibi-explain model explainer\n\nAn example is shown below for a SKLearn model for iris classification:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nIts Kubernetes spec has two core requirements\n\nA storageUri specifying the location of the artifact. This can be any rclone URI specification.\nA requirements list which provides tags that need to be matched by the Server that can run this artifact type. By default when you install Seldon we provide a set of Servers that cover a range of artifact types.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/model/index.html", "key": "kubernetes/resources/model"}}, "examples/inference": {"sections": {"inference-examples": "\nInference Examples\u00b6\nRun these examples from the samples folder.\n\nInference Examples\u00b6\nWe will show:\n\nModel inference to a Tensorflow model\n\nREST and gRPC using seldon CLI, curl and grpcurl\n\n\nPipeline inference\n\nREST and gRPC using seldon CLI, curl and grpcurl\n\n\n\n%env INFER_ENDPOINT=0.0.0.0:9000\n\n\nenv: INFER_ENDPOINT=0.0.0.0:9000\n\n\n\n\nTensorflow Model\u00b6\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer tfsimple1 --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"tfsimple1_1\",\n\t\"model_version\": \"1\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer tfsimple1 --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple1\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"tfsimple1_1\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple1 \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple\",\"uid\":\"cg5fm6c6dpcs73c4qhe0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple1\"}],\"output\":{\"steps\":[\"tfsimple1.outputs\"]},\"kubernetesMeta\":{}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T09:40:41.317797761Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple  --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline infer tfsimple --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple.pipeline\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"\",\"outputs\":[{\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32],\"name\":\"OUTPUT0\",\"shape\":[1,16],\"datatype\":\"INT32\"},{\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"name\":\"OUTPUT1\",\"shape\":[1,16],\"datatype\":\"INT32\"}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple.pipeline \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\nseldon pipeline unload tfsimple\nseldon model unload tfsimple1\n\n\n{}\n{}\n\n\n\n\n\n\n\n", "id1": "\nInference Examples\u00b6\nWe will show:\n\nModel inference to a Tensorflow model\n\nREST and gRPC using seldon CLI, curl and grpcurl\n\n\nPipeline inference\n\nREST and gRPC using seldon CLI, curl and grpcurl\n\n\n\n%env INFER_ENDPOINT=0.0.0.0:9000\n\n\nenv: INFER_ENDPOINT=0.0.0.0:9000\n\n\n\n\nTensorflow Model\u00b6\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer tfsimple1 --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"tfsimple1_1\",\n\t\"model_version\": \"1\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer tfsimple1 --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple1\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"tfsimple1_1\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple1 \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple\",\"uid\":\"cg5fm6c6dpcs73c4qhe0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple1\"}],\"output\":{\"steps\":[\"tfsimple1.outputs\"]},\"kubernetesMeta\":{}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T09:40:41.317797761Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple  --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline infer tfsimple --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple.pipeline\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"\",\"outputs\":[{\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32],\"name\":\"OUTPUT0\",\"shape\":[1,16],\"datatype\":\"INT32\"},{\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"name\":\"OUTPUT1\",\"shape\":[1,16],\"datatype\":\"INT32\"}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple.pipeline \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\nseldon pipeline unload tfsimple\nseldon model unload tfsimple1\n\n\n{}\n{}\n\n\n\n\n\n\n", "tensorflow-model": "\nTensorflow Model\u00b6\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer tfsimple1 --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"tfsimple1_1\",\n\t\"model_version\": \"1\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer tfsimple1 --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple1\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"tfsimple1_1\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple1 \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple\",\"uid\":\"cg5fm6c6dpcs73c4qhe0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple1\"}],\"output\":{\"steps\":[\"tfsimple1.outputs\"]},\"kubernetesMeta\":{}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T09:40:41.317797761Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple  --inference-host ${INFER_ENDPOINT} \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline infer tfsimple --inference-mode grpc  --inference-host ${INFER_ENDPOINT} \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\ncurl http://${INFER_ENDPOINT}/v2/models/tfsimple1/infer -H \"Content-Type: application/json\" -H \"seldon-model: tfsimple.pipeline\" \\\n        -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\"model_name\":\"\",\"outputs\":[{\"data\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32],\"name\":\"OUTPUT0\",\"shape\":[1,16],\"datatype\":\"INT32\"},{\"data\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"name\":\"OUTPUT1\",\"shape\":[1,16],\"datatype\":\"INT32\"}]}\n\n\ngrpcurl -d '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimple.pipeline \\\n    ${INFER_ENDPOINT} inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ]\n    }\n  ],\n  \"rawOutputContents\": [\n    \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\n    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"\n  ]\n}\n\n\nseldon pipeline unload tfsimple\nseldon model unload tfsimple1\n\n\n{}\n{}\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/inference.html", "key": "examples/inference"}}, "kubernetes/metrics": {"sections": {"metrics": "\nMetrics\u00b6\nMetrics are exposed for scrapping by Prometheus.\n\nExample Installation\u00b6\nWe recommend to install kube-prometheus that provides an all-in-one package with the Prometheus operator.\n\nRBAC\u00b6\nYou will need to modify the default RBAC installed by kube-prometheus as described here.\nFrom the prometheus folder in the project run:\nkubectl apply -f rbac/cr.yaml\n\n\n\n\n\nMonitors\u00b6\nWe use a PodMonitor for scrapping agent metrics. The envoy and server monitors are there for completeness but not presently needed.\nkubectl apply -f monitors\n\n\nIncludes:\n\nAgent pod monitor. Monitors the metrics port of server inference pods.\nServer pod monitor. Monitors the server-metrics port of inference server pods.\nEnvoy service monitor. Monitors the Envoy gateway proxies.\nPipeline gateway pod monitor. Monitors the metrics port of pipeline gateway pods.\n\nPod monitors were chosen as ports for metrics are not exposed at service level as we do not have a top level service for server replicas but 1 headless service per replica. Future discussions could reference this.\n\n\nExample Grafana Dashboard\u00b6\nCheck metrics for more information.\n\n\nReference\u00b6\n\nPrometheus CRDs\n\n\n", "example-installation": "\nExample Installation\u00b6\nWe recommend to install kube-prometheus that provides an all-in-one package with the Prometheus operator.\n\nRBAC\u00b6\nYou will need to modify the default RBAC installed by kube-prometheus as described here.\nFrom the prometheus folder in the project run:\nkubectl apply -f rbac/cr.yaml\n\n\n\n", "rbac": "\nRBAC\u00b6\nYou will need to modify the default RBAC installed by kube-prometheus as described here.\nFrom the prometheus folder in the project run:\nkubectl apply -f rbac/cr.yaml\n\n\n", "monitors": "\nMonitors\u00b6\nWe use a PodMonitor for scrapping agent metrics. The envoy and server monitors are there for completeness but not presently needed.\nkubectl apply -f monitors\n\n\nIncludes:\n\nAgent pod monitor. Monitors the metrics port of server inference pods.\nServer pod monitor. Monitors the server-metrics port of inference server pods.\nEnvoy service monitor. Monitors the Envoy gateway proxies.\nPipeline gateway pod monitor. Monitors the metrics port of pipeline gateway pods.\n\nPod monitors were chosen as ports for metrics are not exposed at service level as we do not have a top level service for server replicas but 1 headless service per replica. Future discussions could reference this.\n", "example-grafana-dashboard": "\nExample Grafana Dashboard\u00b6\nCheck metrics for more information.\n", "reference": "\nReference\u00b6\n\nPrometheus CRDs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/metrics/index.html", "key": "kubernetes/metrics"}}, "kubernetes/resources/seldonconfig": {"sections": {"seldon-config": "\nSeldon Config\u00b6\n\nNote\nThis section is for advanced usage where you want to define how seldon is installed in each namespace.\n\nThe SeldonConfig resource defines the core installation components installed by Seldon. If you wish to install Seldon, you can use the SeldonRuntime resource which allows easy overriding of some parts defined in this specification. In general, we advise core DevOps to use the default SeldonConfig or customize it for their usage. Individual installation of Seldon can then use the SeldonRuntime with a few overrides for special customisation needed in that namespace.\nThe specification contains core PodSpecs for each core component and a section for general configuration including the ConfigMaps that are created for the Agent (rclone defaults), Kafka and Tracing (open telemetry).\ntype SeldonConfigSpec struct {\n\tComponents []*ComponentDefn    `json:\"components,omitempty\"`\n\tConfig     SeldonConfiguration `json:\"config,omitempty\"`\n}\n\ntype SeldonConfiguration struct {\n\tTracingConfig TracingConfig      `json:\"tracingConfig,omitempty\"`\n\tKafkaConfig   KafkaConfig        `json:\"kafkaConfig,omitempty\"`\n\tAgentConfig   AgentConfiguration `json:\"agentConfig,omitempty\"`\n\tServiceConfig ServiceConfig      `json:\"serviceConfig,omitempty\"`\n}\n\ntype ServiceConfig struct {\n\tGrpcServicePrefix string         `json:\"grpcServicePrefix,omitempty\"`\n\tServiceType       v1.ServiceType `json:\"serviceType,omitempty\"`\n}\n\ntype KafkaConfig struct {\n\tBootstrapServers      string                        `json:\"bootstrap.servers,omitempty\"`\n\tConsumerGroupIdPrefix string                        `json:\"consumerGroupIdPrefix,omitempty\"`\n\tDebug                 string                        `json:\"debug,omitempty\"`\n\tConsumer              map[string]intstr.IntOrString `json:\"consumer,omitempty\"`\n\tProducer              map[string]intstr.IntOrString `json:\"producer,omitempty\"`\n\tStreams               map[string]intstr.IntOrString `json:\"streams,omitempty\"`\n\tTopicPrefix           string                        `json:\"topicPrefix,omitempty\"`\n}\n\ntype AgentConfiguration struct {\n\tRclone RcloneConfiguration `json:\"rclone,omitempty\" yaml:\"rclone,omitempty\"`\n}\n\ntype RcloneConfiguration struct {\n\tConfigSecrets []string `json:\"config_secrets,omitempty\" yaml:\"config_secrets,omitempty\"`\n\tConfig        []string `json:\"config,omitempty\" yaml:\"config,omitempty\"`\n}\n\ntype TracingConfig struct {\n\tDisable              bool   `json:\"disable,omitempty\"`\n\tOtelExporterEndpoint string `json:\"otelExporterEndpoint,omitempty\"`\n\tOtelExporterProtocol string `json:\"otelExporterProtocol,omitempty\"`\n\tRatio                string `json:\"ratio,omitempty\"`\n}\n\ntype ComponentDefn struct {\n\t// +kubebuilder:validation:Required\n\tName                 string                  `json:\"name\"`\n\tReplicas             *int32                  `json:\"replicas,omitempty\"`\n\tPodSpec              *v1.PodSpec             `json:\"podSpec,omitempty\"`\n\tVolumeClaimTemplates []PersistentVolumeClaim `json:\"volumeClaimTemplates,omitempty\"`\n}\n\n\nSome of these values can be overridden on a per namespace basis via the SeldonRuntime resource.\nThe default configuration is shown below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonConfig\nmetadata:\n  name: default\nspec:\n  components:\n  - name: seldon-dataflow-engine\n    replicas: 1\n    podSpec:\n      containers:\n      - env:\n        - name: SELDON_UPSTREAM_HOST\n          value: seldon-scheduler\n        - name: SELDON_UPSTREAM_PORT\n          value: \"9008\"\n        - name: OTEL_JAVAAGENT_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: OTEL_JAVAAGENT_ENABLED\n              name: seldon-tracing\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              key: OTEL_EXPORTER_OTLP_ENDPOINT\n              name: seldon-tracing\n        - name: OTEL_EXPORTER_OTLP_PROTOCOL\n          valueFrom:\n            configMapKeyRef:\n              key: OTEL_EXPORTER_OTLP_PROTOCOL\n              name: seldon-tracing\n        - name: SELDON_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: seldonio/seldon-dataflow-engine:latest\n        imagePullPolicy: Always\n        name: dataflow-engine\n        resources:\n          limits:\n            memory: 1G\n          requests:\n            cpu: 100m\n            memory: 1G\n      serviceAccountName: seldon-scheduler\n      terminationGracePeriodSeconds: 5\n  - name: seldon-envoy\n    replicas: 1\n    podSpec:\n      containers:\n      - image: seldonio/seldon-envoy:latest\n        imagePullPolicy: Always\n        name: envoy\n        ports:\n        - containerPort: 9000\n          name: http\n        - containerPort: 9003\n          name: envoy-admin\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: envoy-admin\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          failureThreshold: 3\n      terminationGracePeriodSeconds: 5\n  - name: hodometer\n    replicas: 1\n    podSpec:\n      containers:\n      - env:\n        - name: PUBLISH_URL\n          value: http://hodometer.seldon.io\n        - name: SCHEDULER_HOST\n          value: seldon-scheduler\n        - name: SCHEDULER_PLAINTXT_PORT\n          value: \"9004\"\n        - name: SCHEDULER_TLS_PORT\n          value: \"9044\"\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: seldonio/seldon-hodometer:latest\n        imagePullPolicy: Always\n        name: hodometer\n        resources:\n          limits:\n            memory: 32Mi\n          requests:\n            cpu: 1m\n            memory: 32Mi\n      serviceAccountName: hodometer\n      terminationGracePeriodSeconds: 5\n  - name: seldon-modelgateway\n    replicas: 1\n    podSpec:\n      containers:\n      - args:\n        - --scheduler-host=seldon-scheduler\n        - --scheduler-plaintxt-port=$(SELDON_SCHEDULER_PLAINTXT_PORT)\n        - --scheduler-tls-port=$(SELDON_SCHEDULER_TLS_PORT)\n        - --envoy-host=seldon-mesh\n        - --envoy-port=80\n        - --kafka-config-path=/mnt/kafka/kafka.json\n        - --tracing-config-path=/mnt/tracing/tracing.json\n        command:\n        - /bin/modelgateway\n        env:\n        - name: SELDON_SCHEDULER_PLAINTXT_PORT\n          value: \"9004\"\n        - name: SELDON_SCHEDULER_TLS_PORT\n          value: \"9044\"\n        - name: MODELGATEWAY_MAX_NUM_CONSUMERS\n          value: \"100\"\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: seldonio/seldon-modelgateway:latest\n        imagePullPolicy: Always\n        name: modelgateway\n        resources:\n          limits:\n            memory: 1G\n          requests:\n            cpu: 100m\n            memory: 1G\n        volumeMounts:\n        - mountPath: /mnt/kafka\n          name: kafka-config-volume\n        - mountPath: /mnt/tracing\n          name: tracing-config-volume\n      serviceAccountName: seldon-scheduler\n      terminationGracePeriodSeconds: 5\n      volumes:\n      - configMap:\n          name: seldon-kafka\n        name: kafka-config-volume\n      - configMap:\n          name: seldon-tracing\n        name: tracing-config-volume\n  - name: seldon-pipelinegateway\n    replicas: 1\n    podSpec:\n      containers:\n      - args:\n        - --http-port=9010\n        - --grpc-port=9011\n        - --metrics-port=9006\n        - --scheduler-host=seldon-scheduler\n        - --scheduler-plaintxt-port=$(SELDON_SCHEDULER_PLAINTXT_PORT)\n        - --scheduler-tls-port=$(SELDON_SCHEDULER_TLS_PORT)\n        - --envoy-host=seldon-mesh\n        - --envoy-port=80\n        - --kafka-config-path=/mnt/kafka/kafka.json\n        - --tracing-config-path=/mnt/tracing/tracing.json\n        command:\n        - /bin/pipelinegateway\n        env:\n        - name: SELDON_SCHEDULER_PLAINTXT_PORT\n          value: \"9004\"\n        - name: SELDON_SCHEDULER_TLS_PORT\n          value: \"9044\"\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: seldonio/seldon-pipelinegateway\n        imagePullPolicy: Always\n        name: pipelinegateway\n        ports:\n        - containerPort: 9010\n          name: http\n          protocol: TCP\n        - containerPort: 9011\n          name: grpc\n          protocol: TCP\n        - containerPort: 9006\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            memory: 1G\n          requests:\n            cpu: 100m\n            memory: 1G\n        volumeMounts:\n        - mountPath: /mnt/kafka\n          name: kafka-config-volume\n        - mountPath: /mnt/tracing\n          name: tracing-config-volume\n      serviceAccountName: seldon-scheduler\n      terminationGracePeriodSeconds: 5\n      volumes:\n      - configMap:\n          name: seldon-kafka\n        name: kafka-config-volume\n      - configMap:\n          name: seldon-tracing\n        name: tracing-config-volume\n  - name: seldon-scheduler\n    replicas: 1\n    podSpec:\n      containers:\n      - args:\n        - --pipeline-gateway-host=seldon-pipelinegateway\n        - --tracing-config-path=/mnt/tracing/tracing.json\n        - --db-path=/mnt/scheduler/db\n        - --allow-plaintxt=$(ALLOW_PLAINTXT)\n        - --kafka-config-path=/mnt/kafka/kafka.json\n        command:\n        - /bin/scheduler\n        env:\n        - name: ALLOW_PLAINTXT\n          value: \"true\"\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: seldonio/seldon-scheduler:latest\n        imagePullPolicy: Always\n        name: scheduler\n        ports:\n        - containerPort: 9002\n          name: xds\n        - containerPort: 9004\n          name: scheduler\n        - containerPort: 9044\n          name: scheduler-mtls\n        - containerPort: 9005\n          name: agent\n        - containerPort: 9055\n          name: agent-mtls\n        - containerPort: 9008\n          name: dataflow\n        resources:\n          limits:\n            memory: 1G\n          requests:\n            cpu: 100m\n            memory: 1G\n        volumeMounts:\n        - mountPath: /mnt/kafka\n          name: kafka-config-volume\n        - mountPath: /mnt/tracing\n          name: tracing-config-volume\n        - mountPath: /mnt/scheduler\n          name: scheduler-state\n      serviceAccountName: seldon-scheduler\n      terminationGracePeriodSeconds: 5\n      volumes:\n      - configMap:\n          name: seldon-kafka\n        name: kafka-config-volume\n      - configMap:\n          name: seldon-tracing\n        name: tracing-config-volume\n    volumeClaimTemplates:\n    - name: scheduler-state\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1G\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/seldonconfig/index.html", "key": "kubernetes/resources/seldonconfig"}}, "cli/docs/seldon_pipeline_infer": {"sections": {"seldon-pipeline-infer": "\nseldon pipeline infer\u00b6\nrun inference on a pipeline\n\nSynopsis\u00b6\ncall a pipeline with a given input and get a prediction\nseldon pipeline infer <pipelineName> (data) [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        inference payload file\n      --header stringArray      add a header, e.g. key=value; use the flag multiple times to add more than one header\n  -h, --help                    help for infer\n      --inference-host string   seldon inference host (default \"0.0.0.0:9000\")\n      --inference-mode string   inference mode (rest or grpc) (default \"rest\")\n  -i, --iterations int          how many times to run inference (default 1)\n  -t, --seconds int             number of secs to run inference\n      --show-headers            show request and response headers\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -s, --sticky-session          use sticky session from last inference (only works with experiments)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n\n", "synopsis": "\nSynopsis\u00b6\ncall a pipeline with a given input and get a prediction\nseldon pipeline infer <pipelineName> (data) [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        inference payload file\n      --header stringArray      add a header, e.g. key=value; use the flag multiple times to add more than one header\n  -h, --help                    help for infer\n      --inference-host string   seldon inference host (default \"0.0.0.0:9000\")\n      --inference-mode string   inference mode (rest or grpc) (default \"rest\")\n  -i, --iterations int          how many times to run inference (default 1)\n  -t, --seconds int             number of secs to run inference\n      --show-headers            show request and response headers\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -s, --sticky-session          use sticky session from last inference (only works with experiments)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline_infer.html", "key": "cli/docs/seldon_pipeline_infer"}}, "getting-started/kubernetes-installation/security/strimzi-mtls": {"sections": {"strimzi-mtls-example": "\nStrimzi mTLS Example\u00b6\n\nCluster Setup\u00b6\nIf you have installed Strimzi we have an example Helm chart to create a Kafka cluster for seldon and an associated user in kafka/strimzi folder. Ensure the tls is enabled with:\nbroker:\n  tls:\n    enabled: true\n    port: 9093\n    listenerType: internal\n    authentication:\n      type: tls\n\n\nThe Ansible setup-ecosystem playbook will also install Strimzi and include a mTLS endpoint. See here.\n\n\nmTLS Example\u00b6\nCreate a Kafka User seldon in the namespace seldon was installed. This assumes Strimzi Kafka cluster is installed in the same namespace or is running with cluster wide permissions. Our Ansible scripts to setup the ecosystem will also create this user if tls is active.\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\n  name: seldon\n  labels:\n    strimzi.io/cluster: seldon\nspec:\n  authentication:\n    type: tls\n\n\nIf you don\u2019t have this user you can install it with in your desired namespace (here seldon-mesh):\nkubectl create -f k8s/samples/strimzi-example-tls-user.yaml -n seldon-mesh\n\n\nInstall seldon with the Strimzi certificate secrets using a custom values file. This sets the secret created by Strimzi for the user created above (seldon) and targets the server certificate authority secret from the name of the cluster created on install of the Kafka cluster (seldon-cluster-ca-cert).\nConfigure Seldon Core v2 by setting following Helm values:\n---\nkafka:\n  bootstrap: seldon-kafka-bootstrap.seldon-mesh.svc.cluster.local:9093\n\nsecurity:\n  kafka:\n    protocol: SSL\n    ssl:\n      client:\n        secret: seldon\n        brokerValidationSecret: seldon-cluster-ca-cert\n        keyPath: /tmp/certs/kafka/client/user.key\n        crtPath: /tmp/certs/kafka/client/user.crt\n        caPath: /tmp/certs/kafka/client/ca.crt\n        brokerCaPath: /tmp/certs/kafka/broker/ca.crt\n\n\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-strimzi-kafka-mtls.yaml\n\n\nYou can now go ahead and install a SeldonRuntime in your desired install namespace (here seldon-mesh), e.g.\nhelm install seldon-v2-runtime ../k8s/helm-charts/seldon-core-v2-runtime  -n seldon-mesh\n\n\n\n", "cluster-setup": "\nCluster Setup\u00b6\nIf you have installed Strimzi we have an example Helm chart to create a Kafka cluster for seldon and an associated user in kafka/strimzi folder. Ensure the tls is enabled with:\nbroker:\n  tls:\n    enabled: true\n    port: 9093\n    listenerType: internal\n    authentication:\n      type: tls\n\n\nThe Ansible setup-ecosystem playbook will also install Strimzi and include a mTLS endpoint. See here.\n", "mtls-example": "\nmTLS Example\u00b6\nCreate a Kafka User seldon in the namespace seldon was installed. This assumes Strimzi Kafka cluster is installed in the same namespace or is running with cluster wide permissions. Our Ansible scripts to setup the ecosystem will also create this user if tls is active.\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\n  name: seldon\n  labels:\n    strimzi.io/cluster: seldon\nspec:\n  authentication:\n    type: tls\n\n\nIf you don\u2019t have this user you can install it with in your desired namespace (here seldon-mesh):\nkubectl create -f k8s/samples/strimzi-example-tls-user.yaml -n seldon-mesh\n\n\nInstall seldon with the Strimzi certificate secrets using a custom values file. This sets the secret created by Strimzi for the user created above (seldon) and targets the server certificate authority secret from the name of the cluster created on install of the Kafka cluster (seldon-cluster-ca-cert).\nConfigure Seldon Core v2 by setting following Helm values:\n---\nkafka:\n  bootstrap: seldon-kafka-bootstrap.seldon-mesh.svc.cluster.local:9093\n\nsecurity:\n  kafka:\n    protocol: SSL\n    ssl:\n      client:\n        secret: seldon\n        brokerValidationSecret: seldon-cluster-ca-cert\n        keyPath: /tmp/certs/kafka/client/user.key\n        crtPath: /tmp/certs/kafka/client/user.crt\n        caPath: /tmp/certs/kafka/client/ca.crt\n        brokerCaPath: /tmp/certs/kafka/broker/ca.crt\n\n\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-strimzi-kafka-mtls.yaml\n\n\nYou can now go ahead and install a SeldonRuntime in your desired install namespace (here seldon-mesh), e.g.\nhelm install seldon-v2-runtime ../k8s/helm-charts/seldon-core-v2-runtime  -n seldon-mesh\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/strimzi-mtls.html", "key": "getting-started/kubernetes-installation/security/strimzi-mtls"}}, "cli/docs/seldon_model_load": {"sections": {"seldon-model-load": "\nseldon model load\u00b6\nload a model\n\nSynopsis\u00b6\nload a model\nseldon model load [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        model manifest file (YAML)\n  -h, --help                    help for load\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n\n", "synopsis": "\nSynopsis\u00b6\nload a model\nseldon model load [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        model manifest file (YAML)\n  -h, --help                    help for load\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_model_load.html", "key": "cli/docs/seldon_model_load"}}, "kubernetes/resources/experiment": {"sections": {"experiment": "\nExperiment\u00b6\nAn Experiment defines a traffic split between Models or Pipelines. This allows new versions of models and pipelines to be tested.\nAn experiment spec has three sections:\n\ncandidates (required) : a set of candidate models to split traffic.\ndefault (optional) : an existing candidate who endpoint should be modified to split traffic as defined by the candidates.\n\nEach candidate has a traffic weight. The percentage of traffic will be this weight divided by the sum of traffic weights.\n\n\nmirror (optional) : a single model to mirror traffic to the candidates. Responses from this model will not be returned to the caller.\n\nAn example experiment with a defaultModel is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nThis defines a split of 50% traffic between two models iris and iris2. In this case we want to expose this traffic split on the existing endpoint created for the iris model. This allows us to test new versions of models (in this case iris2) on an existing endpoint (in this case iris). The default key defines the model whose endpoint we want to change. The experiment will become active when both underplying models are in Ready status.\nAn experiment over two separate models which exposes a new API endpoint is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-iris\nspec:\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nTo call the endpoint add the header seldon-model: <experiment-name>.experiment in this case: seldon-model: experiment-iris.experiment. For example with curl:\ncurl http://${MESH_IP}/v2/models/experiment-iris/infer \\\n   -H \"Content-Type: application/json\" \\\n   -H \"seldon-model: experiment-iris.experiment\" \\\n   -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nFor examples see the local experiments notebook.\n\nPipeline Experiments\u00b6\nRunning an experiment between some pipelines is very similar. The difference is resourceType: pipeline needs to be defined and in this case the candidates or mirrors will refer to pipelines. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 50\n  - name: pipeline-mul10\n    weight: 50\n\n\nFor an example see the local experiments notebook.\n\n\nMirror Experiments\u00b6\nA mirror can be added easily for model or pipeline experiments. An example model mirror experiment is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: sklearn-mirror\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 100\n  mirror:\n    name: iris2\n    percent: 100\n\n\nFor an example see the local experiments notebook.\nAn example pipeline mirror experiment is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10-mirror\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 100\n  mirror:\n    name: pipeline-mul10\n    percent: 100\n\n\nFor an example see the local experiments notebook.\n\n\nSticky Sessions\u00b6\nTo allow cohorts to get consistent views in an experiment each inference request passes back a response header x-seldon-route which can be passed in future requests to an experiment to bypass the random traffic splits and get a prediction from the sequence of models and pipelines used in the initial request.\nNote: you must pass the normal seldon-model header along with the x-seldon-route header.\nThis is illustrated in the local experiments notebook.\nCaveats:\n\nNote the models used will be the same but not necessarily the same replica instances. This means at present this will not work for stateful models that need to go to the same model replica instance.\n\n\n\nService Meshes\u00b6\nAs an alternative you can choose to run experiments at the service mesh level if you use one of the popular service meshes that allow header based routing in traffic splits. For further discussion see here.\n\n", "pipeline-experiments": "\nPipeline Experiments\u00b6\nRunning an experiment between some pipelines is very similar. The difference is resourceType: pipeline needs to be defined and in this case the candidates or mirrors will refer to pipelines. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 50\n  - name: pipeline-mul10\n    weight: 50\n\n\nFor an example see the local experiments notebook.\n", "mirror-experiments": "\nMirror Experiments\u00b6\nA mirror can be added easily for model or pipeline experiments. An example model mirror experiment is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: sklearn-mirror\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 100\n  mirror:\n    name: iris2\n    percent: 100\n\n\nFor an example see the local experiments notebook.\nAn example pipeline mirror experiment is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10-mirror\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 100\n  mirror:\n    name: pipeline-mul10\n    percent: 100\n\n\nFor an example see the local experiments notebook.\n", "sticky-sessions": "\nSticky Sessions\u00b6\nTo allow cohorts to get consistent views in an experiment each inference request passes back a response header x-seldon-route which can be passed in future requests to an experiment to bypass the random traffic splits and get a prediction from the sequence of models and pipelines used in the initial request.\nNote: you must pass the normal seldon-model header along with the x-seldon-route header.\nThis is illustrated in the local experiments notebook.\nCaveats:\n\nNote the models used will be the same but not necessarily the same replica instances. This means at present this will not work for stateful models that need to go to the same model replica instance.\n\n", "service-meshes": "\nService Meshes\u00b6\nAs an alternative you can choose to run experiments at the service mesh level if you use one of the popular service meshes that allow header based routing in traffic splits. For further discussion see here.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/experiment/index.html", "key": "kubernetes/resources/experiment"}}, "getting-started/cli": {"sections": {"seldon-cli": "\nSeldon CLI\u00b6\nSeldon Core V2 can be managed via a CLI tool.\n\nDownload Linux Binary\u00b6\nDownload from a recent release from https://github.com/SeldonIO/seldon-core/releases.\nIt is dynamically linked and will require and *nix architecture and glibc 2.25+.\nmv seldon-linux-amd64 seldon\nchmod u+x seldon\n\n\nAdd to your PATH.\n\n\nLocal build (requires Go)\u00b6\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\ncd seldon-core/operator\nmake build-seldon\n\n\nAdd <project-root>/operator/bin to your PATH.\n\n\nLocal macOS ARM build (requires Go and librdkafka)\u00b6\n# install dependencies\nbrew install go librdkafka\n\n\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\ncd seldon-core/operator\nmake build-seldon-arm\n\n\nAdd <project-root>/operator/bin to your PATH.\n\n", "download-linux-binary": "\nDownload Linux Binary\u00b6\nDownload from a recent release from https://github.com/SeldonIO/seldon-core/releases.\nIt is dynamically linked and will require and *nix architecture and glibc 2.25+.\nmv seldon-linux-amd64 seldon\nchmod u+x seldon\n\n\nAdd to your PATH.\n", "local-build-requires-go": "\nLocal build (requires Go)\u00b6\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\ncd seldon-core/operator\nmake build-seldon\n\n\nAdd <project-root>/operator/bin to your PATH.\n", "local-macos-arm-build-requires-go-and-librdkafka": "\nLocal macOS ARM build (requires Go and librdkafka)\u00b6\n# install dependencies\nbrew install go librdkafka\n\n\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\ncd seldon-core/operator\nmake build-seldon-arm\n\n\nAdd <project-root>/operator/bin to your PATH.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/cli.html", "key": "getting-started/cli"}}, "development": {"sections": {"development": "\nDevelopment\u00b6\n\nDevelopment Requirements\u00b6\n\nGo 1.17+ with glibc development libraries installed.\n\ne.g. for Ubuntu systems: sudo apt-get install libc6-dev\n\n\nJava JDK 17+ and Kotlin 1.6.10+\nKubebuilder V2\nDocker and docker-compose\nHelm\nKustomize\nAnsible\n\n\n\nTesting resources\u00b6\n\nKind\nk6\nunix utils: jq, curl, grpcurl\n\n\n\nRelease Process\u00b6\nReleases are carried out via Github.\n\n\n\n", "development-requirements": "\nDevelopment Requirements\u00b6\n\nGo 1.17+ with glibc development libraries installed.\n\ne.g. for Ubuntu systems: sudo apt-get install libc6-dev\n\n\nJava JDK 17+ and Kotlin 1.6.10+\nKubebuilder V2\nDocker and docker-compose\nHelm\nKustomize\nAnsible\n\n", "testing-resources": "\nTesting resources\u00b6\n\nKind\nk6\nunix utils: jq, curl, grpcurl\n\n", "release-process": "\nRelease Process\u00b6\nReleases are carried out via Github.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/development/index.html", "key": "development"}}, "getting-started/kubernetes-installation/security/azure-event-hub-sasl": {"sections": {"azure-event-hub-sasl-example": "\nAzure Event Hub SASL Example\u00b6\n\nNew in Seldon Core 2.5.0\n\nSeldon Core v2 can integrate with Azure Event Hub via Kafka protocol.\n\nWarning\nYou will need at least Standard tier for your Event Hub Namespace as Basic tier does not support Kafka protocol.\n\n\nWarning\nSeldon Core v2 creates 2 Kafka topics for each pipeline and model plus one global topic for errors.\nThis means that total number of topics will be 2 x (#models + #pipelines) + 1 which will likely exceed the limit of Standard tier in Azure Event Hub.\nYou can find more information on quotas, like the number of partitions per Event Hub, here.\n\n\nPrerequisites\u00b6\nTo start you will need to have an Azure Event Hub Namespace.\nYou can create one following Azure quickstart docs.\nNote that you do not need to create an Event Hub (topics) as Core v2 will require all the topics it needs automatically.\n\n\nCreate API Keys\u00b6\nTo connect to Azure Event Hub provided Kafka API you need to obtain:\n\nKafka Endpoint\nConnection String\n\nYou can obtain both using Azure Portal as documented here.\n\nNote\nYou should get the Connection String for a namespace level as we will need to dynamically create new topics.\n\nThe Connection String should be in format of\nEndpoint=sb://<namespace>.servicebus.windows.net/;SharedAccessKeyName=XXXXXX;SharedAccessKey=XXXXXX\n\n\n\n\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic azure-kafka-secret -n seldon-mesh --from-literal password=\"Endpoint=sb://<namespace>.servicebus.windows.net/;SharedAccessKeyName=XXXXXX;SharedAccessKey=XXXXXX\"\n\n\n\n\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: <namespace>.servicebus.windows.net\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: \"PLAIN\"\n      client:\n        username: $ConnectionString\n        secret: azure-kafka-secret\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration. The username should read $ConnectionString and this is not a variable for you to replace.\n\n\nTroubleshooting\u00b6\n\nFirst check Azure Event Hub troubleshooting guide.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\nVerify that you did not hit quotas for topics or partitions in your Event Hub namespace.\n\n\n", "prerequisites": "\nPrerequisites\u00b6\nTo start you will need to have an Azure Event Hub Namespace.\nYou can create one following Azure quickstart docs.\nNote that you do not need to create an Event Hub (topics) as Core v2 will require all the topics it needs automatically.\n", "create-api-keys": "\nCreate API Keys\u00b6\nTo connect to Azure Event Hub provided Kafka API you need to obtain:\n\nKafka Endpoint\nConnection String\n\nYou can obtain both using Azure Portal as documented here.\n\nNote\nYou should get the Connection String for a namespace level as we will need to dynamically create new topics.\n\nThe Connection String should be in format of\nEndpoint=sb://<namespace>.servicebus.windows.net/;SharedAccessKeyName=XXXXXX;SharedAccessKey=XXXXXX\n\n\n", "create-kubernetes-secret": "\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic azure-kafka-secret -n seldon-mesh --from-literal password=\"Endpoint=sb://<namespace>.servicebus.windows.net/;SharedAccessKeyName=XXXXXX;SharedAccessKey=XXXXXX\"\n\n\n", "configure-seldon-core-v2": "\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: <namespace>.servicebus.windows.net\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: \"PLAIN\"\n      client:\n        username: $ConnectionString\n        secret: azure-kafka-secret\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration. The username should read $ConnectionString and this is not a variable for you to replace.\n", "troubleshooting": "\nTroubleshooting\u00b6\n\nFirst check Azure Event Hub troubleshooting guide.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\nVerify that you did not hit quotas for topics or partitions in your Event Hub namespace.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/azure-event-hub-sasl.html", "key": "getting-started/kubernetes-installation/security/azure-event-hub-sasl"}}, "kubernetes": {"sections": {"kubernetes": "\nKubernetes\u00b6\nKubernetes provides the core platform for production use. However, the architecture in agnostic to the underlying cluster technology as shown by the local Docker installation.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/index.html", "key": "kubernetes"}}, "cli/docs/seldon_config_activate": {"sections": {"seldon-config-activate": "\nseldon config activate\u00b6\nactivate config\n\nSynopsis\u00b6\nactivate config\nseldon config activate [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for activate\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n\n", "synopsis": "\nSynopsis\u00b6\nactivate config\nseldon config activate [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for activate\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config_activate.html", "key": "cli/docs/seldon_config_activate"}}, "pipelines": {"sections": {"pipelines": "\nPipelines\u00b6\nPipelines allow models to be connected into flows of data transformations. This allows more complex machine learning pipelines to be created with multiple models, feature transformations and monitoring components such as drift and outlier detectors.\n\nCreating Pipelines\u00b6\nThe simplest way to create Pipelines is by defining them with the Pipeline resource we provide for Kubernetes. This format is accepted by our Kubernetes implementation but also locally via our seldon CLI.\nInternally in both cases Pipelines are created via our Scheduler API. Advanced users could submit Pipelines directly using this gRPC service.\nAn example that chains two models together is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: chain\n  namespace: seldon-mesh\nspec:\n  steps:\n    - name: model1\n    - name: model2\n      inputs:\n      - model1\n  output:\n    steps:\n    - model2\n\n\n\nsteps allow you to specify the models you want to combine into a pipeline. Each step name will correspond to a model of the same name. These models will need to have been deployed and available for the Pipeline to function, however Pipelines can be deployed before or at the same time you deploy the underlying models.\nsteps.inputs allow you to specify the inputs to this step.\noutputs.steps allow you to specify the output of the Pipeline. A pipeline can have multiple paths include flows of data that do not reach the output, e.g. Drift detection steps. However, if you wish to call your Pipeline in a synchronous manner via REST/gRPC then an output must be present so the Pipeline can be treated as a function.\n\n\n\nExpressing input data sources\u00b6\nModel step inputs are defined with a dot notation of the form:\n<stepName>|<pipelineName>.<inputs|outputs>.<tensorName>\n\n\nInputs with just a step name will be assumed to be step.outputs.\nThe default payloads for Pipelines is the V2 protocol which requires named tensors as inputs and outputs from a model. If you require just certain tensors from a model you can reference those in the inputs, e.g. mymodel.outputs.t1 will reference the tensor t1 from the model mymodel.\nFor the specification of the V2 protocol.\n\n\nChain\u00b6\nThe simplest Pipeline chains models together: the output of one model goes into the input of the next. This will work out of the box if the output tensor names from a model match the input tensor names for the one being chained to. If they do not then the tensorMap construct presently needs to be used to define the mapping explicitly, e.g. see below for a simple chained pipeline of two tfsimple example models:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n\n      subgraph input\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      TF1 --->|OUTPUT0: INPUT0| TF2(tfsimple2)\n      TF1 --->|OUTPUT1: INPUT1| TF2\n\n      subgraph output\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n\n      TF2 --> OUTPUT0\n      TF2 --> OUTPUT1\n        \nA simple chain of two models.\u00b6\n\n\nIn the above we rename tensor OUTPUT0 to INPUT0 and OUTPUT1 to INPUT1. This allows these models to be chained together. The shape and data-type of the tensors needs to match as well.\nThis example can be found in the pipeline-examples examples.\n\n\nJoin\u00b6\nJoining allows us to combine outputs from multiple steps as input to a new step.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef hidden fill:#ffffff,stroke:#ffffff\n\n      subgraph input\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      INPUT0 --> TF2(tfsimple2)\n      INPUT1 --> TF2\n      TF1 -.-> |OUTPUT1| tf1( ):::hidden\n      TF1 --> |OUTPUT0: INPUT0| TF3(tfsimple3)\n      TF2 -.-> |OUTPUT0| tf2( ):::hidden\n      TF2 --> |OUTPUT1: INPUT1| TF3\n\n      subgraph output\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n      TF3 --> OUTPUT0\n      TF3 --> OUTPUT1\n        \nJoining the outputs of two models into a third model. The dashed lines signify model outputs that are not captured in the output of the pipeline.\u00b6\n\n\nHere we pass the pipeline inputs to two models and then take one output tensor from each and pass to the final model. We use the same tensorMap technique to rename tensors as disucssed in the previous section.\nJoins can have a join type which can be specified with inputsJoinType and can take the values:\n\ninner : require all inputs to be available to join.\nouter : wait for joinWindowMs to join any inputs. Ignoring any inputs that have not sent any data at that point. This will mean this step of the pipeline is guaranteed to have a latency of at least joinWindowMs.\nany : Wait for any of the specified data sources.\n\nThis example can be found in the pipeline-examples examples.\n\n\nConditional Logic\u00b6\nPipelines can create conditional flows via various methods. We will discuss each in turn.\n\nModel routing via tensors\u00b6\nThe simplest way is to create a model that outputs different named tensors based on its decision. This way downstream steps can be dependant on different expected tensors. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef join fill:#CEE741;\n\n      subgraph input\n      INPUT0:::pipeIO\n          CHOICE:::pipeIO\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      CHOICE --> conditional\n      INPUT0 --> conditional\n      INPUT1 --> conditional\n\n      conditional --> |OUTPUT0: INPUT| add10\n      conditional --> |OUTPUT1: INPUT| mul10\n\n      add10 --> |OUTPUT| any(any):::join\n      mul10 --> |OUTPUT| any\n\n      subgraph output\n          OUTPUT(OUTPUT):::pipeIO\n      end\n\n      any --> OUTPUT\n\n      linkStyle 3 stroke:blue,color:blue;\n      linkStyle 5 stroke:blue,color:blue;\n      linkStyle 4 stroke:red,color:red;\n      linkStyle 6 stroke:red,color:red;\n        \nPipeline with a conditional output model. The model conditional only outputs one of the two tensors, so only one path through the graph (red or blue) is taken by a single request\u00b6\n\n\nIn the above we have a step conditional that either outputs a tensor named OUTPUT0 or a tensor named OUTPUT1. The mul10 step depends on an output in OUTPUT0 while the add10 step depends on an output from OUTPUT1.\nNote, we also have a final Pipeline output step that does an any join on these two models essentially outputting fron the pipeline whichever data arrives from either model. This type of Pipeline can be used for Multi-Armed bandit solutions where you want to route traffic dynamically.\nThis example can be found in the pipeline-examples examples.\n\n\nErrors\u00b6\nIts also possible to abort pipelines when an error is produced to in effect create a condition. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: error\nspec:\n  steps:\n    - name: outlier-error\n  output:\n    steps:\n    - outlier-error\n\n\nThis Pipeline runs normally or throws an error based on whether the input tensors have certain values.\nThis example can be found in the pipeline-examples examples.\n\n\nTriggers\u00b6\nSometimes you want to run a step if an output is received from a previous step but not to send the data from that step to the model. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: joincheck\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: check\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n      triggers:\n      - check.outputs.OUTPUT\n  output:\n    steps:\n    - tfsimple3\n\n\n\n\n              flowchart LR\n    classDef pipeIO fill:#F6E083\n    classDef hidden fill:#ffffff,stroke:#ffffff\n\n    subgraph input\n        INPUT0:::pipeIO\n        INPUT1:::pipeIO\n    end\n\n    INPUT0 --> TF1(tfsimple1)\n    INPUT1 --> TF1\n    INPUT0 --> TF2(tfsimple2)\n    INPUT1 --> TF2\n    TF1 -.-> |OUTPUT1| tf1( ):::hidden\n\n    TF1 --> |OUTPUT0: INPUT| check\n    TF1 --> |OUTPUT0: INPUT0| TF3(tfsimple3)\n    TF2 --> |OUTPUT1: INPUT1| TF3\n    TF2 -.-> |OUTPUT0| tf2( ):::hidden\n\n    check --o |OUTPUT| TF3\n    linkStyle 9 stroke:#CEE741,color:black;\n\n    subgraph output\n      OUTPUT0:::pipeIO\n      OUTPUT1:::pipeIO\n    end\n\n    TF3 --> OUTPUT0\n    TF3 --> OUTPUT1\n        \nA pipeline with a single trigger. The model tfsimple3 only runs if the model check returns a tensor named OUTPUT. The green edge signifies that this is a trigger and not an additional input to tfsimple3. The dashed lines signify model outputs that are not captured in the output of the pipeline.\u00b6\n\n\nIn this example the last step tfsimple3 runs only if there are outputs from tfsimple1 and tfsimple2 but also data from the check step. However, if the step tfsimple3 is run it only receives the join of data from tfsimple1 and tfsimple2.\nThis example can be found in the pipeline-examples examples.\n\n\nTrigger Joins\u00b6\nYou can also define multiple triggers which need to happen based on a particulr join type. For example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef pipeIOopt fill:#F6E083,stroke-dasharray: 5 5;\n      classDef trigger fill:#CEE741;\n      classDef hidden fill:#ffffff,stroke:#ffffff\n      classDef join fill:#CEE741;\n\n      subgraph input\n          ok1:::pipeIOopt\n          ok2:::pipeIOopt\n          INPUT:::pipeIO\n          ok3:::pipeIOopt\n      end\n\n      ok1 --o any\n      ok2 --o any\n      any((any)):::trigger --o mul10\n      linkStyle 0 stroke:#CEE741,color:green;\n      linkStyle 1 stroke:#CEE741,color:green;\n      linkStyle 2 stroke:#CEE741,color:green;\n\n      INPUT --> mul10\n      INPUT --> add10\n\n      ok3 --o add10\n      linkStyle 5 stroke:#CEE741,color:green;\n\n      subgraph output\n        OUTPUT:::pipeIO\n      end\n\n      mul10 -->|OUTPUT| anyOut(any):::join\n      add10 --> |OUTPUT| anyOut\n      anyOut --> OUTPUT\n        \nA pipeline with multiple triggers and a trigger join of type any. The pipeline has four inputs, but three of these are optional (signified by the dashed borders).\u00b6\n\n\nHere the mul10 step is run if data is seen on the pipeline inputs in the ok1 or ok2 tensors based on the any join type. If data is seen on ok3 then the add10 step is run.\nIf we changed the triggersJoinType for mul10 to inner then both ok1 and ok2 would need to appear before mul10 is run.\n\n\nPipeline Inputs\u00b6\nPipelines by default can be accessed synchronously via http/grpc or asynchronously via the Kafka topic created for them. However, it\u2019s also possible to create a pipeline to take input from one or more other pipelines by specifying an input section. If for example we already have the tfsimple pipeline shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nWe can create another pipeline which takes its input from this pipeline, as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n\n      subgraph tfsimple.inputs\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n      \n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      \n      subgraph tfsimple.outputs\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n      \n      TF1 --> OUTPUT0\n      TF1 --> OUTPUT1\n\n      subgraph tfsimple-extended.inputs\n        INPUT10(INPUT0):::pipeIO\n        INPUT11(INPUT1):::pipeIO\n      end\n\n    OUTPUT0 --> INPUT10\n    OUTPUT1 --> INPUT11\n\n    INPUT10 --> TF2(tfsimple2)\n    INPUT11 --> TF2\n\n      subgraph tfsimple-extended.outputs\n          OUTPUT10(OUTPUT0):::pipeIO\n          OUTPUT11(OUTPUT1):::pipeIO\n      end\n      \n      TF2 --> OUTPUT10\n      TF2 --> OUTPUT11\n        \nA pipeline taking as input the output of another pipeline.\u00b6\n\n\nIn this way pipelines can be built to extend existing running pipelines to allow extensibility and sharing of data flows.\nThe spec follows the same spec for a step except that references to other pipelines are contained in the externalInputs section which takes the form of pipeline or pipeline.step references:\n\n<pipelineName>.(inputs|outputs).<tensorName>\n<pipelineName>.(step).<stepName>.<tensorName>\n\nTensor names are optional and only needed if you want to take just one tensor from an input or output.\nThere is also an externalTriggers section which allows triggers from other pipelines.\nFurther examples can be found in the pipeline-to-pipeline examples.\nPresent caveats:\n\nCircular dependencies are not presently detected.\nPipeline status is local to each pipeline.\n\n\n\n\nData Centric Implementation\u00b6\nInternally Pipelines are implemented using Kafka. Each input and output to a pipeline step has an associated Kafka topic. This has many advantages and allows auditing, replay and debugging easier as data is preserved from every step in your pipeline.\nTracing allows you to monitor the processing latency of your pipelines.\n\nAs each request to a pipelines moves through the steps its data will appear in input and output topics. This allows a full audit of every transformation to be carried out.\n\n", "creating-pipelines": "\nCreating Pipelines\u00b6\nThe simplest way to create Pipelines is by defining them with the Pipeline resource we provide for Kubernetes. This format is accepted by our Kubernetes implementation but also locally via our seldon CLI.\nInternally in both cases Pipelines are created via our Scheduler API. Advanced users could submit Pipelines directly using this gRPC service.\nAn example that chains two models together is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: chain\n  namespace: seldon-mesh\nspec:\n  steps:\n    - name: model1\n    - name: model2\n      inputs:\n      - model1\n  output:\n    steps:\n    - model2\n\n\n\nsteps allow you to specify the models you want to combine into a pipeline. Each step name will correspond to a model of the same name. These models will need to have been deployed and available for the Pipeline to function, however Pipelines can be deployed before or at the same time you deploy the underlying models.\nsteps.inputs allow you to specify the inputs to this step.\noutputs.steps allow you to specify the output of the Pipeline. A pipeline can have multiple paths include flows of data that do not reach the output, e.g. Drift detection steps. However, if you wish to call your Pipeline in a synchronous manner via REST/gRPC then an output must be present so the Pipeline can be treated as a function.\n\n", "expressing-input-data-sources": "\nExpressing input data sources\u00b6\nModel step inputs are defined with a dot notation of the form:\n<stepName>|<pipelineName>.<inputs|outputs>.<tensorName>\n\n\nInputs with just a step name will be assumed to be step.outputs.\nThe default payloads for Pipelines is the V2 protocol which requires named tensors as inputs and outputs from a model. If you require just certain tensors from a model you can reference those in the inputs, e.g. mymodel.outputs.t1 will reference the tensor t1 from the model mymodel.\nFor the specification of the V2 protocol.\n", "chain": "\nChain\u00b6\nThe simplest Pipeline chains models together: the output of one model goes into the input of the next. This will work out of the box if the output tensor names from a model match the input tensor names for the one being chained to. If they do not then the tensorMap construct presently needs to be used to define the mapping explicitly, e.g. see below for a simple chained pipeline of two tfsimple example models:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n\n      subgraph input\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      TF1 --->|OUTPUT0: INPUT0| TF2(tfsimple2)\n      TF1 --->|OUTPUT1: INPUT1| TF2\n\n      subgraph output\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n\n      TF2 --> OUTPUT0\n      TF2 --> OUTPUT1\n        \nA simple chain of two models.\u00b6\n\n\nIn the above we rename tensor OUTPUT0 to INPUT0 and OUTPUT1 to INPUT1. This allows these models to be chained together. The shape and data-type of the tensors needs to match as well.\nThis example can be found in the pipeline-examples examples.\n", "join": "\nJoin\u00b6\nJoining allows us to combine outputs from multiple steps as input to a new step.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef hidden fill:#ffffff,stroke:#ffffff\n\n      subgraph input\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      INPUT0 --> TF2(tfsimple2)\n      INPUT1 --> TF2\n      TF1 -.-> |OUTPUT1| tf1( ):::hidden\n      TF1 --> |OUTPUT0: INPUT0| TF3(tfsimple3)\n      TF2 -.-> |OUTPUT0| tf2( ):::hidden\n      TF2 --> |OUTPUT1: INPUT1| TF3\n\n      subgraph output\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n      TF3 --> OUTPUT0\n      TF3 --> OUTPUT1\n        \nJoining the outputs of two models into a third model. The dashed lines signify model outputs that are not captured in the output of the pipeline.\u00b6\n\n\nHere we pass the pipeline inputs to two models and then take one output tensor from each and pass to the final model. We use the same tensorMap technique to rename tensors as disucssed in the previous section.\nJoins can have a join type which can be specified with inputsJoinType and can take the values:\n\ninner : require all inputs to be available to join.\nouter : wait for joinWindowMs to join any inputs. Ignoring any inputs that have not sent any data at that point. This will mean this step of the pipeline is guaranteed to have a latency of at least joinWindowMs.\nany : Wait for any of the specified data sources.\n\nThis example can be found in the pipeline-examples examples.\n", "conditional-logic": "\nConditional Logic\u00b6\nPipelines can create conditional flows via various methods. We will discuss each in turn.\n\nModel routing via tensors\u00b6\nThe simplest way is to create a model that outputs different named tensors based on its decision. This way downstream steps can be dependant on different expected tensors. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef join fill:#CEE741;\n\n      subgraph input\n      INPUT0:::pipeIO\n          CHOICE:::pipeIO\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      CHOICE --> conditional\n      INPUT0 --> conditional\n      INPUT1 --> conditional\n\n      conditional --> |OUTPUT0: INPUT| add10\n      conditional --> |OUTPUT1: INPUT| mul10\n\n      add10 --> |OUTPUT| any(any):::join\n      mul10 --> |OUTPUT| any\n\n      subgraph output\n          OUTPUT(OUTPUT):::pipeIO\n      end\n\n      any --> OUTPUT\n\n      linkStyle 3 stroke:blue,color:blue;\n      linkStyle 5 stroke:blue,color:blue;\n      linkStyle 4 stroke:red,color:red;\n      linkStyle 6 stroke:red,color:red;\n        \nPipeline with a conditional output model. The model conditional only outputs one of the two tensors, so only one path through the graph (red or blue) is taken by a single request\u00b6\n\n\nIn the above we have a step conditional that either outputs a tensor named OUTPUT0 or a tensor named OUTPUT1. The mul10 step depends on an output in OUTPUT0 while the add10 step depends on an output from OUTPUT1.\nNote, we also have a final Pipeline output step that does an any join on these two models essentially outputting fron the pipeline whichever data arrives from either model. This type of Pipeline can be used for Multi-Armed bandit solutions where you want to route traffic dynamically.\nThis example can be found in the pipeline-examples examples.\n\n\nErrors\u00b6\nIts also possible to abort pipelines when an error is produced to in effect create a condition. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: error\nspec:\n  steps:\n    - name: outlier-error\n  output:\n    steps:\n    - outlier-error\n\n\nThis Pipeline runs normally or throws an error based on whether the input tensors have certain values.\nThis example can be found in the pipeline-examples examples.\n\n\nTriggers\u00b6\nSometimes you want to run a step if an output is received from a previous step but not to send the data from that step to the model. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: joincheck\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: check\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n      triggers:\n      - check.outputs.OUTPUT\n  output:\n    steps:\n    - tfsimple3\n\n\n\n\n              flowchart LR\n    classDef pipeIO fill:#F6E083\n    classDef hidden fill:#ffffff,stroke:#ffffff\n\n    subgraph input\n        INPUT0:::pipeIO\n        INPUT1:::pipeIO\n    end\n\n    INPUT0 --> TF1(tfsimple1)\n    INPUT1 --> TF1\n    INPUT0 --> TF2(tfsimple2)\n    INPUT1 --> TF2\n    TF1 -.-> |OUTPUT1| tf1( ):::hidden\n\n    TF1 --> |OUTPUT0: INPUT| check\n    TF1 --> |OUTPUT0: INPUT0| TF3(tfsimple3)\n    TF2 --> |OUTPUT1: INPUT1| TF3\n    TF2 -.-> |OUTPUT0| tf2( ):::hidden\n\n    check --o |OUTPUT| TF3\n    linkStyle 9 stroke:#CEE741,color:black;\n\n    subgraph output\n      OUTPUT0:::pipeIO\n      OUTPUT1:::pipeIO\n    end\n\n    TF3 --> OUTPUT0\n    TF3 --> OUTPUT1\n        \nA pipeline with a single trigger. The model tfsimple3 only runs if the model check returns a tensor named OUTPUT. The green edge signifies that this is a trigger and not an additional input to tfsimple3. The dashed lines signify model outputs that are not captured in the output of the pipeline.\u00b6\n\n\nIn this example the last step tfsimple3 runs only if there are outputs from tfsimple1 and tfsimple2 but also data from the check step. However, if the step tfsimple3 is run it only receives the join of data from tfsimple1 and tfsimple2.\nThis example can be found in the pipeline-examples examples.\n\n\nTrigger Joins\u00b6\nYou can also define multiple triggers which need to happen based on a particulr join type. For example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef pipeIOopt fill:#F6E083,stroke-dasharray: 5 5;\n      classDef trigger fill:#CEE741;\n      classDef hidden fill:#ffffff,stroke:#ffffff\n      classDef join fill:#CEE741;\n\n      subgraph input\n          ok1:::pipeIOopt\n          ok2:::pipeIOopt\n          INPUT:::pipeIO\n          ok3:::pipeIOopt\n      end\n\n      ok1 --o any\n      ok2 --o any\n      any((any)):::trigger --o mul10\n      linkStyle 0 stroke:#CEE741,color:green;\n      linkStyle 1 stroke:#CEE741,color:green;\n      linkStyle 2 stroke:#CEE741,color:green;\n\n      INPUT --> mul10\n      INPUT --> add10\n\n      ok3 --o add10\n      linkStyle 5 stroke:#CEE741,color:green;\n\n      subgraph output\n        OUTPUT:::pipeIO\n      end\n\n      mul10 -->|OUTPUT| anyOut(any):::join\n      add10 --> |OUTPUT| anyOut\n      anyOut --> OUTPUT\n        \nA pipeline with multiple triggers and a trigger join of type any. The pipeline has four inputs, but three of these are optional (signified by the dashed borders).\u00b6\n\n\nHere the mul10 step is run if data is seen on the pipeline inputs in the ok1 or ok2 tensors based on the any join type. If data is seen on ok3 then the add10 step is run.\nIf we changed the triggersJoinType for mul10 to inner then both ok1 and ok2 would need to appear before mul10 is run.\n\n\nPipeline Inputs\u00b6\nPipelines by default can be accessed synchronously via http/grpc or asynchronously via the Kafka topic created for them. However, it\u2019s also possible to create a pipeline to take input from one or more other pipelines by specifying an input section. If for example we already have the tfsimple pipeline shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nWe can create another pipeline which takes its input from this pipeline, as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n\n      subgraph tfsimple.inputs\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n      \n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      \n      subgraph tfsimple.outputs\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n      \n      TF1 --> OUTPUT0\n      TF1 --> OUTPUT1\n\n      subgraph tfsimple-extended.inputs\n        INPUT10(INPUT0):::pipeIO\n        INPUT11(INPUT1):::pipeIO\n      end\n\n    OUTPUT0 --> INPUT10\n    OUTPUT1 --> INPUT11\n\n    INPUT10 --> TF2(tfsimple2)\n    INPUT11 --> TF2\n\n      subgraph tfsimple-extended.outputs\n          OUTPUT10(OUTPUT0):::pipeIO\n          OUTPUT11(OUTPUT1):::pipeIO\n      end\n      \n      TF2 --> OUTPUT10\n      TF2 --> OUTPUT11\n        \nA pipeline taking as input the output of another pipeline.\u00b6\n\n\nIn this way pipelines can be built to extend existing running pipelines to allow extensibility and sharing of data flows.\nThe spec follows the same spec for a step except that references to other pipelines are contained in the externalInputs section which takes the form of pipeline or pipeline.step references:\n\n<pipelineName>.(inputs|outputs).<tensorName>\n<pipelineName>.(step).<stepName>.<tensorName>\n\nTensor names are optional and only needed if you want to take just one tensor from an input or output.\nThere is also an externalTriggers section which allows triggers from other pipelines.\nFurther examples can be found in the pipeline-to-pipeline examples.\nPresent caveats:\n\nCircular dependencies are not presently detected.\nPipeline status is local to each pipeline.\n\n\n", "model-routing-via-tensors": "\nModel routing via tensors\u00b6\nThe simplest way is to create a model that outputs different named tensors based on its decision. This way downstream steps can be dependant on different expected tensors. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef join fill:#CEE741;\n\n      subgraph input\n      INPUT0:::pipeIO\n          CHOICE:::pipeIO\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n\n      CHOICE --> conditional\n      INPUT0 --> conditional\n      INPUT1 --> conditional\n\n      conditional --> |OUTPUT0: INPUT| add10\n      conditional --> |OUTPUT1: INPUT| mul10\n\n      add10 --> |OUTPUT| any(any):::join\n      mul10 --> |OUTPUT| any\n\n      subgraph output\n          OUTPUT(OUTPUT):::pipeIO\n      end\n\n      any --> OUTPUT\n\n      linkStyle 3 stroke:blue,color:blue;\n      linkStyle 5 stroke:blue,color:blue;\n      linkStyle 4 stroke:red,color:red;\n      linkStyle 6 stroke:red,color:red;\n        \nPipeline with a conditional output model. The model conditional only outputs one of the two tensors, so only one path through the graph (red or blue) is taken by a single request\u00b6\n\n\nIn the above we have a step conditional that either outputs a tensor named OUTPUT0 or a tensor named OUTPUT1. The mul10 step depends on an output in OUTPUT0 while the add10 step depends on an output from OUTPUT1.\nNote, we also have a final Pipeline output step that does an any join on these two models essentially outputting fron the pipeline whichever data arrives from either model. This type of Pipeline can be used for Multi-Armed bandit solutions where you want to route traffic dynamically.\nThis example can be found in the pipeline-examples examples.\n", "errors": "\nErrors\u00b6\nIts also possible to abort pipelines when an error is produced to in effect create a condition. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: error\nspec:\n  steps:\n    - name: outlier-error\n  output:\n    steps:\n    - outlier-error\n\n\nThis Pipeline runs normally or throws an error based on whether the input tensors have certain values.\nThis example can be found in the pipeline-examples examples.\n", "triggers": "\nTriggers\u00b6\nSometimes you want to run a step if an output is received from a previous step but not to send the data from that step to the model. This is illustrated below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: joincheck\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: check\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n      triggers:\n      - check.outputs.OUTPUT\n  output:\n    steps:\n    - tfsimple3\n\n\n\n\n              flowchart LR\n    classDef pipeIO fill:#F6E083\n    classDef hidden fill:#ffffff,stroke:#ffffff\n\n    subgraph input\n        INPUT0:::pipeIO\n        INPUT1:::pipeIO\n    end\n\n    INPUT0 --> TF1(tfsimple1)\n    INPUT1 --> TF1\n    INPUT0 --> TF2(tfsimple2)\n    INPUT1 --> TF2\n    TF1 -.-> |OUTPUT1| tf1( ):::hidden\n\n    TF1 --> |OUTPUT0: INPUT| check\n    TF1 --> |OUTPUT0: INPUT0| TF3(tfsimple3)\n    TF2 --> |OUTPUT1: INPUT1| TF3\n    TF2 -.-> |OUTPUT0| tf2( ):::hidden\n\n    check --o |OUTPUT| TF3\n    linkStyle 9 stroke:#CEE741,color:black;\n\n    subgraph output\n      OUTPUT0:::pipeIO\n      OUTPUT1:::pipeIO\n    end\n\n    TF3 --> OUTPUT0\n    TF3 --> OUTPUT1\n        \nA pipeline with a single trigger. The model tfsimple3 only runs if the model check returns a tensor named OUTPUT. The green edge signifies that this is a trigger and not an additional input to tfsimple3. The dashed lines signify model outputs that are not captured in the output of the pipeline.\u00b6\n\n\nIn this example the last step tfsimple3 runs only if there are outputs from tfsimple1 and tfsimple2 but also data from the check step. However, if the step tfsimple3 is run it only receives the join of data from tfsimple1 and tfsimple2.\nThis example can be found in the pipeline-examples examples.\n", "trigger-joins": "\nTrigger Joins\u00b6\nYou can also define multiple triggers which need to happen based on a particulr join type. For example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n      classDef pipeIOopt fill:#F6E083,stroke-dasharray: 5 5;\n      classDef trigger fill:#CEE741;\n      classDef hidden fill:#ffffff,stroke:#ffffff\n      classDef join fill:#CEE741;\n\n      subgraph input\n          ok1:::pipeIOopt\n          ok2:::pipeIOopt\n          INPUT:::pipeIO\n          ok3:::pipeIOopt\n      end\n\n      ok1 --o any\n      ok2 --o any\n      any((any)):::trigger --o mul10\n      linkStyle 0 stroke:#CEE741,color:green;\n      linkStyle 1 stroke:#CEE741,color:green;\n      linkStyle 2 stroke:#CEE741,color:green;\n\n      INPUT --> mul10\n      INPUT --> add10\n\n      ok3 --o add10\n      linkStyle 5 stroke:#CEE741,color:green;\n\n      subgraph output\n        OUTPUT:::pipeIO\n      end\n\n      mul10 -->|OUTPUT| anyOut(any):::join\n      add10 --> |OUTPUT| anyOut\n      anyOut --> OUTPUT\n        \nA pipeline with multiple triggers and a trigger join of type any. The pipeline has four inputs, but three of these are optional (signified by the dashed borders).\u00b6\n\n\nHere the mul10 step is run if data is seen on the pipeline inputs in the ok1 or ok2 tensors based on the any join type. If data is seen on ok3 then the add10 step is run.\nIf we changed the triggersJoinType for mul10 to inner then both ok1 and ok2 would need to appear before mul10 is run.\n", "pipeline-inputs": "\nPipeline Inputs\u00b6\nPipelines by default can be accessed synchronously via http/grpc or asynchronously via the Kafka topic created for them. However, it\u2019s also possible to create a pipeline to take input from one or more other pipelines by specifying an input section. If for example we already have the tfsimple pipeline shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nWe can create another pipeline which takes its input from this pipeline, as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\n\n\n              flowchart LR\n      classDef pipeIO fill:#F6E083\n\n      subgraph tfsimple.inputs\n          INPUT0:::pipeIO\n          INPUT1:::pipeIO\n      end\n      \n      INPUT0 --> TF1(tfsimple1)\n      INPUT1 --> TF1\n      \n      subgraph tfsimple.outputs\n          OUTPUT0:::pipeIO\n          OUTPUT1:::pipeIO\n      end\n      \n      TF1 --> OUTPUT0\n      TF1 --> OUTPUT1\n\n      subgraph tfsimple-extended.inputs\n        INPUT10(INPUT0):::pipeIO\n        INPUT11(INPUT1):::pipeIO\n      end\n\n    OUTPUT0 --> INPUT10\n    OUTPUT1 --> INPUT11\n\n    INPUT10 --> TF2(tfsimple2)\n    INPUT11 --> TF2\n\n      subgraph tfsimple-extended.outputs\n          OUTPUT10(OUTPUT0):::pipeIO\n          OUTPUT11(OUTPUT1):::pipeIO\n      end\n      \n      TF2 --> OUTPUT10\n      TF2 --> OUTPUT11\n        \nA pipeline taking as input the output of another pipeline.\u00b6\n\n\nIn this way pipelines can be built to extend existing running pipelines to allow extensibility and sharing of data flows.\nThe spec follows the same spec for a step except that references to other pipelines are contained in the externalInputs section which takes the form of pipeline or pipeline.step references:\n\n<pipelineName>.(inputs|outputs).<tensorName>\n<pipelineName>.(step).<stepName>.<tensorName>\n\nTensor names are optional and only needed if you want to take just one tensor from an input or output.\nThere is also an externalTriggers section which allows triggers from other pipelines.\nFurther examples can be found in the pipeline-to-pipeline examples.\nPresent caveats:\n\nCircular dependencies are not presently detected.\nPipeline status is local to each pipeline.\n\n", "data-centric-implementation": "\nData Centric Implementation\u00b6\nInternally Pipelines are implemented using Kafka. Each input and output to a pipeline step has an associated Kafka topic. This has many advantages and allows auditing, replay and debugging easier as data is preserved from every step in your pipeline.\nTracing allows you to monitor the processing latency of your pipelines.\n\nAs each request to a pipelines moves through the steps its data will appear in input and output topics. This allows a full audit of every transformation to be carried out.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/pipelines/index.html", "key": "pipelines"}}, "kubernetes/service-meshes/traefik": {"sections": {"traefik": "\nTraefik\u00b6\nTraefik provides a service mesh and ingress solution.\nWe will run through some examples as shown in the notebook service-meshes/traefik/traefik.ipynb\n\nSingle Model\u00b6\n\nA Seldon Iris Model\nTraefik Service\nTraefik IngressRoute\nTraefik Middleware for adding a header\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapps\n  namespace: seldon-mesh\nspec:\n  ports:\n  - name: web\n    port: 80\n    protocol: TCP\n  selector:\n    app: traefik-ingress-lb\n  type: LoadBalancer\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  entryPoints:\n  - web\n  routes:\n  - kind: Rule\n    match: PathPrefix(`/`)\n    middlewares:\n    - name: iris-header\n    services:\n    - name: seldon-mesh\n      port: 80\n      scheme: h2c\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: iris-header\n  namespace: seldon-mesh\nspec:\n  headers:\n    customRequestHeaders:\n      seldon-model: iris\n\n\n\n\nTraffic Split\u00b6\n\nWarning\nTraffic splitting does not presently work due to this issue. We recommend you use a Seldon Experiment instead.\n\n\n\nTraefik Examples\u00b6\nAssumes\n\nYou have installed Traefik as per their docs into namespace traefik-v2\n\nTested with traefik-10.19.4\nINGRESS_IP=!kubectl get svc traefik -n traefik-v2 -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nTraefik Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapps\n  namespace: seldon-mesh\nspec:\n  ports:\n  - name: web\n    port: 80\n    protocol: TCP\n  selector:\n    app: traefik-ingress-lb\n  type: LoadBalancer\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  entryPoints:\n  - web\n  routes:\n  - kind: Rule\n    match: PathPrefix(`/`)\n    middlewares:\n    - name: iris-header\n    services:\n    - name: seldon-mesh\n      port: 80\n      scheme: h2c\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: iris-header\n  namespace: seldon-mesh\nspec:\n  headers:\n    customRequestHeaders:\n      seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nservice/myapps created\nmodel.mlops.seldon.io/iris created\ningressroute.traefik.containo.us/iris created\nmiddleware.traefik.containo.us/iris-header created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< Content-Length: 196\n< Content-Type: application/json\n< Date: Sat, 16 Apr 2022 15:53:27 GMT\n< Seldon-Route: iris_1\n< Server: envoy\n< X-Envoy-Upstream-Service-Time: 895\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"0dccf477-78fa-4a11-92ff-4d7e4f1cdda8\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nservice \"myapps\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\ningressroute.traefik.containo.us \"iris\" deleted\nmiddleware.traefik.containo.us \"iris-header\" deleted\n\n\n\n\n\n\n\n", "single-model": "\nSingle Model\u00b6\n\nA Seldon Iris Model\nTraefik Service\nTraefik IngressRoute\nTraefik Middleware for adding a header\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapps\n  namespace: seldon-mesh\nspec:\n  ports:\n  - name: web\n    port: 80\n    protocol: TCP\n  selector:\n    app: traefik-ingress-lb\n  type: LoadBalancer\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  entryPoints:\n  - web\n  routes:\n  - kind: Rule\n    match: PathPrefix(`/`)\n    middlewares:\n    - name: iris-header\n    services:\n    - name: seldon-mesh\n      port: 80\n      scheme: h2c\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: iris-header\n  namespace: seldon-mesh\nspec:\n  headers:\n    customRequestHeaders:\n      seldon-model: iris\n\n\n", "traffic-split": "\nTraffic Split\u00b6\n\nWarning\nTraffic splitting does not presently work due to this issue. We recommend you use a Seldon Experiment instead.\n\n", "traefik-examples": "\nTraefik Examples\u00b6\nAssumes\n\nYou have installed Traefik as per their docs into namespace traefik-v2\n\nTested with traefik-10.19.4\nINGRESS_IP=!kubectl get svc traefik -n traefik-v2 -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nTraefik Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapps\n  namespace: seldon-mesh\nspec:\n  ports:\n  - name: web\n    port: 80\n    protocol: TCP\n  selector:\n    app: traefik-ingress-lb\n  type: LoadBalancer\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  entryPoints:\n  - web\n  routes:\n  - kind: Rule\n    match: PathPrefix(`/`)\n    middlewares:\n    - name: iris-header\n    services:\n    - name: seldon-mesh\n      port: 80\n      scheme: h2c\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: iris-header\n  namespace: seldon-mesh\nspec:\n  headers:\n    customRequestHeaders:\n      seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nservice/myapps created\nmodel.mlops.seldon.io/iris created\ningressroute.traefik.containo.us/iris created\nmiddleware.traefik.containo.us/iris-header created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< Content-Length: 196\n< Content-Type: application/json\n< Date: Sat, 16 Apr 2022 15:53:27 GMT\n< Seldon-Route: iris_1\n< Server: envoy\n< X-Envoy-Upstream-Service-Time: 895\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"0dccf477-78fa-4a11-92ff-4d7e4f1cdda8\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nservice \"myapps\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\ningressroute.traefik.containo.us \"iris\" deleted\nmiddleware.traefik.containo.us \"iris-header\" deleted\n\n\n\n\n\n\n", "traefik-single-model-example": "\nTraefik Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapps\n  namespace: seldon-mesh\nspec:\n  ports:\n  - name: web\n    port: 80\n    protocol: TCP\n  selector:\n    app: traefik-ingress-lb\n  type: LoadBalancer\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  entryPoints:\n  - web\n  routes:\n  - kind: Rule\n    match: PathPrefix(`/`)\n    middlewares:\n    - name: iris-header\n    services:\n    - name: seldon-mesh\n      port: 80\n      scheme: h2c\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: iris-header\n  namespace: seldon-mesh\nspec:\n  headers:\n    customRequestHeaders:\n      seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nservice/myapps created\nmodel.mlops.seldon.io/iris created\ningressroute.traefik.containo.us/iris created\nmiddleware.traefik.containo.us/iris-header created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< Content-Length: 196\n< Content-Type: application/json\n< Date: Sat, 16 Apr 2022 15:53:27 GMT\n< Seldon-Route: iris_1\n< Server: envoy\n< X-Envoy-Upstream-Service-Time: 895\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"0dccf477-78fa-4a11-92ff-4d7e4f1cdda8\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nservice \"myapps\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\ningressroute.traefik.containo.us \"iris\" deleted\nmiddleware.traefik.containo.us \"iris-header\" deleted\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/service-meshes/traefik/index.html", "key": "kubernetes/service-meshes/traefik"}}, "getting-started/docker-installation": {"sections": {"docker-installation": "\nDocker Installation\u00b6\n\nPreparation\u00b6\n\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\nBuild Seldon CLI\nInstall Docker Compose (or directly from GitHub release if not using Docker Desktop).\nInstall make. This will depend on your version of Linux, for example on Ubuntu run sudo apt-get install build-essential.\n\n\n\nDeploy\u00b6\nFrom the project root run:\nmake deploy-local\n\n\nThis will run with latest images for the components.\nNote: Triton and MLServer are large images at present (11G and 9G respectively) so will take time to download on first usage.\n\nRun a particular version\u00b6\nTo run a particular release set the environment variable CUSTOM_IMAGE_TAG to the desired version before running the command, e.g.:\nexport CUSTOM_IMAGE_TAG=0.2.0\nmake deploy-local\n\n\n\n\nGPU support\u00b6\nTo enable GPU on servers:\n\nMake sure that nvidia-container-runtime is installed, follow link\nEnable GPU: export GPU_ENABLED=1\n\n\n\nLocal Models\u00b6\nTo deploy with a local folder available for loading models set the environment variable LOCAL_MODEL_FOLDER to the folder, e.g.:\nexport LOCAL_MODEL_FOLDER=/home/seldon/models\nmake deploy-local\n\n\nThis folder will be mounted at /mnt/models. You can then specify models as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/mnt/models/iris\"\n  requirements:\n  - sklearn\n\n\nIf you have set the local model folder as above then this would be looking at /home/seldon/models/iris.\n\n\n\nTracing\u00b6\nThe default local install will provide Jaeger tracing at http://0.0.0.0:16686/search.\n\n\nMetrics\u00b6\nThe default local install will expose Grafana at http://localhost:3000.\n\n\nUndeploy\u00b6\nFrom the project root run:\nmake undeploy-local\n\n\n\n\n\n", "preparation": "\nPreparation\u00b6\n\ngit clone https://github.com/SeldonIO/seldon-core --branch=v2\nBuild Seldon CLI\nInstall Docker Compose (or directly from GitHub release if not using Docker Desktop).\nInstall make. This will depend on your version of Linux, for example on Ubuntu run sudo apt-get install build-essential.\n\n", "deploy": "\nDeploy\u00b6\nFrom the project root run:\nmake deploy-local\n\n\nThis will run with latest images for the components.\nNote: Triton and MLServer are large images at present (11G and 9G respectively) so will take time to download on first usage.\n\nRun a particular version\u00b6\nTo run a particular release set the environment variable CUSTOM_IMAGE_TAG to the desired version before running the command, e.g.:\nexport CUSTOM_IMAGE_TAG=0.2.0\nmake deploy-local\n\n\n\n\nGPU support\u00b6\nTo enable GPU on servers:\n\nMake sure that nvidia-container-runtime is installed, follow link\nEnable GPU: export GPU_ENABLED=1\n\n\n\nLocal Models\u00b6\nTo deploy with a local folder available for loading models set the environment variable LOCAL_MODEL_FOLDER to the folder, e.g.:\nexport LOCAL_MODEL_FOLDER=/home/seldon/models\nmake deploy-local\n\n\nThis folder will be mounted at /mnt/models. You can then specify models as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/mnt/models/iris\"\n  requirements:\n  - sklearn\n\n\nIf you have set the local model folder as above then this would be looking at /home/seldon/models/iris.\n\n", "run-a-particular-version": "\nRun a particular version\u00b6\nTo run a particular release set the environment variable CUSTOM_IMAGE_TAG to the desired version before running the command, e.g.:\nexport CUSTOM_IMAGE_TAG=0.2.0\nmake deploy-local\n\n\n", "gpu-support": "\nGPU support\u00b6\nTo enable GPU on servers:\n\nMake sure that nvidia-container-runtime is installed, follow link\nEnable GPU: export GPU_ENABLED=1\n\n", "local-models": "\nLocal Models\u00b6\nTo deploy with a local folder available for loading models set the environment variable LOCAL_MODEL_FOLDER to the folder, e.g.:\nexport LOCAL_MODEL_FOLDER=/home/seldon/models\nmake deploy-local\n\n\nThis folder will be mounted at /mnt/models. You can then specify models as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/mnt/models/iris\"\n  requirements:\n  - sklearn\n\n\nIf you have set the local model folder as above then this would be looking at /home/seldon/models/iris.\n", "tracing": "\nTracing\u00b6\nThe default local install will provide Jaeger tracing at http://0.0.0.0:16686/search.\n", "metrics": "\nMetrics\u00b6\nThe default local install will expose Grafana at http://localhost:3000.\n", "undeploy": "\nUndeploy\u00b6\nFrom the project root run:\nmake undeploy-local\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/docker-installation/index.html", "key": "getting-started/docker-installation"}}, "getting-started/kubernetes-installation/helm": {"sections": {"helm-installation": "\nHelm Installation\u00b6\nWe provide several Helm charts.\n\nseldon-core-v2-crds : cluster wide install of custom resources.\nseldon-core-v2-setup : installation of the manager to manage resources in the namespace or clusterwide. This also installs default SeldonConfig and ServerConfig resources which allow Runtimes and Servers to be installed easily on demand.\nseldon-core-v2-runtime : this installs a SeldonRuntime custom resource which creates the core components in a namespace.\nseldon-core-v2-servers : this installs Server custom resources which provide example core servers to load models.\nseldon-core-v2-certs : a default set of certificates for TLS.\n\nThe Helm charts can be found within the k8s/helm-charts folder and they are published here\nAssuming you have installed any ecosystem components: Jaeger, Prometheus, Kafka as discussed here you can follow the\nfollowing steps.\nNote that for Kafka follow the steps discussed here\n\nAdd Seldon Core v2 Charts\u00b6\nhelm repo add seldon-charts https://seldonio.github.io/helm-charts\nhelm repo update seldon-charts\n\n\n\n\nInstall the CRDs\u00b6\nhelm install seldon-core-v2-crds  seldon-charts/seldon-core-v2-crds\n\n\n\n\nInstall the Seldon Core V2 Components\u00b6\nYou can install into any namespace. For illustration we will use seldon-mesh. This will install the core manager which will handle the key resources  used by Seldon including the SeldonRuntime and Server resources.\nkubectl create namespace seldon-mesh\n\n\nhelm install seldon-core-v2  seldon-charts/seldon-core-v2-setup --namespace seldon-mesh\n\n\nThis will install the operator namespaced so it will only control resources in the provided namespace. To allow cluster wide usage add the --set controller.clusterwide=true, e.g.\nhelm install seldon-core-v2  seldon-charts/seldon-core-v2-setup --namespace seldon-mesh --set controller.clusterwide=true\n\n\nCluster wide operations will require ClusterRoles to be created so when deploying be aware your user will require the required permissions. With cluster wide operations you can create SeldonRuntimes in any namespace.\n\n\nInstall the default Seldon Core V2 Runtime\u00b6\nhelm install seldon-v2-runtime seldon-charts/seldon-core-v2-runtime --namespace seldon-mesh\n\n\nThis will install the core components in your desired namespace.\n\n\nInstall example servers\u00b6\nTo install some MLServer and Triton servers you can either create Server resources yourself or for initial testing you can use our example Helm chart seldon-core-v2-servers:\nhelm install seldon-v2-servers seldon-charts/seldon-core-v2-servers --namespace seldon-mesh\n\n\nBy default this will install 1 MLServer and 1 Triton in the desired namespace. This namespace should be the same namespace you installed a Seldon Core Runtime.\n\n\nUninstall\u00b6\nRemove any models, pipelines that are running.\nRemove the runtime:\nhelm uninstall seldon-core-v2-runtime  --namespace seldon-mesh\n\n\nRemove the core components:\nhelm uninstall seldon-core-v2  --namespace seldon-mesh\n\n\nRemove the CRDs\nhelm uninstall seldon-core-v2-crds\n\n\n\n", "add-seldon-core-v2-charts": "\nAdd Seldon Core v2 Charts\u00b6\nhelm repo add seldon-charts https://seldonio.github.io/helm-charts\nhelm repo update seldon-charts\n\n\n", "install-the-crds": "\nInstall the CRDs\u00b6\nhelm install seldon-core-v2-crds  seldon-charts/seldon-core-v2-crds\n\n\n", "install-the-seldon-core-v2-components": "\nInstall the Seldon Core V2 Components\u00b6\nYou can install into any namespace. For illustration we will use seldon-mesh. This will install the core manager which will handle the key resources  used by Seldon including the SeldonRuntime and Server resources.\nkubectl create namespace seldon-mesh\n\n\nhelm install seldon-core-v2  seldon-charts/seldon-core-v2-setup --namespace seldon-mesh\n\n\nThis will install the operator namespaced so it will only control resources in the provided namespace. To allow cluster wide usage add the --set controller.clusterwide=true, e.g.\nhelm install seldon-core-v2  seldon-charts/seldon-core-v2-setup --namespace seldon-mesh --set controller.clusterwide=true\n\n\nCluster wide operations will require ClusterRoles to be created so when deploying be aware your user will require the required permissions. With cluster wide operations you can create SeldonRuntimes in any namespace.\n", "install-the-default-seldon-core-v2-runtime": "\nInstall the default Seldon Core V2 Runtime\u00b6\nhelm install seldon-v2-runtime seldon-charts/seldon-core-v2-runtime --namespace seldon-mesh\n\n\nThis will install the core components in your desired namespace.\n", "install-example-servers": "\nInstall example servers\u00b6\nTo install some MLServer and Triton servers you can either create Server resources yourself or for initial testing you can use our example Helm chart seldon-core-v2-servers:\nhelm install seldon-v2-servers seldon-charts/seldon-core-v2-servers --namespace seldon-mesh\n\n\nBy default this will install 1 MLServer and 1 Triton in the desired namespace. This namespace should be the same namespace you installed a Seldon Core Runtime.\n", "uninstall": "\nUninstall\u00b6\nRemove any models, pipelines that are running.\nRemove the runtime:\nhelm uninstall seldon-core-v2-runtime  --namespace seldon-mesh\n\n\nRemove the core components:\nhelm uninstall seldon-core-v2  --namespace seldon-mesh\n\n\nRemove the CRDs\nhelm uninstall seldon-core-v2-crds\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/helm.html", "key": "getting-started/kubernetes-installation/helm"}}, "getting-started/kubernetes-installation/ansible": {"sections": {"ansible-installation": "\nAnsible Installation\u00b6\n\nWarning\nThe Ansible installation of a Seldon Core and associated ecosystem is meant for dev/testing purposes.\nFor production use cases follow Helm installation.\n\n\nInstalling Ansible\u00b6\nProvided Ansible playbooks and roles depends on kubernetes.core Ansible Collection for performing kubectl and helm operations.\nCheck Ansible [documentation] for further information.\nTo install Ansible and required collections\npip install ansible openshift kubernetes docker\nansible-galaxy collection install kubernetes.core\n\n\nWe have tested provided instructions on Python 3.8 - 3.11 with following version of Python libraries\n\n\nPython\nAnsible\nDocker\nKubernetes\n\n\n\n3.8\n6.7.0\n6.0.1\n26.1.0\n\n3.9\n7.2.0\n6.0.1\n26.1.0\n\n3.10\n7.2.0\n6.0.1\n26.1.0\n\n3.11\n7.2.0\n6.0.1\n26.1.0\n\n\n\nand kubernetes.core collection in version 2.4.0.\nOnce installed you can use the following Playbooks that you will find in Ansible folder of Seldon Core V2 repository.\nYou also need to have installed kubectl CLI.\n\n\nInstalling Seldon Core v2 using Ansible\u00b6\n\nOne-liner local kind install (from scratch)\u00b6\nIf you simply want to install into a fresh local kind\nk8s cluster, the seldon-all playbook allows you to do so with a single command:\nansible-playbook playbooks/seldon-all.yaml\n\n\nThis will create a Kind cluster and install ecosystem dependencies (kafka,\nprometheus, opentelemetry, jager) as well as all the seldon-specific components.\nThe seldon components are installed using helm-charts from the current git\ncheckout (../k8s/helm-charts/).\nInternally this runs, in order, the following playbooks (described in more detail\nin the sections below):\n\nkind-cluster.yaml\nsetup-ecosystem.yaml\nsetup-seldon.yaml\n\nYou may pass any of the additonal variables which are configurable for those playbooks\nto seldon-all. See the Customizing Ansible Instalation section for details.\nFor example:\nansible-playbook playbooks/seldon-all.yaml -e seldon_mesh_namespace=my-seldon-mesh -e install_prometheus=no -e @playbooks/vars/set-custom-images.yaml\n\n\nRunning the playbooks individually, as described in the sections below, will give you\nmore control over what gets run and when (for example, if you want to install into an\nexisting k8s cluster).\n\n\nCreate Kind Cluster\u00b6\nIt is recommended to first install Seldon Core v2 inside Kind cluster.\nThis allow to test and trial the installation in isolated environment that is easy to remove.\nansible-playbook playbooks/kind-cluster.yaml\n\n\n\n\nSetup Ecosystem\u00b6\nSeldon runs by default in seldon-mesh namespace and a Jaeger pod and  and OpenTelemetry collector are installed in the chosen namespace. Run the following:\nansible-playbook playbooks/setup-ecosystem.yaml\n\n\nThe most common change will be to install in another namespace with:\nansible-playbook playbooks/setup-ecosystem.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n\n\nInstall Seldon Core v2\u00b6\nRun the following from the ansible/ folder:\nansible-playbook playbooks/setup-seldon.yaml\n\n\nIf you have changed the namespace you wish to use you will need to run with:\nansible-playbook playbooks/setup-seldon.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n\n\n\nCustomizing Ansible Installation\u00b6\n\nEcosystem configuration options\u00b6\nThe ecosystem setup can be parametrized by providing extra Ansible variables, e.g. using -e flag to ansible-playbook command.\nFor example run the following from the ansible/ folder:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\nwill only install Kafka when setting up the ecosystem.\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon Core v2\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka Cluster for Core v2\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_kafka\nbool\n{{ full_install }}\ninstalls Strimzi Kafka Operator\n\ninstall_prometheus\nbool\n{{ full_install }}\ninstalls Prometheus Operator\n\ninstall_certmanager\nbool\n{{ full_install }}\ninstalls Cert Manager\n\ninstall_jaeger\nbool\n{{ full_install }}\ninstalls Jaeger\n\ninstall_opentelemetry\nbool\n{{ full_install }}\ninstalls OpenTelemetry\n\nconfigure_kafka\nbool\n{{ install_kafka }}\nconfigures Kafka Cluster for Core v2\n\nconfigure_prometheus\nbool\n{{ install_prometheus }}\nconfigure Prometheus using Core v2 specific resources\n\nconfigure_jaeger\nbool\n{{ install_jaeger }}\nconfigure Jaeger using Core v2 specific resources\n\nconfigure_opentelemetry\nbool\n{{ install_opentelemetry }}\nconfigure OpenTelemetry using Core v2 specific resources\n\n\n\n\n\nSeldon Core v2 configuration options\u00b6\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon\n\nseldon_crds_namespace\nstring\ndefault\nnamespace to install Seldon CRDs\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_crds\nbool\n{{ full_install }}\ninstalls Seldon CRDs\n\ninstall_components\nbool\n{{ full_install }}\ninstall Seldon components\n\ninstall_servers\nbool\n{{ full_install }}\ninstall Seldon servers\n\n\n\n\nCustom Seldon images and private registries\u00b6\nBy default, the container images used in the install are the ones defined by the helm\ncharts (referring to images publicly available on dockerhub).\nIf you need to customize the images (i.e pull from private registry, pull given\ntag), create a custom images config file following the example in\nplaybooks/vars/set-custom-images.yaml and run with:\nansible-playbook playbooks/setup-seldon.yaml -e @<path-to-custom-images-config.yaml>\n\n\n\nPrivate registries\u00b6\nWhen using private registries, access needs to be authenticated (typically, via a\nservice account key), and the k8s cluster will need to have access to a secret holding\nthis key to be able to pull images.\nThe setup-seldon.yaml playbook will create the required k8s secrets inside the\ncluster if it is provided with an auth file in dockerconfigjson format. You provide\nthe path to this file by cusomizing the custom_image_pull_secrets.dockerconfigjson\nvariable and define the secret name via custom_image_pull_secrets.name in the custom\nimages config file (the one passed to the playbook via -e @file).\nBy default, docker creates the dockerconfigjson auth file in ~/.docker/config.json\nafter passing the service-account key to docker login.\nThe docker login command would look like this (key in json format):\ncat registry-sa-key.json | docker login -u _json_key --password-stdin <registry-url>\n\n\nor, for keys in base64 format:\ncat registry-sa-key | docker login -u _json_key_base64 --password-stdin <registry-url>\n\n\n\n\nSaving helm-chart customisations\u00b6\nBecause the additional custom images config file (starting from the\nplaybooks/vars/set-custom-images.yaml example) overrides values in the helm-charts\navailable in the repo, there\u2019s also a playbook option of saving those overrides as a\nseparate values file, which could be used if deploying manually via helm.\nThis is controlled via two variables:\n\n\n\ntype\ndefault\ncomment\n\n\n\nsave_helm_components_overrides\nbool\nfalse\nenable saving helm values overrides\n\nsave_helm_components_overrides_file\nstring\n~/seldon_helm_comp_values.yaml\npath/filename for saving overrides\n\n\n\nYou can either pass those within the custom images config file or directly when running the\nplaybook. For example, for just saving the helm-chart overrides (without installing seldon\ncomponents), you would run:\nansible-playbook playbooks/setup-seldon.yaml -e full_install=no -e save_helm_components_overrides=yes -e @<path-to-custom-images-config.yaml>\n\n\nPlease note that when deploying outside ansible via helm using this saved overrides file,\nand using private registries, you will have to manually create the service-account key\nsecret with the same name as the one defined in your custom image config file under\ncustom_image_pull_secrets.name.\n\n\n\n\n\nUninstall\u00b6\nTo fully remove the Ansible installation delete the created Kind cluster\nkind delete cluster --name seldon\n\n\nThis will stop and delete the Kind cluster freeing all of the resources taken by the dev/trial installation.\nYou may want to also remove cache resources used for the installation with\nrm -rf ~/.cache/seldon/\n\n\n\nNote\nIf you used Ansible to install Seldon Core v2 and its ecosystem into K8s cluster other than Kind you need to manually remove all the components.\nNotes on how to remove Seldon Core v2 Helm installation itself you can find here.\n\n\n", "installing-ansible": "\nInstalling Ansible\u00b6\nProvided Ansible playbooks and roles depends on kubernetes.core Ansible Collection for performing kubectl and helm operations.\nCheck Ansible [documentation] for further information.\nTo install Ansible and required collections\npip install ansible openshift kubernetes docker\nansible-galaxy collection install kubernetes.core\n\n\nWe have tested provided instructions on Python 3.8 - 3.11 with following version of Python libraries\n\n\nPython\nAnsible\nDocker\nKubernetes\n\n\n\n3.8\n6.7.0\n6.0.1\n26.1.0\n\n3.9\n7.2.0\n6.0.1\n26.1.0\n\n3.10\n7.2.0\n6.0.1\n26.1.0\n\n3.11\n7.2.0\n6.0.1\n26.1.0\n\n\n\nand kubernetes.core collection in version 2.4.0.\nOnce installed you can use the following Playbooks that you will find in Ansible folder of Seldon Core V2 repository.\nYou also need to have installed kubectl CLI.\n", "installing-seldon-core-v2-using-ansible": "\nInstalling Seldon Core v2 using Ansible\u00b6\n\nOne-liner local kind install (from scratch)\u00b6\nIf you simply want to install into a fresh local kind\nk8s cluster, the seldon-all playbook allows you to do so with a single command:\nansible-playbook playbooks/seldon-all.yaml\n\n\nThis will create a Kind cluster and install ecosystem dependencies (kafka,\nprometheus, opentelemetry, jager) as well as all the seldon-specific components.\nThe seldon components are installed using helm-charts from the current git\ncheckout (../k8s/helm-charts/).\nInternally this runs, in order, the following playbooks (described in more detail\nin the sections below):\n\nkind-cluster.yaml\nsetup-ecosystem.yaml\nsetup-seldon.yaml\n\nYou may pass any of the additonal variables which are configurable for those playbooks\nto seldon-all. See the Customizing Ansible Instalation section for details.\nFor example:\nansible-playbook playbooks/seldon-all.yaml -e seldon_mesh_namespace=my-seldon-mesh -e install_prometheus=no -e @playbooks/vars/set-custom-images.yaml\n\n\nRunning the playbooks individually, as described in the sections below, will give you\nmore control over what gets run and when (for example, if you want to install into an\nexisting k8s cluster).\n\n\nCreate Kind Cluster\u00b6\nIt is recommended to first install Seldon Core v2 inside Kind cluster.\nThis allow to test and trial the installation in isolated environment that is easy to remove.\nansible-playbook playbooks/kind-cluster.yaml\n\n\n\n\nSetup Ecosystem\u00b6\nSeldon runs by default in seldon-mesh namespace and a Jaeger pod and  and OpenTelemetry collector are installed in the chosen namespace. Run the following:\nansible-playbook playbooks/setup-ecosystem.yaml\n\n\nThe most common change will be to install in another namespace with:\nansible-playbook playbooks/setup-ecosystem.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n\n\nInstall Seldon Core v2\u00b6\nRun the following from the ansible/ folder:\nansible-playbook playbooks/setup-seldon.yaml\n\n\nIf you have changed the namespace you wish to use you will need to run with:\nansible-playbook playbooks/setup-seldon.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n\n", "one-liner-local-kind-install-from-scratch": "\nOne-liner local kind install (from scratch)\u00b6\nIf you simply want to install into a fresh local kind\nk8s cluster, the seldon-all playbook allows you to do so with a single command:\nansible-playbook playbooks/seldon-all.yaml\n\n\nThis will create a Kind cluster and install ecosystem dependencies (kafka,\nprometheus, opentelemetry, jager) as well as all the seldon-specific components.\nThe seldon components are installed using helm-charts from the current git\ncheckout (../k8s/helm-charts/).\nInternally this runs, in order, the following playbooks (described in more detail\nin the sections below):\n\nkind-cluster.yaml\nsetup-ecosystem.yaml\nsetup-seldon.yaml\n\nYou may pass any of the additonal variables which are configurable for those playbooks\nto seldon-all. See the Customizing Ansible Instalation section for details.\nFor example:\nansible-playbook playbooks/seldon-all.yaml -e seldon_mesh_namespace=my-seldon-mesh -e install_prometheus=no -e @playbooks/vars/set-custom-images.yaml\n\n\nRunning the playbooks individually, as described in the sections below, will give you\nmore control over what gets run and when (for example, if you want to install into an\nexisting k8s cluster).\n", "create-kind-cluster": "\nCreate Kind Cluster\u00b6\nIt is recommended to first install Seldon Core v2 inside Kind cluster.\nThis allow to test and trial the installation in isolated environment that is easy to remove.\nansible-playbook playbooks/kind-cluster.yaml\n\n\n", "setup-ecosystem": "\nSetup Ecosystem\u00b6\nSeldon runs by default in seldon-mesh namespace and a Jaeger pod and  and OpenTelemetry collector are installed in the chosen namespace. Run the following:\nansible-playbook playbooks/setup-ecosystem.yaml\n\n\nThe most common change will be to install in another namespace with:\nansible-playbook playbooks/setup-ecosystem.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n", "install-seldon-core-v2": "\nInstall Seldon Core v2\u00b6\nRun the following from the ansible/ folder:\nansible-playbook playbooks/setup-seldon.yaml\n\n\nIf you have changed the namespace you wish to use you will need to run with:\nansible-playbook playbooks/setup-seldon.yaml -e seldon_mesh_namespace=<mynamespace>\n\n\n", "customizing-ansible-installation": "\nCustomizing Ansible Installation\u00b6\n\nEcosystem configuration options\u00b6\nThe ecosystem setup can be parametrized by providing extra Ansible variables, e.g. using -e flag to ansible-playbook command.\nFor example run the following from the ansible/ folder:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\nwill only install Kafka when setting up the ecosystem.\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon Core v2\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka Cluster for Core v2\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_kafka\nbool\n{{ full_install }}\ninstalls Strimzi Kafka Operator\n\ninstall_prometheus\nbool\n{{ full_install }}\ninstalls Prometheus Operator\n\ninstall_certmanager\nbool\n{{ full_install }}\ninstalls Cert Manager\n\ninstall_jaeger\nbool\n{{ full_install }}\ninstalls Jaeger\n\ninstall_opentelemetry\nbool\n{{ full_install }}\ninstalls OpenTelemetry\n\nconfigure_kafka\nbool\n{{ install_kafka }}\nconfigures Kafka Cluster for Core v2\n\nconfigure_prometheus\nbool\n{{ install_prometheus }}\nconfigure Prometheus using Core v2 specific resources\n\nconfigure_jaeger\nbool\n{{ install_jaeger }}\nconfigure Jaeger using Core v2 specific resources\n\nconfigure_opentelemetry\nbool\n{{ install_opentelemetry }}\nconfigure OpenTelemetry using Core v2 specific resources\n\n\n\n\n\nSeldon Core v2 configuration options\u00b6\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon\n\nseldon_crds_namespace\nstring\ndefault\nnamespace to install Seldon CRDs\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_crds\nbool\n{{ full_install }}\ninstalls Seldon CRDs\n\ninstall_components\nbool\n{{ full_install }}\ninstall Seldon components\n\ninstall_servers\nbool\n{{ full_install }}\ninstall Seldon servers\n\n\n\n\nCustom Seldon images and private registries\u00b6\nBy default, the container images used in the install are the ones defined by the helm\ncharts (referring to images publicly available on dockerhub).\nIf you need to customize the images (i.e pull from private registry, pull given\ntag), create a custom images config file following the example in\nplaybooks/vars/set-custom-images.yaml and run with:\nansible-playbook playbooks/setup-seldon.yaml -e @<path-to-custom-images-config.yaml>\n\n\n\nPrivate registries\u00b6\nWhen using private registries, access needs to be authenticated (typically, via a\nservice account key), and the k8s cluster will need to have access to a secret holding\nthis key to be able to pull images.\nThe setup-seldon.yaml playbook will create the required k8s secrets inside the\ncluster if it is provided with an auth file in dockerconfigjson format. You provide\nthe path to this file by cusomizing the custom_image_pull_secrets.dockerconfigjson\nvariable and define the secret name via custom_image_pull_secrets.name in the custom\nimages config file (the one passed to the playbook via -e @file).\nBy default, docker creates the dockerconfigjson auth file in ~/.docker/config.json\nafter passing the service-account key to docker login.\nThe docker login command would look like this (key in json format):\ncat registry-sa-key.json | docker login -u _json_key --password-stdin <registry-url>\n\n\nor, for keys in base64 format:\ncat registry-sa-key | docker login -u _json_key_base64 --password-stdin <registry-url>\n\n\n\n\nSaving helm-chart customisations\u00b6\nBecause the additional custom images config file (starting from the\nplaybooks/vars/set-custom-images.yaml example) overrides values in the helm-charts\navailable in the repo, there\u2019s also a playbook option of saving those overrides as a\nseparate values file, which could be used if deploying manually via helm.\nThis is controlled via two variables:\n\n\n\ntype\ndefault\ncomment\n\n\n\nsave_helm_components_overrides\nbool\nfalse\nenable saving helm values overrides\n\nsave_helm_components_overrides_file\nstring\n~/seldon_helm_comp_values.yaml\npath/filename for saving overrides\n\n\n\nYou can either pass those within the custom images config file or directly when running the\nplaybook. For example, for just saving the helm-chart overrides (without installing seldon\ncomponents), you would run:\nansible-playbook playbooks/setup-seldon.yaml -e full_install=no -e save_helm_components_overrides=yes -e @<path-to-custom-images-config.yaml>\n\n\nPlease note that when deploying outside ansible via helm using this saved overrides file,\nand using private registries, you will have to manually create the service-account key\nsecret with the same name as the one defined in your custom image config file under\ncustom_image_pull_secrets.name.\n\n\n\n", "ecosystem-configuration-options": "\nEcosystem configuration options\u00b6\nThe ecosystem setup can be parametrized by providing extra Ansible variables, e.g. using -e flag to ansible-playbook command.\nFor example run the following from the ansible/ folder:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\nwill only install Kafka when setting up the ecosystem.\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon Core v2\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka Cluster for Core v2\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_kafka\nbool\n{{ full_install }}\ninstalls Strimzi Kafka Operator\n\ninstall_prometheus\nbool\n{{ full_install }}\ninstalls Prometheus Operator\n\ninstall_certmanager\nbool\n{{ full_install }}\ninstalls Cert Manager\n\ninstall_jaeger\nbool\n{{ full_install }}\ninstalls Jaeger\n\ninstall_opentelemetry\nbool\n{{ full_install }}\ninstalls OpenTelemetry\n\nconfigure_kafka\nbool\n{{ install_kafka }}\nconfigures Kafka Cluster for Core v2\n\nconfigure_prometheus\nbool\n{{ install_prometheus }}\nconfigure Prometheus using Core v2 specific resources\n\nconfigure_jaeger\nbool\n{{ install_jaeger }}\nconfigure Jaeger using Core v2 specific resources\n\nconfigure_opentelemetry\nbool\n{{ install_opentelemetry }}\nconfigure OpenTelemetry using Core v2 specific resources\n\n\n\n", "seldon-core-v2-configuration-options": "\nSeldon Core v2 configuration options\u00b6\n\n\n\ntype\ndefault\ncomment\n\n\n\nseldon_kafka_namespace\nstring\nseldon-mesh\nnamespace to install Kafka\n\nseldon_mesh_namespace\nstring\nseldon-mesh\nnamespace to install Seldon\n\nseldon_crds_namespace\nstring\ndefault\nnamespace to install Seldon CRDs\n\nfull_install\nbool\nyes\nenables full ecosystem installation\n\ninstall_crds\nbool\n{{ full_install }}\ninstalls Seldon CRDs\n\ninstall_components\nbool\n{{ full_install }}\ninstall Seldon components\n\ninstall_servers\nbool\n{{ full_install }}\ninstall Seldon servers\n\n\n\n\nCustom Seldon images and private registries\u00b6\nBy default, the container images used in the install are the ones defined by the helm\ncharts (referring to images publicly available on dockerhub).\nIf you need to customize the images (i.e pull from private registry, pull given\ntag), create a custom images config file following the example in\nplaybooks/vars/set-custom-images.yaml and run with:\nansible-playbook playbooks/setup-seldon.yaml -e @<path-to-custom-images-config.yaml>\n\n\n\nPrivate registries\u00b6\nWhen using private registries, access needs to be authenticated (typically, via a\nservice account key), and the k8s cluster will need to have access to a secret holding\nthis key to be able to pull images.\nThe setup-seldon.yaml playbook will create the required k8s secrets inside the\ncluster if it is provided with an auth file in dockerconfigjson format. You provide\nthe path to this file by cusomizing the custom_image_pull_secrets.dockerconfigjson\nvariable and define the secret name via custom_image_pull_secrets.name in the custom\nimages config file (the one passed to the playbook via -e @file).\nBy default, docker creates the dockerconfigjson auth file in ~/.docker/config.json\nafter passing the service-account key to docker login.\nThe docker login command would look like this (key in json format):\ncat registry-sa-key.json | docker login -u _json_key --password-stdin <registry-url>\n\n\nor, for keys in base64 format:\ncat registry-sa-key | docker login -u _json_key_base64 --password-stdin <registry-url>\n\n\n\n\nSaving helm-chart customisations\u00b6\nBecause the additional custom images config file (starting from the\nplaybooks/vars/set-custom-images.yaml example) overrides values in the helm-charts\navailable in the repo, there\u2019s also a playbook option of saving those overrides as a\nseparate values file, which could be used if deploying manually via helm.\nThis is controlled via two variables:\n\n\n\ntype\ndefault\ncomment\n\n\n\nsave_helm_components_overrides\nbool\nfalse\nenable saving helm values overrides\n\nsave_helm_components_overrides_file\nstring\n~/seldon_helm_comp_values.yaml\npath/filename for saving overrides\n\n\n\nYou can either pass those within the custom images config file or directly when running the\nplaybook. For example, for just saving the helm-chart overrides (without installing seldon\ncomponents), you would run:\nansible-playbook playbooks/setup-seldon.yaml -e full_install=no -e save_helm_components_overrides=yes -e @<path-to-custom-images-config.yaml>\n\n\nPlease note that when deploying outside ansible via helm using this saved overrides file,\nand using private registries, you will have to manually create the service-account key\nsecret with the same name as the one defined in your custom image config file under\ncustom_image_pull_secrets.name.\n\n\n", "custom-seldon-images-and-private-registries": "\nCustom Seldon images and private registries\u00b6\nBy default, the container images used in the install are the ones defined by the helm\ncharts (referring to images publicly available on dockerhub).\nIf you need to customize the images (i.e pull from private registry, pull given\ntag), create a custom images config file following the example in\nplaybooks/vars/set-custom-images.yaml and run with:\nansible-playbook playbooks/setup-seldon.yaml -e @<path-to-custom-images-config.yaml>\n\n\n\nPrivate registries\u00b6\nWhen using private registries, access needs to be authenticated (typically, via a\nservice account key), and the k8s cluster will need to have access to a secret holding\nthis key to be able to pull images.\nThe setup-seldon.yaml playbook will create the required k8s secrets inside the\ncluster if it is provided with an auth file in dockerconfigjson format. You provide\nthe path to this file by cusomizing the custom_image_pull_secrets.dockerconfigjson\nvariable and define the secret name via custom_image_pull_secrets.name in the custom\nimages config file (the one passed to the playbook via -e @file).\nBy default, docker creates the dockerconfigjson auth file in ~/.docker/config.json\nafter passing the service-account key to docker login.\nThe docker login command would look like this (key in json format):\ncat registry-sa-key.json | docker login -u _json_key --password-stdin <registry-url>\n\n\nor, for keys in base64 format:\ncat registry-sa-key | docker login -u _json_key_base64 --password-stdin <registry-url>\n\n\n\n\nSaving helm-chart customisations\u00b6\nBecause the additional custom images config file (starting from the\nplaybooks/vars/set-custom-images.yaml example) overrides values in the helm-charts\navailable in the repo, there\u2019s also a playbook option of saving those overrides as a\nseparate values file, which could be used if deploying manually via helm.\nThis is controlled via two variables:\n\n\n\ntype\ndefault\ncomment\n\n\n\nsave_helm_components_overrides\nbool\nfalse\nenable saving helm values overrides\n\nsave_helm_components_overrides_file\nstring\n~/seldon_helm_comp_values.yaml\npath/filename for saving overrides\n\n\n\nYou can either pass those within the custom images config file or directly when running the\nplaybook. For example, for just saving the helm-chart overrides (without installing seldon\ncomponents), you would run:\nansible-playbook playbooks/setup-seldon.yaml -e full_install=no -e save_helm_components_overrides=yes -e @<path-to-custom-images-config.yaml>\n\n\nPlease note that when deploying outside ansible via helm using this saved overrides file,\nand using private registries, you will have to manually create the service-account key\nsecret with the same name as the one defined in your custom image config file under\ncustom_image_pull_secrets.name.\n\n", "private-registries": "\nPrivate registries\u00b6\nWhen using private registries, access needs to be authenticated (typically, via a\nservice account key), and the k8s cluster will need to have access to a secret holding\nthis key to be able to pull images.\nThe setup-seldon.yaml playbook will create the required k8s secrets inside the\ncluster if it is provided with an auth file in dockerconfigjson format. You provide\nthe path to this file by cusomizing the custom_image_pull_secrets.dockerconfigjson\nvariable and define the secret name via custom_image_pull_secrets.name in the custom\nimages config file (the one passed to the playbook via -e @file).\nBy default, docker creates the dockerconfigjson auth file in ~/.docker/config.json\nafter passing the service-account key to docker login.\nThe docker login command would look like this (key in json format):\ncat registry-sa-key.json | docker login -u _json_key --password-stdin <registry-url>\n\n\nor, for keys in base64 format:\ncat registry-sa-key | docker login -u _json_key_base64 --password-stdin <registry-url>\n\n\n", "saving-helm-chart-customisations": "\nSaving helm-chart customisations\u00b6\nBecause the additional custom images config file (starting from the\nplaybooks/vars/set-custom-images.yaml example) overrides values in the helm-charts\navailable in the repo, there\u2019s also a playbook option of saving those overrides as a\nseparate values file, which could be used if deploying manually via helm.\nThis is controlled via two variables:\n\n\n\ntype\ndefault\ncomment\n\n\n\nsave_helm_components_overrides\nbool\nfalse\nenable saving helm values overrides\n\nsave_helm_components_overrides_file\nstring\n~/seldon_helm_comp_values.yaml\npath/filename for saving overrides\n\n\n\nYou can either pass those within the custom images config file or directly when running the\nplaybook. For example, for just saving the helm-chart overrides (without installing seldon\ncomponents), you would run:\nansible-playbook playbooks/setup-seldon.yaml -e full_install=no -e save_helm_components_overrides=yes -e @<path-to-custom-images-config.yaml>\n\n\nPlease note that when deploying outside ansible via helm using this saved overrides file,\nand using private registries, you will have to manually create the service-account key\nsecret with the same name as the one defined in your custom image config file under\ncustom_image_pull_secrets.name.\n", "uninstall": "\nUninstall\u00b6\nTo fully remove the Ansible installation delete the created Kind cluster\nkind delete cluster --name seldon\n\n\nThis will stop and delete the Kind cluster freeing all of the resources taken by the dev/trial installation.\nYou may want to also remove cache resources used for the installation with\nrm -rf ~/.cache/seldon/\n\n\n\nNote\nIf you used Ansible to install Seldon Core v2 and its ecosystem into K8s cluster other than Kind you need to manually remove all the components.\nNotes on how to remove Seldon Core v2 Helm installation itself you can find here.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/ansible.html", "key": "getting-started/kubernetes-installation/ansible"}}, "cli/docs/seldon_pipeline_status": {"sections": {"seldon-pipeline-status": "\nseldon pipeline status\u00b6\nstatus of a pipeline\n\nSynopsis\u00b6\nstatus of a pipeline\nseldon pipeline status <pipelineName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait string             pipeline wait condition\n\n\n\n\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n\n", "synopsis": "\nSynopsis\u00b6\nstatus of a pipeline\nseldon pipeline status <pipelineName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait string             pipeline wait condition\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline_status.html", "key": "cli/docs/seldon_pipeline_status"}}, "examples/pipeline-examples": {"sections": {"pipeline-examples": "\nPipeline Examples\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Pipeline Examples\u00b6\nThis notebook illustrates a series of Pipelines showing of different ways of combining flows of data and conditional logic. We assume you have Seldon Core V2 running locally.\n\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n\n\nModel Chaining\u00b6\nChain the output of one model into the next. Also shows chaning the tensor names via tensorMap to conform to the expected input tensor names of the second model.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThe pipeline below chains the output of tfsimple1 into tfsimple2. As these models have compatible shape and data type this can be done. However, the output tensor names from tfsimple1 need to be renamed to match the input tensor names for tfsimple2. We do this with the tensorMap feature.\nThe output of the Pipeline is the output from tfsimple2.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\n\n\nseldon pipeline status tfsimples -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples\",\n        \"uid\": \"ciep26qi8ufs73flaiqg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.outputs\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:11:40.101677847Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nWe use the Seldon CLI pipeline inspect feature to look at the data for all steps of the pipeline for the last data item passed through the pipeline (the default). This can be useful for debugging.\nseldon pipeline inspect tfsimples\n\n\nseldon.default.model.tfsimple1.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.model.tfsimple2.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimples.inputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimples\", \"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimples.outputs\tciep298fh5ss73dpdir0\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nNext, we look get the output as json and use the jq tool to get just one value.\nseldon pipeline inspect tfsimples --format json | jq -M .topics[0].msgs[0].value\n\n\n{\n  \"inputs\": [\n    {\n      \"name\": \"INPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"INPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nModel Chaining from inputs\u00b6\nChain the output of one model into the next. Shows using the input and outputs and combining.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimples-input.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples-input\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1.inputs.INPUT0\n      - tfsimple1.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples-input.yaml\n\n\nseldon pipeline status tfsimples-input -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples-input\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples-input\",\n        \"uid\": \"ciep2fii8ufs73flair0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.inputs.INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:14.711416101Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples-input\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nModel Join\u00b6\nJoin two flows of data from two models as input to a third model. This shows how individual flows of data can be combined.\ncat ./models/tfsimple1.yaml\necho \"---\"\ncat ./models/tfsimple2.yaml\necho \"---\"\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model load -f ./models/tfsimple3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon model status tfsimple3 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nIn the pipeline below for the input to tfsimple3 we join 1 output tensor each from the two previous models tfsimple1 and tfsimple2. We need to use the tensorMap feature to rename each output tensor to one of the expected input tensors for the tfsimple3 model.\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nseldon pipeline load -f ./pipelines/tfsimples-join.yaml\n\n\nseldon pipeline status join -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"join\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"join\",\n        \"uid\": \"ciep2k2i8ufs73flairg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\"\n          },\n          {\n            \"name\": \"tfsimple3\",\n            \"inputs\": [\n              \"tfsimple1.outputs.OUTPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple3.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:32.938603415Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe outputs are the sequence \u201c2,4,6\u2026\u201d which conforms to the logic of this model (addition and subtraction) when fed the output of the first two models.\nseldon pipeline infer join --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload join\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon model unload tfsimple3\n\n\n\n\nConditional\u00b6\nShows conditional data flows - one of two models is run based on output tensors from first.\ncat ./models/conditional.yaml\necho \"---\"\ncat ./models/add10.yaml\necho \"---\"\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: conditional\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/conditional\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/conditional.yaml\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status conditional -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\nseldon model status mul10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nHere we assume the conditional model can output two tensors OUTPUT0 and OUTPUT1 but only outputs the former if the CHOICE input tensor is set to 0 otherwise it outputs tensor OUTPUT1. By this means only one of the two downstream models will receive data and run. The output steps does an any join from both models and whichever data appears first will be sent as output to pipeline. As in this case only 1 of the two models add10 and mul10 runs we will receive their output.\ncat ./pipelines/conditional.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/conditional.yaml\n\n\nseldon pipeline status tfsimple-conditional -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-conditional\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-conditional\",\n        \"uid\": \"ciepga2i8ufs73flais0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"conditional\"\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT0\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:41:45.133142725Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe mul10 model will run as the CHOICE tensor is set to 0.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[0]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nThe add10 model will run as the CHOICE tensor is not set to zero.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[1]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimple-conditional\n\n\nseldon model unload conditional\nseldon model unload add10\nseldon model unload mul10\n\n\n\n\nPipeline Input Tensors\u00b6\nAccess to indivudal tensors in pipeline inputs\ncat ./models/mul10.yaml\necho \"---\"\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThis pipeline shows how we can access pipeline inputs INPUT0 and INPUT1 from different steps.\ncat ./pipelines/pipeline-inputs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-inputs\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - pipeline-inputs.inputs.INPUT0\n    tensorMap:\n      pipeline-inputs.inputs.INPUT0: INPUT\n  - name: add10\n    inputs:\n    - pipeline-inputs.inputs.INPUT1\n    tensorMap:\n      pipeline-inputs.inputs.INPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n\n\nseldon pipeline load -f ./pipelines/pipeline-inputs.yaml\n\n\nseldon pipeline status pipeline-inputs -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"pipeline-inputs\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"pipeline-inputs\",\n        \"uid\": \"ciepgeqi8ufs73flaisg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT1\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT0\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:04.202598715Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-inputs --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload pipeline-inputs\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n\n\nTrigger Joins\u00b6\nShows how joins can be used for triggers as well.\ncat ./models/mul10.yaml\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nHere we required tensors names ok1 or ok2 to exist on pipeline inputs to run the mul10 model but require tensor ok3 to exist on pipeline inputs to run the add10 model. The logic on mul10 is handled by a trigger join of any meaning either of these input data can exist to satisfy the trigger join.\ncat ./pipelines/trigger-joins.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/trigger-joins.yaml\n\n\nseldon pipeline status trigger-joins -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"trigger-joins\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"trigger-joins\",\n        \"uid\": \"ciepgkqi8ufs73flait0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok3\"\n            ]\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok1\",\n              \"trigger-joins.inputs.ok2\"\n            ],\n            \"triggersJoin\": \"ANY\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:27.595300698Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok1\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok3\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload trigger-joins\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n\n\n\n\n\n", "seldon-v2-pipeline-examples": "\nSeldon V2 Pipeline Examples\u00b6\nThis notebook illustrates a series of Pipelines showing of different ways of combining flows of data and conditional logic. We assume you have Seldon Core V2 running locally.\n\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n\n\nModel Chaining\u00b6\nChain the output of one model into the next. Also shows chaning the tensor names via tensorMap to conform to the expected input tensor names of the second model.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThe pipeline below chains the output of tfsimple1 into tfsimple2. As these models have compatible shape and data type this can be done. However, the output tensor names from tfsimple1 need to be renamed to match the input tensor names for tfsimple2. We do this with the tensorMap feature.\nThe output of the Pipeline is the output from tfsimple2.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\n\n\nseldon pipeline status tfsimples -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples\",\n        \"uid\": \"ciep26qi8ufs73flaiqg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.outputs\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:11:40.101677847Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nWe use the Seldon CLI pipeline inspect feature to look at the data for all steps of the pipeline for the last data item passed through the pipeline (the default). This can be useful for debugging.\nseldon pipeline inspect tfsimples\n\n\nseldon.default.model.tfsimple1.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.model.tfsimple2.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimples.inputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimples\", \"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimples.outputs\tciep298fh5ss73dpdir0\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nNext, we look get the output as json and use the jq tool to get just one value.\nseldon pipeline inspect tfsimples --format json | jq -M .topics[0].msgs[0].value\n\n\n{\n  \"inputs\": [\n    {\n      \"name\": \"INPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"INPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nModel Chaining from inputs\u00b6\nChain the output of one model into the next. Shows using the input and outputs and combining.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimples-input.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples-input\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1.inputs.INPUT0\n      - tfsimple1.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples-input.yaml\n\n\nseldon pipeline status tfsimples-input -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples-input\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples-input\",\n        \"uid\": \"ciep2fii8ufs73flair0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.inputs.INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:14.711416101Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples-input\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nModel Join\u00b6\nJoin two flows of data from two models as input to a third model. This shows how individual flows of data can be combined.\ncat ./models/tfsimple1.yaml\necho \"---\"\ncat ./models/tfsimple2.yaml\necho \"---\"\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model load -f ./models/tfsimple3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon model status tfsimple3 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nIn the pipeline below for the input to tfsimple3 we join 1 output tensor each from the two previous models tfsimple1 and tfsimple2. We need to use the tensorMap feature to rename each output tensor to one of the expected input tensors for the tfsimple3 model.\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nseldon pipeline load -f ./pipelines/tfsimples-join.yaml\n\n\nseldon pipeline status join -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"join\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"join\",\n        \"uid\": \"ciep2k2i8ufs73flairg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\"\n          },\n          {\n            \"name\": \"tfsimple3\",\n            \"inputs\": [\n              \"tfsimple1.outputs.OUTPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple3.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:32.938603415Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe outputs are the sequence \u201c2,4,6\u2026\u201d which conforms to the logic of this model (addition and subtraction) when fed the output of the first two models.\nseldon pipeline infer join --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload join\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon model unload tfsimple3\n\n\n\n\nConditional\u00b6\nShows conditional data flows - one of two models is run based on output tensors from first.\ncat ./models/conditional.yaml\necho \"---\"\ncat ./models/add10.yaml\necho \"---\"\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: conditional\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/conditional\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/conditional.yaml\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status conditional -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\nseldon model status mul10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nHere we assume the conditional model can output two tensors OUTPUT0 and OUTPUT1 but only outputs the former if the CHOICE input tensor is set to 0 otherwise it outputs tensor OUTPUT1. By this means only one of the two downstream models will receive data and run. The output steps does an any join from both models and whichever data appears first will be sent as output to pipeline. As in this case only 1 of the two models add10 and mul10 runs we will receive their output.\ncat ./pipelines/conditional.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/conditional.yaml\n\n\nseldon pipeline status tfsimple-conditional -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-conditional\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-conditional\",\n        \"uid\": \"ciepga2i8ufs73flais0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"conditional\"\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT0\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:41:45.133142725Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe mul10 model will run as the CHOICE tensor is set to 0.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[0]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nThe add10 model will run as the CHOICE tensor is not set to zero.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[1]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimple-conditional\n\n\nseldon model unload conditional\nseldon model unload add10\nseldon model unload mul10\n\n\n\n\nPipeline Input Tensors\u00b6\nAccess to indivudal tensors in pipeline inputs\ncat ./models/mul10.yaml\necho \"---\"\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThis pipeline shows how we can access pipeline inputs INPUT0 and INPUT1 from different steps.\ncat ./pipelines/pipeline-inputs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-inputs\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - pipeline-inputs.inputs.INPUT0\n    tensorMap:\n      pipeline-inputs.inputs.INPUT0: INPUT\n  - name: add10\n    inputs:\n    - pipeline-inputs.inputs.INPUT1\n    tensorMap:\n      pipeline-inputs.inputs.INPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n\n\nseldon pipeline load -f ./pipelines/pipeline-inputs.yaml\n\n\nseldon pipeline status pipeline-inputs -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"pipeline-inputs\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"pipeline-inputs\",\n        \"uid\": \"ciepgeqi8ufs73flaisg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT1\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT0\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:04.202598715Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-inputs --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload pipeline-inputs\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n\n\nTrigger Joins\u00b6\nShows how joins can be used for triggers as well.\ncat ./models/mul10.yaml\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nHere we required tensors names ok1 or ok2 to exist on pipeline inputs to run the mul10 model but require tensor ok3 to exist on pipeline inputs to run the add10 model. The logic on mul10 is handled by a trigger join of any meaning either of these input data can exist to satisfy the trigger join.\ncat ./pipelines/trigger-joins.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/trigger-joins.yaml\n\n\nseldon pipeline status trigger-joins -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"trigger-joins\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"trigger-joins\",\n        \"uid\": \"ciepgkqi8ufs73flait0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok3\"\n            ]\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok1\",\n              \"trigger-joins.inputs.ok2\"\n            ],\n            \"triggersJoin\": \"ANY\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:27.595300698Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok1\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok3\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload trigger-joins\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n\n\n\n\n", "models-used": "\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n", "model-chaining": "\nModel Chaining\u00b6\nChain the output of one model into the next. Also shows chaning the tensor names via tensorMap to conform to the expected input tensor names of the second model.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThe pipeline below chains the output of tfsimple1 into tfsimple2. As these models have compatible shape and data type this can be done. However, the output tensor names from tfsimple1 need to be renamed to match the input tensor names for tfsimple2. We do this with the tensorMap feature.\nThe output of the Pipeline is the output from tfsimple2.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\n\n\nseldon pipeline status tfsimples -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples\",\n        \"uid\": \"ciep26qi8ufs73flaiqg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.outputs\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:11:40.101677847Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nWe use the Seldon CLI pipeline inspect feature to look at the data for all steps of the pipeline for the last data item passed through the pipeline (the default). This can be useful for debugging.\nseldon pipeline inspect tfsimples\n\n\nseldon.default.model.tfsimple1.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.model.tfsimple2.inputs\tciep298fh5ss73dpdir0\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimples.inputs\tciep298fh5ss73dpdir0\t{\"modelName\":\"tfsimples\", \"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimples.outputs\tciep298fh5ss73dpdir0\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nNext, we look get the output as json and use the jq tool to get just one value.\nseldon pipeline inspect tfsimples --format json | jq -M .topics[0].msgs[0].value\n\n\n{\n  \"inputs\": [\n    {\n      \"name\": \"INPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"INPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n", "model-chaining-from-inputs": "\nModel Chaining from inputs\u00b6\nChain the output of one model into the next. Shows using the input and outputs and combining.\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimples-input.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples-input\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1.inputs.INPUT0\n      - tfsimple1.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimples-input.yaml\n\n\nseldon pipeline status tfsimples-input -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimples-input\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimples-input\",\n        \"uid\": \"ciep2fii8ufs73flair0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\",\n            \"inputs\": [\n              \"tfsimple1.inputs.INPUT0\",\n              \"tfsimple1.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:14.711416101Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimples-input --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          1,\n          2,\n          3,\n          4,\n          5,\n          6,\n          7,\n          8,\n          9,\n          10,\n          11,\n          12,\n          13,\n          14,\n          15,\n          16\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimples-input\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n", "model-join": "\nModel Join\u00b6\nJoin two flows of data from two models as input to a third model. This shows how individual flows of data can be combined.\ncat ./models/tfsimple1.yaml\necho \"---\"\ncat ./models/tfsimple2.yaml\necho \"---\"\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model load -f ./models/tfsimple3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon model status tfsimple3 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nIn the pipeline below for the input to tfsimple3 we join 1 output tensor each from the two previous models tfsimple1 and tfsimple2. We need to use the tensorMap feature to rename each output tensor to one of the expected input tensors for the tfsimple3 model.\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nseldon pipeline load -f ./pipelines/tfsimples-join.yaml\n\n\nseldon pipeline status join -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"join\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"join\",\n        \"uid\": \"ciep2k2i8ufs73flairg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          },\n          {\n            \"name\": \"tfsimple2\"\n          },\n          {\n            \"name\": \"tfsimple3\",\n            \"inputs\": [\n              \"tfsimple1.outputs.OUTPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n              \"tfsimple2.outputs.OUTPUT1\": \"INPUT1\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple3.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:12:32.938603415Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe outputs are the sequence \u201c2,4,6\u2026\u201d which conforms to the logic of this model (addition and subtraction) when fed the output of the first two models.\nseldon pipeline infer join --inference-mode grpc \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload join\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon model unload tfsimple3\n\n\n", "conditional": "\nConditional\u00b6\nShows conditional data flows - one of two models is run based on output tensors from first.\ncat ./models/conditional.yaml\necho \"---\"\ncat ./models/add10.yaml\necho \"---\"\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: conditional\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/conditional\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/conditional.yaml\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status conditional -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\nseldon model status mul10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n{}\n\n\nHere we assume the conditional model can output two tensors OUTPUT0 and OUTPUT1 but only outputs the former if the CHOICE input tensor is set to 0 otherwise it outputs tensor OUTPUT1. By this means only one of the two downstream models will receive data and run. The output steps does an any join from both models and whichever data appears first will be sent as output to pipeline. As in this case only 1 of the two models add10 and mul10 runs we will receive their output.\ncat ./pipelines/conditional.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-conditional\nspec:\n  steps:\n  - name: conditional\n  - name: mul10\n    inputs:\n    - conditional.outputs.OUTPUT0\n    tensorMap:\n      conditional.outputs.OUTPUT0: INPUT\n  - name: add10\n    inputs:\n    - conditional.outputs.OUTPUT1\n    tensorMap:\n      conditional.outputs.OUTPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/conditional.yaml\n\n\nseldon pipeline status tfsimple-conditional -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-conditional\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-conditional\",\n        \"uid\": \"ciepga2i8ufs73flais0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT1\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"conditional\"\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"conditional.outputs.OUTPUT0\"\n            ],\n            \"tensorMap\": {\n              \"conditional.outputs.OUTPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:41:45.133142725Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nThe mul10 model will run as the CHOICE tensor is set to 0.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[0]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nThe add10 model will run as the CHOICE tensor is not set to zero.\nseldon pipeline infer tfsimple-conditional --inference-mode grpc \\\n '{\"model_name\":\"conditional\",\"inputs\":[{\"name\":\"CHOICE\",\"contents\":{\"int_contents\":[1]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload tfsimple-conditional\n\n\nseldon model unload conditional\nseldon model unload add10\nseldon model unload mul10\n\n\n", "pipeline-input-tensors": "\nPipeline Input Tensors\u00b6\nAccess to indivudal tensors in pipeline inputs\ncat ./models/mul10.yaml\necho \"---\"\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nThis pipeline shows how we can access pipeline inputs INPUT0 and INPUT1 from different steps.\ncat ./pipelines/pipeline-inputs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-inputs\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - pipeline-inputs.inputs.INPUT0\n    tensorMap:\n      pipeline-inputs.inputs.INPUT0: INPUT\n  - name: add10\n    inputs:\n    - pipeline-inputs.inputs.INPUT1\n    tensorMap:\n      pipeline-inputs.inputs.INPUT1: INPUT\n  output:\n    steps:\n    - mul10\n    - add10\n\n\nseldon pipeline load -f ./pipelines/pipeline-inputs.yaml\n\n\nseldon pipeline status pipeline-inputs -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"pipeline-inputs\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"pipeline-inputs\",\n        \"uid\": \"ciepgeqi8ufs73flaisg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT1\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT1\": \"INPUT\"\n            }\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"pipeline-inputs.inputs.INPUT0\"\n            ],\n            \"tensorMap\": {\n              \"pipeline-inputs.inputs.INPUT0\": \"INPUT\"\n            }\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:04.202598715Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-inputs --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]},{\"name\":\"INPUT1\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload pipeline-inputs\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n", "trigger-joins": "\nTrigger Joins\u00b6\nShows how joins can be used for triggers as well.\ncat ./models/mul10.yaml\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/mul10.yaml\nseldon model load -f ./models/add10.yaml\n\n\n{}\n{}\n\n\nseldon model status mul10 -w ModelAvailable | jq -M .\nseldon model status add10 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nHere we required tensors names ok1 or ok2 to exist on pipeline inputs to run the mul10 model but require tensor ok3 to exist on pipeline inputs to run the add10 model. The logic on mul10 is handled by a trigger join of any meaning either of these input data can exist to satisfy the trigger join.\ncat ./pipelines/trigger-joins.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: trigger-joins\nspec:\n  steps:\n  - name: mul10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok1\n    - trigger-joins.inputs.ok2\n    triggersJoinType: any\n  - name: add10\n    inputs:\n    - trigger-joins.inputs.INPUT\n    triggers:\n    - trigger-joins.inputs.ok3\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ./pipelines/trigger-joins.yaml\n\n\nseldon pipeline status trigger-joins -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"trigger-joins\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"trigger-joins\",\n        \"uid\": \"ciepgkqi8ufs73flait0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok3\"\n            ]\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"trigger-joins.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"trigger-joins.inputs.ok1\",\n              \"trigger-joins.inputs.ok2\"\n            ],\n            \"triggersJoin\": \"ANY\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:42:27.595300698Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok1\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer trigger-joins --inference-mode grpc \\\n    '{\"model_name\":\"pipeline\",\"inputs\":[{\"name\":\"ok3\",\"contents\":{\"fp32_contents\":[1]},\"datatype\":\"FP32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline unload trigger-joins\n\n\nseldon model unload mul10\nseldon model unload add10\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/pipeline-examples.html", "key": "examples/pipeline-examples"}}, "cli/docs/seldon_model": {"sections": {"seldon-model": "\nseldon model\u00b6\nmanage models\n\nSynopsis\u00b6\nload and unload and get status for models\nseldon model <subcomand> [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for model\n\n\n\n\nSEE ALSO\u00b6\n\nseldon\t -\nseldon model infer\t - run inference on a model\nseldon model list\t - get list of models\nseldon model load\t - load a model\nseldon model metadata\t - get model metadata\nseldon model status\t - get status for model\nseldon model unload\t - unload a model\n\n\n", "synopsis": "\nSynopsis\u00b6\nload and unload and get status for models\nseldon model <subcomand> [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for model\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon\t -\nseldon model infer\t - run inference on a model\nseldon model list\t - get list of models\nseldon model load\t - load a model\nseldon model metadata\t - get model metadata\nseldon model status\t - get status for model\nseldon model unload\t - unload a model\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_model.html", "key": "cli/docs/seldon_model"}}, "examples/tritonclient-examples": {"sections": {"tritonclient-examples-with-seldon-core-v2": "\nTritonclient Examples with Seldon Core V2\u00b6\nTo install tritonclient\npip install tritonclient[all]\n\n\n\nTritonclient Examples with Seldon Core V2\u00b6\n\nNote: for compatibility of Tritonclient check this issue https://github.com/SeldonIO/seldon-core-v2/issues/471\n\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\n\n\nWith MLServer\u00b6\n\nNote: binary data support in HTTP is blocked by https://github.com/SeldonIO/MLServer/issues/324\n\n\nDeploy Model and Pipeline\u00b6\ncat models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\ncat pipelines/iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: iris-pipeline\nspec:\n  steps:\n    - name: iris\n  output:\n    steps:\n    - iris\n\n\nkubectl apply -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\npipeline.mlops.seldon.io/iris-pipeline created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model iris -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines iris-pipeline -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\npipeline.mlops.seldon.io/iris-pipeline condition met\n\n\n\n\n\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n# Against pipeline\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris-pipeline.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"iris_1\"\n\n\n\n\nAgainst Model\u00b6\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\nAgainst Pipeline\u00b6\nmodel_name = \"iris-pipeline.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\n\n\nWith Tritonserver\u00b6\n\nNote: binary data support in HTTP is blocked by https://github.com/SeldonIO/seldon-core-v2/issues/475\n\n\nDeploy Model and Pipeline\u00b6\ncat models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\ncat pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nkubectl apply -f models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/tfsimple.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\npipeline.mlops.seldon.io/tfsimple created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model tfsimple1 -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines tfsimple -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\npipeline.mlops.seldon.io/tfsimple condition met\n\n\n\n\n\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against model (with binary data)\n\nbinary_data = True\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n## binary data does not work with http behind pipeline\n\n# import numpy as np\n\n# binary_data = True\n\n# inputs = [\n#     httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n#     httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n# ]\n# inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n# inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\n# outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\n# result = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\n# result.as_numpy(\"OUTPUT0\")\n\n\n\n\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"tfsimple1_1\"\nversions: \"1\"\nplatform: \"tensorflow_graphdef\"\ninputs {\n  name: \"INPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\ninputs {\n  name: \"INPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\n\n\n\n# Against Model\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline\n\nmodel_name = \"tfsimple.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n\n\n\nCleanup\u00b6\nkubectl delete -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\nkubectl delete -f pipelines/iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\npipeline.mlops.seldon.io \"iris-pipeline\" deleted\n\n\n\nkubectl delete -f models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f pipelines/tfsimple.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\npipeline.mlops.seldon.io \"tfsimple\" deleted\n\n\n\n\n\n\n\n", "id1": "\nTritonclient Examples with Seldon Core V2\u00b6\n\nNote: for compatibility of Tritonclient check this issue https://github.com/SeldonIO/seldon-core-v2/issues/471\n\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\n", "with-mlserver": "\nWith MLServer\u00b6\n\nNote: binary data support in HTTP is blocked by https://github.com/SeldonIO/MLServer/issues/324\n\n\nDeploy Model and Pipeline\u00b6\ncat models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\ncat pipelines/iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: iris-pipeline\nspec:\n  steps:\n    - name: iris\n  output:\n    steps:\n    - iris\n\n\nkubectl apply -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\npipeline.mlops.seldon.io/iris-pipeline created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model iris -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines iris-pipeline -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\npipeline.mlops.seldon.io/iris-pipeline condition met\n\n\n\n\n\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n# Against pipeline\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris-pipeline.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"iris_1\"\n\n\n\n\nAgainst Model\u00b6\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\nAgainst Pipeline\u00b6\nmodel_name = \"iris-pipeline.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\n", "deploy-model-and-pipeline": "\nDeploy Model and Pipeline\u00b6\ncat models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\ncat pipelines/iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: iris-pipeline\nspec:\n  steps:\n    - name: iris\n  output:\n    steps:\n    - iris\n\n\nkubectl apply -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\npipeline.mlops.seldon.io/iris-pipeline created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model iris -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines iris-pipeline -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\npipeline.mlops.seldon.io/iris-pipeline condition met\n\n\n\n", "http-transport-protocol": "\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n# Against pipeline\n\nbinary_data = False\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"iris-pipeline.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n", "grpc-transport-protocol": "\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"iris_1\"\n\n\n\n\nAgainst Model\u00b6\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n\nAgainst Pipeline\u00b6\nmodel_name = \"iris-pipeline.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n\n", "against-model": "\nAgainst Model\u00b6\nmodel_name = \"iris\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n", "against-pipeline": "\nAgainst Pipeline\u00b6\nmodel_name = \"iris-pipeline.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n]\ninputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"predict\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"predict\")\n\n\narray([[2]])\n\n\n\n", "with-tritonserver": "\nWith Tritonserver\u00b6\n\nNote: binary data support in HTTP is blocked by https://github.com/SeldonIO/seldon-core-v2/issues/475\n\n\nDeploy Model and Pipeline\u00b6\ncat models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\ncat pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nkubectl apply -f models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/tfsimple.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\npipeline.mlops.seldon.io/tfsimple created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model tfsimple1 -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines tfsimple -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\npipeline.mlops.seldon.io/tfsimple condition met\n\n\n\n\n\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against model (with binary data)\n\nbinary_data = True\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n## binary data does not work with http behind pipeline\n\n# import numpy as np\n\n# binary_data = True\n\n# inputs = [\n#     httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n#     httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n# ]\n# inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n# inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\n# outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\n# result = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\n# result.as_numpy(\"OUTPUT0\")\n\n\n\n\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"tfsimple1_1\"\nversions: \"1\"\nplatform: \"tensorflow_graphdef\"\ninputs {\n  name: \"INPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\ninputs {\n  name: \"INPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\n\n\n\n# Against Model\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline\n\nmodel_name = \"tfsimple.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n\n", "id2": "\nDeploy Model and Pipeline\u00b6\ncat models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\ncat pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nkubectl apply -f models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl apply -f pipelines/tfsimple.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\npipeline.mlops.seldon.io/tfsimple created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model tfsimple1 -n ${NAMESPACE}\nkubectl wait --for condition=ready --timeout=300s pipelines tfsimple -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\npipeline.mlops.seldon.io/tfsimple condition met\n\n\n\n", "id3": "\nHTTP Transport Protocol\u00b6\nimport tritonclient.http as httpclient\nimport numpy as np\n\nhttp_triton_client = httpclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\nprint(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\nprint(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))\n\n\nmodel ready: True\nmodel metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n\n\n\n# Against model (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against model (with binary data)\n\nbinary_data = True\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline (no binary data)\n\nbinary_data = False\n\ninputs = [\n    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\noutputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\nresult = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n## binary data does not work with http behind pipeline\n\n# import numpy as np\n\n# binary_data = True\n\n# inputs = [\n#     httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n#     httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n# ]\n# inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n# inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n\n# outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n\n# result = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\n# result.as_numpy(\"OUTPUT0\")\n\n\n", "id4": "\nGRPC Transport Protocol\u00b6\nimport tritonclient.grpc as grpcclient\nimport numpy as np\n\ngrpc_triton_client = grpcclient.InferenceServerClient(\n    url=f\"{MESH_IP}:80\",\n    verbose=False,\n)\n\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\nprint(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\nprint(grpc_triton_client.get_model_metadata(model_name, headers=headers))\n\n\nmodel ready: True\nname: \"tfsimple1_1\"\nversions: \"1\"\nplatform: \"tensorflow_graphdef\"\ninputs {\n  name: \"INPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\ninputs {\n  name: \"INPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT0\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\noutputs {\n  name: \"OUTPUT1\"\n  datatype: \"INT32\"\n  shape: -1\n  shape: 16\n}\n\n\n\n# Against Model\n\nmodel_name = \"tfsimple1\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n# Against Pipeline\n\nmodel_name = \"tfsimple.pipeline\"\nheaders = {\"seldon-model\": model_name}\n\ninputs = [\n    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n]\ninputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\ninputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n\noutputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n\nresult = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\nresult.as_numpy(\"OUTPUT0\")\n\n\narray([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n      dtype=int32)\n\n\n\n", "cleanup": "\nCleanup\u00b6\nkubectl delete -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\nkubectl delete -f pipelines/iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\npipeline.mlops.seldon.io \"iris-pipeline\" deleted\n\n\n\nkubectl delete -f models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f pipelines/tfsimple.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\npipeline.mlops.seldon.io \"tfsimple\" deleted\n\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/tritonclient-examples.html", "key": "examples/tritonclient-examples"}}, "kubernetes/resources": {"sections": {"resources": "\nResources\u00b6\nFor Kubernetes usage we provide a set of custom resources for interacting with Seldon.\n\nSeldonRuntime - for installing Seldon in a particular namespace.\nServers - for deploying sets of replicas of core inference servers (MLServer or Triton).\nModels - for deploying single machine learning models, custom transformation logic, drift detectors, outliers detectors and explainers.\nExperiments - for testing new versions of models.\nPipelines - for connecting together flows of data between models.\n\n\nAdvanced Customization\u00b6\nSeldonConfig and ServerConfig define the core installation configuration and machine learning inference server configuration for Seldon. Normally, you would not need to customize these but this may be required for your particular custom installation within your organisation.\n\nServerConfigs - for defining new types of inference server that can be reference by a Server resource.\nSeldonConfig - for defining how seldon is installed\n\n\n\n\n", "advanced-customization": "\nAdvanced Customization\u00b6\nSeldonConfig and ServerConfig define the core installation configuration and machine learning inference server configuration for Seldon. Normally, you would not need to customize these but this may be required for your particular custom installation within your organisation.\n\nServerConfigs - for defining new types of inference server that can be reference by a Server resource.\nSeldonConfig - for defining how seldon is installed\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/index.html", "key": "kubernetes/resources"}}, "cli/docs/seldon_experiment_stop": {"sections": {"seldon-experiment-stop": "\nseldon experiment stop\u00b6\nstop an experiment\n\nSynopsis\u00b6\nstop an experiment\nseldon experiment stop <experimentName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for stop\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n\n", "synopsis": "\nSynopsis\u00b6\nstop an experiment\nseldon experiment stop <experimentName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for stop\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_experiment_stop.html", "key": "cli/docs/seldon_experiment_stop"}}, "models/parameterized-models/pandasquery": {"sections": {"pandas-query-model": "\nPandas Query Model\u00b6\nThis model allows a Pandas  query to be run in the input to select rows. An example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-one\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 1\"\n\n\nThis invocation check filters for tensor A having value 1.\n\nThe model also returns a tensor called status which indicates the operation run and whether it was a success. If no rows satisfy the query then just a status tensor output will be returned.\nFurther details on Pandas query can be found here\n\nThis model can be useful for conditional Pipelines. For example, you could have two invocations of this model:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-one\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 1\"\n\n\nand\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-two\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 2\"\n\n\nBy including these in a Pipeline as follows we can define conditional routes:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: choice\nspec:\n  steps:\n  - name: choice-is-one\n  - name: mul10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-one.outputs.choice\n  - name: choice-is-two\n  - name: add10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-two.outputs.choice\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nHere the mul10 model will be called if the choice-is-one model succeeds and the add10 model will be called if the choice-is-two model succeeds.\nThe full notebook can be found here\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/models/parameterized-models/pandasquery.html", "key": "models/parameterized-models/pandasquery"}}, "about": {"sections": {"about": "\nAbout\u00b6\nSeldon V2 APIs provide a state of the art solution for machine learning inference which can be run locally on a laptop as well as on Kubernetes for production.\n\n\n\nFeatures\u00b6\n\nA single platform for inference of wide range of standard and custom artifacts.\nDeploy locally in Docker during development and testing of models.\nDeploy at scale on Kubernetes for production.\nDeploy single models to multi-step pipelines.\nSave infrastructure costs by deploying multiple models transparently in inference servers.\nOvercommit on resources to deploy more models than available memory.\nDynamically extended models with pipelines with a data-centric perspective backed by Kafka.\nExplain individual models and pipelines with state of the art explanation techniques.\nDeploy drift and outlier detectors alongside models.\nKubernetes Service mesh agnostic - use the service mesh of your choice.\n\n\n\nCore features and comparison to Seldon Core V1 APIs\u00b6\nOur V2 APIs separate out core tasks into separate resources allowing users to get started fast with deploying a Model and the progressing to more complex Pipelines, Explanations and Experiments.\n\n\n\nMulti-model serving\u00b6\nSeldon transparently will provision your model onto the correct inference server.\n\nBy packing multiple models onto a smaller set of servers users can save infrastructure costs and efficiently utilize their models.\n\nBy allowing over-commit users can provision model models that available memory resources by allowing Seldon to transparently unload models that are not in use.\n\n\n\nInference Servers\u00b6\nSeldon V2 supports any V2 protocol inference server. At present we include Seldon\u2019s MLServer and NVIDIA\u2019s Triton inference server automatically on install. These servers cover a wide range of artifacts including custom python models.\n\n\n\nService Mesh Agnostic\u00b6\nSeldon core v2 can be integrated with any Kubernetes service mesh. There are current examples with istio, Ambassador and Traefic.\n\n\n\nPublication\u00b6\nThese features are influenced by our position paper on the next generation of ML model serving frameworks:\nTitle: Desiderata for next generation of ML model serving\nWorkshop: Challenges in deploying and monitoring ML systems workshop - NeurIPS 2022\n\n", "features": "\nFeatures\u00b6\n\nA single platform for inference of wide range of standard and custom artifacts.\nDeploy locally in Docker during development and testing of models.\nDeploy at scale on Kubernetes for production.\nDeploy single models to multi-step pipelines.\nSave infrastructure costs by deploying multiple models transparently in inference servers.\nOvercommit on resources to deploy more models than available memory.\nDynamically extended models with pipelines with a data-centric perspective backed by Kafka.\nExplain individual models and pipelines with state of the art explanation techniques.\nDeploy drift and outlier detectors alongside models.\nKubernetes Service mesh agnostic - use the service mesh of your choice.\n\n", "core-features-and-comparison-to-seldon-core-v1-apis": "\nCore features and comparison to Seldon Core V1 APIs\u00b6\nOur V2 APIs separate out core tasks into separate resources allowing users to get started fast with deploying a Model and the progressing to more complex Pipelines, Explanations and Experiments.\n\n", "multi-model-serving": "\nMulti-model serving\u00b6\nSeldon transparently will provision your model onto the correct inference server.\n\nBy packing multiple models onto a smaller set of servers users can save infrastructure costs and efficiently utilize their models.\n\nBy allowing over-commit users can provision model models that available memory resources by allowing Seldon to transparently unload models that are not in use.\n\n", "inference-servers": "\nInference Servers\u00b6\nSeldon V2 supports any V2 protocol inference server. At present we include Seldon\u2019s MLServer and NVIDIA\u2019s Triton inference server automatically on install. These servers cover a wide range of artifacts including custom python models.\n\n", "service-mesh-agnostic": "\nService Mesh Agnostic\u00b6\nSeldon core v2 can be integrated with any Kubernetes service mesh. There are current examples with istio, Ambassador and Traefic.\n\n", "publication": "\nPublication\u00b6\nThese features are influenced by our position paper on the next generation of ML model serving frameworks:\nTitle: Desiderata for next generation of ML model serving\nWorkshop: Challenges in deploying and monitoring ML systems workshop - NeurIPS 2022\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/about/index.html", "key": "about"}}, "metrics/local-metrics-test": {"sections": {"local-metrics": "\nLocal Metrics\u00b6\nRun these examples from the samples folder.\n\nMetric Examples\u00b6\nThis notebook tests the exposed Prometheus metrics of model and pipeline servers.\nRequires: prometheus_client and requests libraries.\nSee docs for full set of metrics available.\nmlserver_metrics_host=\"0.0.0.0:9006\"\ntriton_metrics_host=\"0.0.0.0:9007\"\npipeline_metrics_host=\"0.0.0.0:9009\"\n\n\nfrom prometheus_client.parser import text_string_to_metric_families\nimport requests\n\ndef scrape_metrics(host):\n    data = requests.get(f\"http://{host}/metrics\").text\n    return {\n        family.name: family for family in text_string_to_metric_families(data)\n    }\n\ndef print_sample(family, label, value):\n    for sample in family.samples:\n        if sample.labels[label] == value:\n            print(sample)\n\ndef get_model_infer_count(host, model_name):\n    metrics = scrape_metrics(host)\n    family = metrics[\"seldon_model_infer\"]\n    print_sample(family, \"model\", model_name)\n\ndef get_pipeline_infer_count(host, pipeline_name):\n    metrics = scrape_metrics(host)\n    family = metrics[\"seldon_pipeline_infer\"]\n    print_sample(family, \"pipeline\", pipeline_name)\n\n\n\nMLServer Model\u00b6\nseldon model load -f ./models/sklearn-iris-gs.yaml\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -i 100 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nget_model_infer_count(mlserver_metrics_host,\"iris\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload iris\n\n\n{}\n\n\n\n\nTriton Model\u00b6\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer tfsimple1 -i 50\\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50]\n\n\n\nseldon model infer tfsimple1 --inference-mode grpc -i 100 \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::100]\n\n\n\nget_model_infer_count(triton_metrics_host,\"tfsimple1\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\n\n\n{}\n\n\n\n\nPipeline\u00b6\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{}\n{}\n{}\n{}\n{}\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"cdqji39qa12c739ab3o0\", \"version\":2, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":2, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2022-11-16T19:25:01.255955114Z\"}}]}\n\n\nseldon pipeline infer tfsimples -i 50 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50 :tfsimple2_1::50 :tfsimples.pipeline::50]\n\n\n\nget_pipeline_infer_count(pipeline_metrics_host,\"tfsimples\")\n\n\nSample(name='seldon_pipeline_infer_total', labels={'code': '200', 'method_type': 'rest', 'pipeline': 'tfsimples', 'server': 'pipeline-gateway'}, value=50.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon pipeline unload tfsimples\n\n\n{}\n{}\n{}\n\n\n\n\n\n\n\n", "metric-examples": "\nMetric Examples\u00b6\nThis notebook tests the exposed Prometheus metrics of model and pipeline servers.\nRequires: prometheus_client and requests libraries.\nSee docs for full set of metrics available.\nmlserver_metrics_host=\"0.0.0.0:9006\"\ntriton_metrics_host=\"0.0.0.0:9007\"\npipeline_metrics_host=\"0.0.0.0:9009\"\n\n\nfrom prometheus_client.parser import text_string_to_metric_families\nimport requests\n\ndef scrape_metrics(host):\n    data = requests.get(f\"http://{host}/metrics\").text\n    return {\n        family.name: family for family in text_string_to_metric_families(data)\n    }\n\ndef print_sample(family, label, value):\n    for sample in family.samples:\n        if sample.labels[label] == value:\n            print(sample)\n\ndef get_model_infer_count(host, model_name):\n    metrics = scrape_metrics(host)\n    family = metrics[\"seldon_model_infer\"]\n    print_sample(family, \"model\", model_name)\n\ndef get_pipeline_infer_count(host, pipeline_name):\n    metrics = scrape_metrics(host)\n    family = metrics[\"seldon_pipeline_infer\"]\n    print_sample(family, \"pipeline\", pipeline_name)\n\n\n\nMLServer Model\u00b6\nseldon model load -f ./models/sklearn-iris-gs.yaml\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -i 100 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nget_model_infer_count(mlserver_metrics_host,\"iris\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload iris\n\n\n{}\n\n\n\n\nTriton Model\u00b6\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer tfsimple1 -i 50\\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50]\n\n\n\nseldon model infer tfsimple1 --inference-mode grpc -i 100 \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::100]\n\n\n\nget_model_infer_count(triton_metrics_host,\"tfsimple1\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\n\n\n{}\n\n\n\n\nPipeline\u00b6\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{}\n{}\n{}\n{}\n{}\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"cdqji39qa12c739ab3o0\", \"version\":2, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":2, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2022-11-16T19:25:01.255955114Z\"}}]}\n\n\nseldon pipeline infer tfsimples -i 50 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50 :tfsimple2_1::50 :tfsimples.pipeline::50]\n\n\n\nget_pipeline_infer_count(pipeline_metrics_host,\"tfsimples\")\n\n\nSample(name='seldon_pipeline_infer_total', labels={'code': '200', 'method_type': 'rest', 'pipeline': 'tfsimples', 'server': 'pipeline-gateway'}, value=50.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon pipeline unload tfsimples\n\n\n{}\n{}\n{}\n\n\n\n\n\n\n", "mlserver-model": "\nMLServer Model\u00b6\nseldon model load -f ./models/sklearn-iris-gs.yaml\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -i 100 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nget_model_infer_count(mlserver_metrics_host,\"iris\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'iris', 'model_internal': 'iris_1', 'server': 'mlserver', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload iris\n\n\n{}\n\n\n", "triton-model": "\nTriton Model\u00b6\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\nseldon model infer tfsimple1 -i 50\\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50]\n\n\n\nseldon model infer tfsimple1 --inference-mode grpc -i 100 \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::100]\n\n\n\nget_model_infer_count(triton_metrics_host,\"tfsimple1\")\n\n\nSample(name='seldon_model_infer_total', labels={'code': '200', 'method_type': 'rest', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=50.0, timestamp=None, exemplar=None)\nSample(name='seldon_model_infer_total', labels={'code': 'OK', 'method_type': 'grpc', 'model': 'tfsimple1', 'model_internal': 'tfsimple1_1', 'server': 'triton', 'server_replica': '0'}, value=100.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\n\n\n{}\n\n\n", "pipeline": "\nPipeline\u00b6\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{}\n{}\n{}\n{}\n{}\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"cdqji39qa12c739ab3o0\", \"version\":2, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":2, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2022-11-16T19:25:01.255955114Z\"}}]}\n\n\nseldon pipeline infer tfsimples -i 50 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\nSuccess: map[:tfsimple1_1::50 :tfsimple2_1::50 :tfsimples.pipeline::50]\n\n\n\nget_pipeline_infer_count(pipeline_metrics_host,\"tfsimples\")\n\n\nSample(name='seldon_pipeline_infer_total', labels={'code': '200', 'method_type': 'rest', 'pipeline': 'tfsimples', 'server': 'pipeline-gateway'}, value=50.0, timestamp=None, exemplar=None)\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\nseldon pipeline unload tfsimples\n\n\n{}\n{}\n{}\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/metrics/local-metrics-test.html", "key": "metrics/local-metrics-test"}}, "kubernetes/storage-secrets": {"sections": {"storage-secrets": "\nStorage Secrets\u00b6\nInference artifacts referenced by Models can be stored in any of the storage backends supported by Rclone.\nThis includes local filesystems, AWS S3, and Google Cloud Storage (GCS), among others.\nConfiguration is provided out-of-the-box for public GCS buckets, which enables the use of Seldon-provided models like in the below example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nThis configuration is provided by the Kubernetes Secret seldon-rclone-gs-public.\nIt is made available to Servers as a preloaded secret.\nYou can define and use your own storage configurations in exactly the same way.\n\nConfiguration Format\u00b6\nTo define a new storage configuration, you need the following details:\n\nRemote name\nRemote type\nProvider parameters\n\nA remote is what Rclone calls a storage location.\nThe type defines what protocol Rclone should use to talk to this remote.\nA provider is a particular implementation for that storage type.\nSome storage types have multiple providers, such as s3 having AWS S3 itself, MinIO, Ceph, and so on.\nThe remote name is your choice.\nThe prefix you use for models in spec.storageUri must be the same as this remote name.\nThe remote type is one of the values supported by Rclone.\nFor example, for AWS S3 it is s3 and for Dropbox it is dropbox.\nThe provider parameters depend entirely on the remote type and the specific provider you are using.\nPlease check the Rclone documentation for the appropriate provider.\nNote that Rclone docs for storage types call the parameters properties and provide both config and env var formats\u2013you need to use the config format.\nFor example, the GCS parameter --gcs-client-id described here should be used as client_id.\nFor reference, this format is described in the Rclone documentation.\nNote that we do not support the use of opts discussed in that section.\n\n\nKubernetes Secrets\u00b6\nKubernetes Secrets are used to store Rclone configurations, or storage secrets, for use by Servers.\nEach Secret should contain exactly one Rclone configuration.\nA Server can use storage secrets in one of two ways:\n\nIt can dynamically load a secret specified by a Model in its .spec.secretName\nIt can use global configurations made available via preloaded secrets\n\nThe name of a Secret is entirely your choice, as is the name of the data key in that Secret.\nAll that matters is that there is a single data key and that its value is in the format described above.\n\nNote\nIt is possible to use preloaded secrets for some Models and dynamically loaded secrets for others.\n\n\nPreloaded Secrets\u00b6\nRather than Models always having to specify which secret to use, a Server can load storage secrets ahead of time.\nThese can then be reused across many Models.\nWhen using a preloaded secret, the Model definition should leave .spec.secretName empty.\nThe protocol prefix in .spec.storageUri still needs to match the remote name specified by a storage secret.\nThe secrets to preload are named in a centralised ConfigMap called seldon-agent.\nThis ConfigMap applies to all Servers managed by the same SeldonRuntime.\nBy default this ConfigMap only includes seldon-rclone-gs-public, but can be extended with your own secrets as shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: seldon-agent\ndata:\n  agent.json: |-\n   {\n      \"rclone\" : {\n          \"config_secrets\": [\"seldon-rclone-gs-public\",\"minio-secret\"]\n      },\n   }\n\n\nThe easiest way to change this is to update your SeldonRuntime.\n\nIf your SeldonRuntime is configured using the seldon-core-v2-runtime Helm chart, the corresponding value is config.agentConfig.rclone.configSecrets.\nThis can be used as shown below:\nconfig:\n  agentConfig:\n    rclone:\n      configSecrets:\n        - my-s3\n        - custom-gcs\n        - minio-in-cluster\n\n\n\nOtherwise, if your SeldonRuntime is configured directly, you can add secrets by setting .spec.config.agentConfig.rclone.config_secrets.\nThis can be used as follows:\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonRuntime\nmetadata:\n  name: seldon\nspec:\n  seldonConfig: default\n  config:\n    agentConfig:\n      rclone:\n        config_secrets:\n          - my-s3\n          - custom-gcs\n          - minio-in-cluster\n  ...\n\n\n\n\n\n\n\nExamples\u00b6\n\nS3 MinIOGoogle Cloud StorageAssuming you have installed MinIO in the minio-system namespace, a corresponding secret could be:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-secret\n  namespace: seldon-mesh\ntype: Opaque\nstringData:\n  s3: |\n    type: s3\n    name: s3\n    parameters:\n      provider: minio\n      env_auth: false\n      access_key_id: minioadmin\n      secret_access_key: minioadmin\n      endpoint: http://minio.minio-system:9000\n\n\nYou can then reference this in a Model with .spec.secretName:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"s3://models/iris\"\n  secretName: \"minio-secret\"\n  requirements:\n  - sklearn\n\n\nGCS can use service accounts for access.\nYou can generate the credentials for a service account using the gcloud CLI:\ngcloud iam service-accounts keys create \\\n  gcloud-application-credentials.json \\\n  --iam-account [SERVICE-ACCOUNT--NAME]@[PROJECT-ID].iam.gserviceaccount.com\n\n\nThe contents of gcloud-application-credentials.json can be put into a secret:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: gcs-bucket\ntype: Opaque\nstringData:\n  gcs: |\n    type: gcs\n    name: gcs\n    parameters:\n      service_account_credentials: '<gcloud-application-credentials.json>'\n\n\nYou can then reference this in a Model with .spec.secretName:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mymodel\nspec:\n  storageUri: \"gcs://my-bucket/my-path/my-pytorch-model\"\n  secretName: \"gcs-bucket\"\n  requirements:\n  - pytorch\n\n\n\n\n", "configuration-format": "\nConfiguration Format\u00b6\nTo define a new storage configuration, you need the following details:\n\nRemote name\nRemote type\nProvider parameters\n\nA remote is what Rclone calls a storage location.\nThe type defines what protocol Rclone should use to talk to this remote.\nA provider is a particular implementation for that storage type.\nSome storage types have multiple providers, such as s3 having AWS S3 itself, MinIO, Ceph, and so on.\nThe remote name is your choice.\nThe prefix you use for models in spec.storageUri must be the same as this remote name.\nThe remote type is one of the values supported by Rclone.\nFor example, for AWS S3 it is s3 and for Dropbox it is dropbox.\nThe provider parameters depend entirely on the remote type and the specific provider you are using.\nPlease check the Rclone documentation for the appropriate provider.\nNote that Rclone docs for storage types call the parameters properties and provide both config and env var formats\u2013you need to use the config format.\nFor example, the GCS parameter --gcs-client-id described here should be used as client_id.\nFor reference, this format is described in the Rclone documentation.\nNote that we do not support the use of opts discussed in that section.\n", "kubernetes-secrets": "\nKubernetes Secrets\u00b6\nKubernetes Secrets are used to store Rclone configurations, or storage secrets, for use by Servers.\nEach Secret should contain exactly one Rclone configuration.\nA Server can use storage secrets in one of two ways:\n\nIt can dynamically load a secret specified by a Model in its .spec.secretName\nIt can use global configurations made available via preloaded secrets\n\nThe name of a Secret is entirely your choice, as is the name of the data key in that Secret.\nAll that matters is that there is a single data key and that its value is in the format described above.\n\nNote\nIt is possible to use preloaded secrets for some Models and dynamically loaded secrets for others.\n\n\nPreloaded Secrets\u00b6\nRather than Models always having to specify which secret to use, a Server can load storage secrets ahead of time.\nThese can then be reused across many Models.\nWhen using a preloaded secret, the Model definition should leave .spec.secretName empty.\nThe protocol prefix in .spec.storageUri still needs to match the remote name specified by a storage secret.\nThe secrets to preload are named in a centralised ConfigMap called seldon-agent.\nThis ConfigMap applies to all Servers managed by the same SeldonRuntime.\nBy default this ConfigMap only includes seldon-rclone-gs-public, but can be extended with your own secrets as shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: seldon-agent\ndata:\n  agent.json: |-\n   {\n      \"rclone\" : {\n          \"config_secrets\": [\"seldon-rclone-gs-public\",\"minio-secret\"]\n      },\n   }\n\n\nThe easiest way to change this is to update your SeldonRuntime.\n\nIf your SeldonRuntime is configured using the seldon-core-v2-runtime Helm chart, the corresponding value is config.agentConfig.rclone.configSecrets.\nThis can be used as shown below:\nconfig:\n  agentConfig:\n    rclone:\n      configSecrets:\n        - my-s3\n        - custom-gcs\n        - minio-in-cluster\n\n\n\nOtherwise, if your SeldonRuntime is configured directly, you can add secrets by setting .spec.config.agentConfig.rclone.config_secrets.\nThis can be used as follows:\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonRuntime\nmetadata:\n  name: seldon\nspec:\n  seldonConfig: default\n  config:\n    agentConfig:\n      rclone:\n        config_secrets:\n          - my-s3\n          - custom-gcs\n          - minio-in-cluster\n  ...\n\n\n\n\n\n", "preloaded-secrets": "\nPreloaded Secrets\u00b6\nRather than Models always having to specify which secret to use, a Server can load storage secrets ahead of time.\nThese can then be reused across many Models.\nWhen using a preloaded secret, the Model definition should leave .spec.secretName empty.\nThe protocol prefix in .spec.storageUri still needs to match the remote name specified by a storage secret.\nThe secrets to preload are named in a centralised ConfigMap called seldon-agent.\nThis ConfigMap applies to all Servers managed by the same SeldonRuntime.\nBy default this ConfigMap only includes seldon-rclone-gs-public, but can be extended with your own secrets as shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: seldon-agent\ndata:\n  agent.json: |-\n   {\n      \"rclone\" : {\n          \"config_secrets\": [\"seldon-rclone-gs-public\",\"minio-secret\"]\n      },\n   }\n\n\nThe easiest way to change this is to update your SeldonRuntime.\n\nIf your SeldonRuntime is configured using the seldon-core-v2-runtime Helm chart, the corresponding value is config.agentConfig.rclone.configSecrets.\nThis can be used as shown below:\nconfig:\n  agentConfig:\n    rclone:\n      configSecrets:\n        - my-s3\n        - custom-gcs\n        - minio-in-cluster\n\n\n\nOtherwise, if your SeldonRuntime is configured directly, you can add secrets by setting .spec.config.agentConfig.rclone.config_secrets.\nThis can be used as follows:\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonRuntime\nmetadata:\n  name: seldon\nspec:\n  seldonConfig: default\n  config:\n    agentConfig:\n      rclone:\n        config_secrets:\n          - my-s3\n          - custom-gcs\n          - minio-in-cluster\n  ...\n\n\n\n\n", "examples": "\nExamples\u00b6\n\nS3 MinIOGoogle Cloud StorageAssuming you have installed MinIO in the minio-system namespace, a corresponding secret could be:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-secret\n  namespace: seldon-mesh\ntype: Opaque\nstringData:\n  s3: |\n    type: s3\n    name: s3\n    parameters:\n      provider: minio\n      env_auth: false\n      access_key_id: minioadmin\n      secret_access_key: minioadmin\n      endpoint: http://minio.minio-system:9000\n\n\nYou can then reference this in a Model with .spec.secretName:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"s3://models/iris\"\n  secretName: \"minio-secret\"\n  requirements:\n  - sklearn\n\n\nGCS can use service accounts for access.\nYou can generate the credentials for a service account using the gcloud CLI:\ngcloud iam service-accounts keys create \\\n  gcloud-application-credentials.json \\\n  --iam-account [SERVICE-ACCOUNT--NAME]@[PROJECT-ID].iam.gserviceaccount.com\n\n\nThe contents of gcloud-application-credentials.json can be put into a secret:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: gcs-bucket\ntype: Opaque\nstringData:\n  gcs: |\n    type: gcs\n    name: gcs\n    parameters:\n      service_account_credentials: '<gcloud-application-credentials.json>'\n\n\nYou can then reference this in a Model with .spec.secretName:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mymodel\nspec:\n  storageUri: \"gcs://my-bucket/my-path/my-pytorch-model\"\n  secretName: \"gcs-bucket\"\n  requirements:\n  - pytorch\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/storage-secrets/index.html", "key": "kubernetes/storage-secrets"}}, "getting-started": {"sections": {"getting-started": "\nGetting Started\u00b6\n\nNote\nSome dependencies may require that the (virtual) machines on which you deploy, support the SSE4.2 instruction set or x86-64-v2 microarchitecture. If lscpu | grep sse4_2 does not return anything on your machine, your CPU is not compatible, and you may need to update the (virtual) host\u2019s CPU.\n\nSeldon Core can be installed either with Docker Compose or with Kubernetes:\n\nInstall locally with Docker Compose\nInstall onto a Kubernetes cluster\n\nOnce installed:\n\nTry the existing examples.\nTrain and deploy your own model artifact.\n\n\nCore Concepts\u00b6\nThere are three core resources you will use:\n\nModels - for deploying single machine learning models, custom transformation logic, drift detectors and outliers detectors.\nPipelines - for connecting together flows of data transformations between Models with a synchronous path and multiple asynchronous paths.\nExperiments - for testing new versions of models\n\nBy default the standard installation will deploy MLServer and Triton inference servers which provide support for a wide range of machine learning model artifacts including Tensorflow models, PyTorch models, SKlearn models, XGBoost models, ONNX models, TensorRT models, custom python models and many more. For advanced use, the creation of new inference servers is manged by two resources:\n\nServers - for deploying sets of replicas of core inference servers (MLServer or Triton by default).\nServerConfigs - for defining server configurations including custom servers.\n\n\n\nAPI for Inference\u00b6\nOnce deployed models can be called using the Seldon V2 inference protocol. This protocol created by Seldon, NVIDIA and the KServe projects is supported by MLServer and Triton inference servers amingst others and allows REST and gRPC calls to your model.\nYour model is exposed via our internal Envoy gateway. If you wish to expose your models in Kubernetes outside the cluster you are free to use any Service Mesh or Ingress technology. Various examples are provided for service mesh integration.\n\n\nInference Metrics\u00b6\nMetrics are exposed for scraping by Prometheus. For Kubernetes we provide example instructions for using kube-prometheus.\n\n\nPipeline requirements\u00b6\nPipelines are built upon Kafka streaming technology.\n\n\n\n", "core-concepts": "\nCore Concepts\u00b6\nThere are three core resources you will use:\n\nModels - for deploying single machine learning models, custom transformation logic, drift detectors and outliers detectors.\nPipelines - for connecting together flows of data transformations between Models with a synchronous path and multiple asynchronous paths.\nExperiments - for testing new versions of models\n\nBy default the standard installation will deploy MLServer and Triton inference servers which provide support for a wide range of machine learning model artifacts including Tensorflow models, PyTorch models, SKlearn models, XGBoost models, ONNX models, TensorRT models, custom python models and many more. For advanced use, the creation of new inference servers is manged by two resources:\n\nServers - for deploying sets of replicas of core inference servers (MLServer or Triton by default).\nServerConfigs - for defining server configurations including custom servers.\n\n", "api-for-inference": "\nAPI for Inference\u00b6\nOnce deployed models can be called using the Seldon V2 inference protocol. This protocol created by Seldon, NVIDIA and the KServe projects is supported by MLServer and Triton inference servers amingst others and allows REST and gRPC calls to your model.\nYour model is exposed via our internal Envoy gateway. If you wish to expose your models in Kubernetes outside the cluster you are free to use any Service Mesh or Ingress technology. Various examples are provided for service mesh integration.\n", "inference-metrics": "\nInference Metrics\u00b6\nMetrics are exposed for scraping by Prometheus. For Kubernetes we provide example instructions for using kube-prometheus.\n", "pipeline-requirements": "\nPipeline requirements\u00b6\nPipelines are built upon Kafka streaming technology.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/index.html", "key": "getting-started"}}, "servers": {"sections": {"servers": "\nServers\u00b6\nBy default Seldon installs two server farms using MLServer and Triton with 1 replica each. Models are scheduled onto servers based on the server\u2019s resources and whether the capabilities of the server matches the requirements specified in the Model request. For example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nThis model specifies the requirement sklearn\nThere is a default capabilities for each server as follows:\n\nMLServer\n        value: \"mlserver,alibi-detect,alibi-explain,huggingface,lightgbm,mlflow,python,sklearn,spark-mlib,xgboost\"\n\n\n\nTriton\n        value: \"triton,dali,fil,onnx,openvino,python,pytorch,tensorflow,tensorrt\"\n\n\n\n\n\nCustom Capabilities\u00b6\nServers can be defined with a capabilities field to indicate custom configurations (e.g. Python dependencies). For instance:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-134\nspec:\n  serverConfig: mlserver\n  capabilities:\n  - mlserver-1.3.4\n  podSpec:\n    containers:\n    - image: seldonio/mlserver:1.3.4\n      name: mlserver\n\n\nThese capabilities override the ones from the serverConfig: mlserver. A model that takes advantage of this is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - mlserver-1.3.4\n\n\nThis above model will be matched with the previous custom server mlserver-134.\nServers can also be set up with the extraCapabilities that add to existing capabilities from the referenced ServerConfig. For instance:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-extra\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - extra\n\n\nThis server, mlserver-extra, inherits a default set of capabilities via serverConfig: mlserver.\nThese defaults are discussed above.\nThe extraCapabilities are appended to these to create a single list of capabilities for this server.\nModels can then specify requirements to select a server that satisfies those requirements as follows.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: extra-model-requirements\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - extra\n\n\nThe capabilities field takes precedence over the extraCapabilities field.\nFor some examples see here.\n\n\nAutoscaling of Servers\u00b6\nWithin docker we don\u2019t support this but for Kubernetes see here\n\n", "custom-capabilities": "\nCustom Capabilities\u00b6\nServers can be defined with a capabilities field to indicate custom configurations (e.g. Python dependencies). For instance:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-134\nspec:\n  serverConfig: mlserver\n  capabilities:\n  - mlserver-1.3.4\n  podSpec:\n    containers:\n    - image: seldonio/mlserver:1.3.4\n      name: mlserver\n\n\nThese capabilities override the ones from the serverConfig: mlserver. A model that takes advantage of this is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - mlserver-1.3.4\n\n\nThis above model will be matched with the previous custom server mlserver-134.\nServers can also be set up with the extraCapabilities that add to existing capabilities from the referenced ServerConfig. For instance:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-extra\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - extra\n\n\nThis server, mlserver-extra, inherits a default set of capabilities via serverConfig: mlserver.\nThese defaults are discussed above.\nThe extraCapabilities are appended to these to create a single list of capabilities for this server.\nModels can then specify requirements to select a server that satisfies those requirements as follows.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: extra-model-requirements\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - extra\n\n\nThe capabilities field takes precedence over the extraCapabilities field.\nFor some examples see here.\n", "autoscaling-of-servers": "\nAutoscaling of Servers\u00b6\nWithin docker we don\u2019t support this but for Kubernetes see here\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/servers/index.html", "key": "servers"}}, "getting-started/kubernetes-installation/security/aws-msk-mtls": {"sections": {"aws-msk-mtls": "\nAWS MSK mTLS\u00b6\nSeldon will run with AWS MSK.\nAt present we support mTLS authentication to MSK which can be run from a Kubernetes cluster inside or outside Amazon. If running outside your MSK cluster must have a public endpoint.\n\nConsiderations\u00b6\n\nPublic Access to MSK Cluster\u00b6\nIf you running your Kubernetes cluster outside AWS you will need to create a public accessible MSK cluster.\nYou will need to setup Kafka ACLs for your user where the username is the CommonName of the certificate of the client and allow full topic access. For example, to add a user with CN=myname to have full operations using the kafka-acls script with mTLS config setup as described in AWS MSK docs:\nkafka-acls.sh --bootstrap-server <mTLS endpoint>  --add --allow-principal User:CN=myname --operation All --topic '*' --command-config client.properties\n\n\nYou will also need to allow the connecting user to be able to perform admin tasks on the cluster so we can create topics on demand.\nkafka-acls.sh --bootstrap-server <nTLS endpoint>  --add --allow-principal User:CN=myname --operation All --cluster '*' --command-config client.properties\n\n\nYou will need to allow group access also.\nkafka-acls.sh --bootstrap-server <mTKS endpoint>  --add --allow-principal User:CN=myname --operation All --group '*' --command-config client.properties\n\n\n\n\n\nCreate TLS Kubernetes Secrets\u00b6\nCreate a secret for the client certificate you created. If you followed the AWS MSK mTLS guide you will need to export your private key from the JKS keystore. The certificate and chain will be provided in PEM format when you get the certificate signed. You can use these to create a secret with:\n\ntls.key : PEM formatted private key\ntls.crt : PEM formatted certificate\nca.crt : Certificate chain\n\nkubectl create secret generic aws-msk-client --from-file=./tls.key --from-file=./tls.crt --from-file=./ca.crt -n seldon-mesh\n\n\nCreate a secret for the broker certificate. If following the AWS MSK mTLS guide you will need to export the trusstore of Amazon into PEM format and save as ca.crt.\nTo extract certificates from truststore do:\nkeytool -importkeystore -srckeystore truststore.jks    -destkeystore truststore.p12    -srcstoretype jks    -deststoretype pkcs12\nopenssl pkcs12 -in truststore.p12  -nodes -out trust.pem\ncat trust.pem | sed  -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > ca.crt\n\n\nAdd ca.crt to a secret.\nkubectl create secret generic aws-msk-broker-ca --from-file=./ca.crt -n seldon-mesh\n\n\n\nExample Helm install\u00b6\nWe provide a template you can extend in k8s/samples/values-aws-msk-kafka-mtls.yaml.tmpl:\nkafka:\n  bootstrap: <MSK Broker Endpoints>\n\nsecurity:\n  kafka:\n    protocol: SSL\n    ssl:\n      client:\n        secret: aws-msk-client\n        brokerValidationSecret: aws-msk-broker-ca\n\n\nCopy this and modify by adding your broker endpoints.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-aws-msk-kafka-mtls.yaml --set kafka.bootstrap=<your aws msk broker endpoints>\n\n\n\n\n\nTroubleshooting\u00b6\nFirst check AWS MSK troubleshooting.\n\nNo messages are being produced to the topics\u00b6\nSet the kafka config map debug setting to \u201call\u201d. For Helm install you can set kafka.debug=all.\nIf you see an error from the producer in the Pipeline gateway complaining about not enough insync replicas then the replication factor Seldon is using is less than the cluster setting for min.insync.replicas which for a default AWS MSK cluster defaults to 2. Ensure this is equal to that of the cluster. This value can be set in the Helm chart with kafka.topics.replicationFactor.\n\n\n", "considerations": "\nConsiderations\u00b6\n\nPublic Access to MSK Cluster\u00b6\nIf you running your Kubernetes cluster outside AWS you will need to create a public accessible MSK cluster.\nYou will need to setup Kafka ACLs for your user where the username is the CommonName of the certificate of the client and allow full topic access. For example, to add a user with CN=myname to have full operations using the kafka-acls script with mTLS config setup as described in AWS MSK docs:\nkafka-acls.sh --bootstrap-server <mTLS endpoint>  --add --allow-principal User:CN=myname --operation All --topic '*' --command-config client.properties\n\n\nYou will also need to allow the connecting user to be able to perform admin tasks on the cluster so we can create topics on demand.\nkafka-acls.sh --bootstrap-server <nTLS endpoint>  --add --allow-principal User:CN=myname --operation All --cluster '*' --command-config client.properties\n\n\nYou will need to allow group access also.\nkafka-acls.sh --bootstrap-server <mTKS endpoint>  --add --allow-principal User:CN=myname --operation All --group '*' --command-config client.properties\n\n\n\n", "public-access-to-msk-cluster": "\nPublic Access to MSK Cluster\u00b6\nIf you running your Kubernetes cluster outside AWS you will need to create a public accessible MSK cluster.\nYou will need to setup Kafka ACLs for your user where the username is the CommonName of the certificate of the client and allow full topic access. For example, to add a user with CN=myname to have full operations using the kafka-acls script with mTLS config setup as described in AWS MSK docs:\nkafka-acls.sh --bootstrap-server <mTLS endpoint>  --add --allow-principal User:CN=myname --operation All --topic '*' --command-config client.properties\n\n\nYou will also need to allow the connecting user to be able to perform admin tasks on the cluster so we can create topics on demand.\nkafka-acls.sh --bootstrap-server <nTLS endpoint>  --add --allow-principal User:CN=myname --operation All --cluster '*' --command-config client.properties\n\n\nYou will need to allow group access also.\nkafka-acls.sh --bootstrap-server <mTKS endpoint>  --add --allow-principal User:CN=myname --operation All --group '*' --command-config client.properties\n\n\n", "create-tls-kubernetes-secrets": "\nCreate TLS Kubernetes Secrets\u00b6\nCreate a secret for the client certificate you created. If you followed the AWS MSK mTLS guide you will need to export your private key from the JKS keystore. The certificate and chain will be provided in PEM format when you get the certificate signed. You can use these to create a secret with:\n\ntls.key : PEM formatted private key\ntls.crt : PEM formatted certificate\nca.crt : Certificate chain\n\nkubectl create secret generic aws-msk-client --from-file=./tls.key --from-file=./tls.crt --from-file=./ca.crt -n seldon-mesh\n\n\nCreate a secret for the broker certificate. If following the AWS MSK mTLS guide you will need to export the trusstore of Amazon into PEM format and save as ca.crt.\nTo extract certificates from truststore do:\nkeytool -importkeystore -srckeystore truststore.jks    -destkeystore truststore.p12    -srcstoretype jks    -deststoretype pkcs12\nopenssl pkcs12 -in truststore.p12  -nodes -out trust.pem\ncat trust.pem | sed  -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > ca.crt\n\n\nAdd ca.crt to a secret.\nkubectl create secret generic aws-msk-broker-ca --from-file=./ca.crt -n seldon-mesh\n\n\n\nExample Helm install\u00b6\nWe provide a template you can extend in k8s/samples/values-aws-msk-kafka-mtls.yaml.tmpl:\nkafka:\n  bootstrap: <MSK Broker Endpoints>\n\nsecurity:\n  kafka:\n    protocol: SSL\n    ssl:\n      client:\n        secret: aws-msk-client\n        brokerValidationSecret: aws-msk-broker-ca\n\n\nCopy this and modify by adding your broker endpoints.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-aws-msk-kafka-mtls.yaml --set kafka.bootstrap=<your aws msk broker endpoints>\n\n\n\n", "example-helm-install": "\nExample Helm install\u00b6\nWe provide a template you can extend in k8s/samples/values-aws-msk-kafka-mtls.yaml.tmpl:\nkafka:\n  bootstrap: <MSK Broker Endpoints>\n\nsecurity:\n  kafka:\n    protocol: SSL\n    ssl:\n      client:\n        secret: aws-msk-client\n        brokerValidationSecret: aws-msk-broker-ca\n\n\nCopy this and modify by adding your broker endpoints.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-aws-msk-kafka-mtls.yaml --set kafka.bootstrap=<your aws msk broker endpoints>\n\n\n", "troubleshooting": "\nTroubleshooting\u00b6\nFirst check AWS MSK troubleshooting.\n\nNo messages are being produced to the topics\u00b6\nSet the kafka config map debug setting to \u201call\u201d. For Helm install you can set kafka.debug=all.\nIf you see an error from the producer in the Pipeline gateway complaining about not enough insync replicas then the replication factor Seldon is using is less than the cluster setting for min.insync.replicas which for a default AWS MSK cluster defaults to 2. Ensure this is equal to that of the cluster. This value can be set in the Helm chart with kafka.topics.replicationFactor.\n\n", "no-messages-are-being-produced-to-the-topics": "\nNo messages are being produced to the topics\u00b6\nSet the kafka config map debug setting to \u201call\u201d. For Helm install you can set kafka.debug=all.\nIf you see an error from the producer in the Pipeline gateway complaining about not enough insync replicas then the replication factor Seldon is using is less than the cluster setting for min.insync.replicas which for a default AWS MSK cluster defaults to 2. Ensure this is equal to that of the cluster. This value can be set in the Helm chart with kafka.topics.replicationFactor.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/aws-msk-mtls.html", "key": "getting-started/kubernetes-installation/security/aws-msk-mtls"}}, "development/release": {"sections": {"release-process": "\nRelease process\u00b6\n", "id1": "\nRelease Process\u00b6\nThis document summarizes the release process for Seldon Core v2.\nIt is aimed mainly at the maintainers.\n\n:warning: NOTE: This is a work in progress.\nThis is an early version of the release process, which is subject to change.\nPlease, always check this document before conducting a release, and verify if everything goes as expected.\n\n\nProcess Summary\u00b6\n\nCut branch for release, e.g. release-0.1\nRun \u201cDraft New Release\u201d workflow (e.g. choose release-0.1 branch and v0.1.0-rc1 version)\nRun \u201cBuild docker images\u201d workflow (e.g. choose release-0.1 branch and 0.1.0-rc1 tag)\nVerify correctness of created artifacts and images (not yet automated!)\nPublish release\nPublish tags for the Go modules\n\n\n\nProcess discussion\u00b6\nThe development process follows a standard GitHub workflow.\n\nThe main development is happening in the v2 branch.\nThis is where new features land through Pull Requests.\nWhen all features for a new release have been merged, for example v0.1.0, we cut a branch for that release, e.g. release-0.1.\nThe release-0.1 branch will be the base for the v0.1.0 release as well as the release candidates, i.e. v0.1.0-rcX, and successive patch releases, i.e. v0.1.X.\nWe use GitHub Actions to prepare the release, build images and run all necessary testing.\nIf the release draft needs to be updated before being published, the new commits should be merged into the release-0.1 branch and relevant workflows re-triggered as required.\n\n\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n\n\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n\n\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n\n\n", "process-summary": "\nProcess Summary\u00b6\n\nCut branch for release, e.g. release-0.1\nRun \u201cDraft New Release\u201d workflow (e.g. choose release-0.1 branch and v0.1.0-rc1 version)\nRun \u201cBuild docker images\u201d workflow (e.g. choose release-0.1 branch and 0.1.0-rc1 tag)\nVerify correctness of created artifacts and images (not yet automated!)\nPublish release\nPublish tags for the Go modules\n\n", "process-discussion": "\nProcess discussion\u00b6\nThe development process follows a standard GitHub workflow.\n\nThe main development is happening in the v2 branch.\nThis is where new features land through Pull Requests.\nWhen all features for a new release have been merged, for example v0.1.0, we cut a branch for that release, e.g. release-0.1.\nThe release-0.1 branch will be the base for the v0.1.0 release as well as the release candidates, i.e. v0.1.0-rcX, and successive patch releases, i.e. v0.1.X.\nWe use GitHub Actions to prepare the release, build images and run all necessary testing.\nIf the release draft needs to be updated before being published, the new commits should be merged into the release-0.1 branch and relevant workflows re-triggered as required.\n\n\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n\n\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n\n\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n\n", "draft-new-release-action": "\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n", "build-docker-images-action": "\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n", "add-go-module-tags": "\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/development/release/index.html", "key": "development/release"}}, "examples/pipeline-to-pipeline": {"sections": {"pipeline-to-pipeline-examples": "\nPipeline to Pipeline Examples\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Pipeline to Pipeline Examples\u00b6\nThis notebook illustrates a series of Pipelines that are joined together.\n\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n\n\nPipeline pulling from one other Pipeline\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq5dqi8ufs73flaj4g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:26:48.074696631Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended\",\n        \"uid\": \"cieq5h2i8ufs73flaj50\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:27:01.095715504Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pulling from two other Pipelines\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq6aai8ufs73flaj5g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:28:41.766794892Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs.OUTPUT0\n      - tfsimple-extended2.outputs.OUTPUT1\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"cieq6dai8ufs73flaj60\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:53.963808852Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"cieq6dai8ufs73flaj6g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.087670106Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined\", \"uid\":\"cieq6dii8ufs73flaj70\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs.OUTPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.330770841Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id2 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id2\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2 --verbose\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-e468d06afdab8f52-01]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.model.tfsimple2.outputs\ttest-id2\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-73bd1ee54a94d8fb-01]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-fee12926788ce3b6-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1:]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-4df8459a992e0278-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-b2f899a739c5cafd-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-dfa399143feec23d-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pullin from one pipeline with a trigger to another\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"ciepkmii8ufs73flaj2g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:51:06.822716088Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-trigger.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-trigger\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs\n    externalTriggers:\n      - tfsimple-extended2.outputs\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-trigger.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-trigger -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"ciepkoii8ufs73flaj30\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:14.937544974Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"ciepkoii8ufs73flaj3g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.062097751Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-trigger\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-trigger\", \"uid\":\"ciepkoqi8ufs73flaj40\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs\"], \"externalTriggers\":[\"tfsimple-extended2.outputs\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.326170068Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id3 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id3\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2\n\n\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-trigger\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined-trigger.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-trigger.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-trigger\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pulling from one other Pipeline Step\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6m46dpcs73c4qhl0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:52.515491456Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple.step.tfsimple1.outputs\n    tensorMap:\n      tfsimple.step.tfsimple1.outputs.OUTPUT0: INPUT0\n      tfsimple.step.tfsimple1.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended-step.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple-extended-step -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended-step\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended-step\",\n        \"uid\": \"cg5g6ns6dpcs73c4qhlg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.step.tfsimple1.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:59.634720740Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple --verbose\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-90824e4cb70c3242-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\nseldon.default.model.tfsimple1.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tpipeline=[tfsimple]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ca023a540fa463b3-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-843d6ce39292396d-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ee7527353e9fe5a2-01]\tx-forwarded-proto=[http]\n\n\n\nseldon pipeline inspect tfsimple-extended-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\nseldon.default.pipeline.tfsimple-extended-step.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended-step.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n\n\nPipeline pulling from two other Pipeline steps from same model\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6u46dpcs73c4qhm0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:16:24.433333171Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\n      - tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\n    tensorMap:\n      tfsimple-extended.step.tfsimple2.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-step.yaml\n\n\n{}\n{}\n{}\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-step -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\",\"uid\":\"cg5g7046dpcs73c4qhmg\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.576588675Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\",\"uid\":\"cg5g7046dpcs73c4qhn0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.711813099Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-step\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-step\",\"uid\":\"cg5g7046dpcs73c4qhng\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\"],\"tensorMap\":{\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:33.017843490Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.model.tfsimple1.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.pipeline.tfsimple.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-combined-step.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-step.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n\n\n\n\n\n", "seldon-v2-pipeline-to-pipeline-examples": "\nSeldon V2 Pipeline to Pipeline Examples\u00b6\nThis notebook illustrates a series of Pipelines that are joined together.\n\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n\n\nPipeline pulling from one other Pipeline\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq5dqi8ufs73flaj4g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:26:48.074696631Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended\",\n        \"uid\": \"cieq5h2i8ufs73flaj50\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:27:01.095715504Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pulling from two other Pipelines\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq6aai8ufs73flaj5g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:28:41.766794892Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs.OUTPUT0\n      - tfsimple-extended2.outputs.OUTPUT1\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"cieq6dai8ufs73flaj60\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:53.963808852Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"cieq6dai8ufs73flaj6g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.087670106Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined\", \"uid\":\"cieq6dii8ufs73flaj70\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs.OUTPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.330770841Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id2 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id2\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2 --verbose\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-e468d06afdab8f52-01]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.model.tfsimple2.outputs\ttest-id2\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-73bd1ee54a94d8fb-01]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-fee12926788ce3b6-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1:]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-4df8459a992e0278-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-b2f899a739c5cafd-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-dfa399143feec23d-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pullin from one pipeline with a trigger to another\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"ciepkmii8ufs73flaj2g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:51:06.822716088Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-trigger.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-trigger\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs\n    externalTriggers:\n      - tfsimple-extended2.outputs\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-trigger.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-trigger -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"ciepkoii8ufs73flaj30\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:14.937544974Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"ciepkoii8ufs73flaj3g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.062097751Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-trigger\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-trigger\", \"uid\":\"ciepkoqi8ufs73flaj40\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs\"], \"externalTriggers\":[\"tfsimple-extended2.outputs\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.326170068Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id3 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id3\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2\n\n\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-trigger\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined-trigger.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-trigger.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-trigger\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n\n\nPipeline pulling from one other Pipeline Step\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6m46dpcs73c4qhl0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:52.515491456Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple.step.tfsimple1.outputs\n    tensorMap:\n      tfsimple.step.tfsimple1.outputs.OUTPUT0: INPUT0\n      tfsimple.step.tfsimple1.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended-step.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple-extended-step -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended-step\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended-step\",\n        \"uid\": \"cg5g6ns6dpcs73c4qhlg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.step.tfsimple1.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:59.634720740Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple --verbose\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-90824e4cb70c3242-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\nseldon.default.model.tfsimple1.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tpipeline=[tfsimple]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ca023a540fa463b3-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-843d6ce39292396d-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ee7527353e9fe5a2-01]\tx-forwarded-proto=[http]\n\n\n\nseldon pipeline inspect tfsimple-extended-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\nseldon.default.pipeline.tfsimple-extended-step.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended-step.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n\n\nPipeline pulling from two other Pipeline steps from same model\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6u46dpcs73c4qhm0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:16:24.433333171Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\n      - tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\n    tensorMap:\n      tfsimple-extended.step.tfsimple2.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-step.yaml\n\n\n{}\n{}\n{}\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-step -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\",\"uid\":\"cg5g7046dpcs73c4qhmg\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.576588675Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\",\"uid\":\"cg5g7046dpcs73c4qhn0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.711813099Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-step\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-step\",\"uid\":\"cg5g7046dpcs73c4qhng\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\"],\"tensorMap\":{\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:33.017843490Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.model.tfsimple1.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.pipeline.tfsimple.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-combined-step.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-step.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n\n\n\n\n", "models-used": "\nModels Used\u00b6\n\ngs://seldon-models/triton/simple an example Triton tensorflow model that takes 2 inputs INPUT0 and INPUT1 and adds them to produce OUTPUT0 and also subtracts INPUT1 from INPUT0 to produce OUTPUT1. See here for the original source code and license.\nOther models can be found at https://github.com/SeldonIO/triton-python-examples\n\n", "pipeline-pulling-from-one-other-pipeline": "\nPipeline pulling from one other Pipeline\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq5dqi8ufs73flaj4g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:26:48.074696631Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended\",\n        \"uid\": \"cieq5h2i8ufs73flaj50\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:27:01.095715504Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n", "pipeline-pulling-from-two-other-pipelines": "\nPipeline pulling from two other Pipelines\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cieq6aai8ufs73flaj5g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T15:28:41.766794892Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs.OUTPUT0\n      - tfsimple-extended2.outputs.OUTPUT1\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"cieq6dai8ufs73flaj60\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:53.963808852Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"cieq6dai8ufs73flaj6g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.087670106Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined\", \"uid\":\"cieq6dii8ufs73flaj70\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs.OUTPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended2.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T15:28:54.330770841Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id2 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id2\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2 --verbose\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-e468d06afdab8f52-01]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.model.tfsimple2.outputs\ttest-id2\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-73bd1ee54a94d8fb-01]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-fee12926788ce3b6-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1:]\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id2\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\t\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1:]\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-4df8459a992e0278-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tpipeline=[tfsimple-extended]\ttraceparent=[00-3a6047efa647efc2b3fc5266ae023d23-b2f899a739c5cafd-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id]\tx-forwarded-proto=[http]\tx-envoy-upstream-service-time=[5]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id2\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\t\tx-envoy-upstream-service-time=[1]\tpipeline=[tfsimple-extended]\ttraceparent=[00-e438b82ad361ac2d5481bcfc494074d2-dfa399143feec23d-01]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[test-id2]\tx-forwarded-proto=[http]\tx-seldon-route=[:tfsimple1_1: :tfsimple2_1:]\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined.inputs\ttest-id\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined.outputs\ttest-id\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n", "pipeline-pullin-from-one-pipeline-with-a-trigger-to-another": "\nPipeline pullin from one pipeline with a trigger to another\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"ciepkmii8ufs73flaj2g\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-29T14:51:06.822716088Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-trigger.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-trigger\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.outputs\n    externalTriggers:\n      - tfsimple-extended2.outputs\n    tensorMap:\n      tfsimple-extended.outputs.OUTPUT0: INPUT0\n      tfsimple-extended.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-trigger.yaml\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-trigger -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\", \"uid\":\"ciepkoii8ufs73flaj30\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:14.937544974Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\", \"uid\":\"ciepkoii8ufs73flaj3g\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple.outputs\"], \"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.062097751Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-trigger\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-trigger\", \"uid\":\"ciepkoqi8ufs73flaj40\", \"version\":1, \"steps\":[{\"name\":\"tfsimple2\"}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}, \"input\":{\"externalInputs\":[\"tfsimple-extended.outputs\"], \"externalTriggers\":[\"tfsimple-extended2.outputs\"], \"tensorMap\":{\"tfsimple-extended.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple-extended.outputs.OUTPUT1\":\"INPUT1\"}}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:51:15.326170068Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple --header x-request-id=test-id3 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.model.tfsimple1.outputs\ttest-id3\t{\"modelName\":\"tfsimple1_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}}]}\nseldon.default.pipeline.tfsimple.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended --offset 2\n\n\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2 --offset 2\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\nseldon.default.pipeline.tfsimple-extended2.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-trigger\n\n\nseldon.default.model.tfsimple2.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\ttest-id3\t{\"modelName\":\"tfsimple2_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\nseldon.default.pipeline.tfsimple-combined-trigger.inputs\ttest-id3\t{\"inputs\":[{\"name\":\"INPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}, {\"name\":\"INPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]}}], \"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\", \"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-trigger.outputs\ttest-id3\t{\"outputs\":[{\"name\":\"OUTPUT0\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]}}, {\"name\":\"OUTPUT1\", \"datatype\":\"INT32\", \"shape\":[\"1\", \"16\"], \"contents\":{\"intContents\":[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-trigger\nseldon pipeline unload tfsimple\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n", "pipeline-pulling-from-one-other-pipeline-step": "\nPipeline pulling from one other Pipeline Step\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6m46dpcs73c4qhl0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:52.515491456Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple.step.tfsimple1.outputs\n    tensorMap:\n      tfsimple.step.tfsimple1.outputs.OUTPUT0: INPUT0\n      tfsimple.step.tfsimple1.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended-step.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple-extended-step -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple-extended-step\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple-extended-step\",\n        \"uid\": \"cg5g6ns6dpcs73c4qhlg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple2\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple2.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {},\n        \"input\": {\n          \"externalInputs\": [\n            \"tfsimple.step.tfsimple1.outputs\"\n          ],\n          \"tensorMap\": {\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT0\": \"INPUT0\",\n            \"tfsimple.step.tfsimple1.outputs.OUTPUT1\": \"INPUT1\"\n          }\n        }\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:15:59.634720740Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple --verbose\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-90824e4cb70c3242-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\nseldon.default.model.tfsimple1.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tpipeline=[tfsimple]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ca023a540fa463b3-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\t\tpipeline=[tfsimple]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-843d6ce39292396d-01]\tx-forwarded-proto=[http]\tx-envoy-expected-rq-timeout-ms=[60000]\nseldon.default.pipeline.tfsimple.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\t\tx-envoy-expected-rq-timeout-ms=[60000]\tx-request-id=[cg5g6ogfh5ss73a44vvg]\tx-envoy-upstream-service-time=[8]\tx-seldon-route=[:tfsimple1_1:]\tpipeline=[tfsimple]\ttraceparent=[00-2c66ff815d920ad238365be52a4467f5-ee7527353e9fe5a2-01]\tx-forwarded-proto=[http]\n\n\n\nseldon pipeline inspect tfsimple-extended-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g6ogfh5ss73a44vvg\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\nseldon.default.pipeline.tfsimple-extended-step.inputs\tcg5g6ogfh5ss73a44vvg\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended-step.outputs\tcg5g6ogfh5ss73a44vvg\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n", "pipeline-pulling-from-two-other-pipeline-steps-from-same-model": "\nPipeline pulling from two other Pipeline steps from same model\u00b6\n\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model load -f ./models/tfsimple2.yaml\n\n\n{}\n{}\n\n\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ./pipelines/tfsimple.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple\nspec:\n  steps:\n    - name: tfsimple1\n  output:\n    steps:\n    - tfsimple1\n\n\nseldon pipeline load -f ./pipelines/tfsimple.yaml\n\n\n{}\n\n\nseldon pipeline status tfsimple -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"tfsimple\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"tfsimple\",\n        \"uid\": \"cg5g6u46dpcs73c4qhm0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"tfsimple1\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"tfsimple1.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-03-10T10:16:24.433333171Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"\",\n  \"outputs\": [\n    {\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ],\n      \"name\": \"OUTPUT0\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    },\n    {\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"name\": \"OUTPUT1\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"datatype\": \"INT32\"\n    }\n  ]\n}\n\n\ncat ./pipelines/tfsimple-extended.yaml\necho \"---\"\ncat ./pipelines/tfsimple-extended2.yaml\necho \"---\"\ncat ./pipelines/tfsimple-combined-step.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-extended2\nspec:\n  input:\n    externalInputs:\n      - tfsimple.outputs\n    tensorMap:\n      tfsimple.outputs.OUTPUT0: INPUT0\n      tfsimple.outputs.OUTPUT1: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimple-combined-step\nspec:\n  input:\n    externalInputs:\n      - tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\n      - tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\n    tensorMap:\n      tfsimple-extended.step.tfsimple2.outputs.OUTPUT0: INPUT0\n      tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0: INPUT1\n  steps:\n    - name: tfsimple2\n  output:\n    steps:\n    - tfsimple2\n\n\nseldon pipeline load -f ./pipelines/tfsimple-extended.yaml\nseldon pipeline load -f ./pipelines/tfsimple-extended2.yaml\nseldon pipeline load -f ./pipelines/tfsimple-combined-step.yaml\n\n\n{}\n{}\n{}\n\n\nseldon pipeline status tfsimple-extended -w PipelineReady\nseldon pipeline status tfsimple-extended2 -w PipelineReady\nseldon pipeline status tfsimple-combined-step -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimple-extended\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended\",\"uid\":\"cg5g7046dpcs73c4qhmg\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.576588675Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-extended2\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-extended2\",\"uid\":\"cg5g7046dpcs73c4qhn0\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple.outputs\"],\"tensorMap\":{\"tfsimple.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple.outputs.OUTPUT1\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:32.711813099Z\",\"modelsReady\":true}}]}\n{\"pipelineName\":\"tfsimple-combined-step\",\"versions\":[{\"pipeline\":{\"name\":\"tfsimple-combined-step\",\"uid\":\"cg5g7046dpcs73c4qhng\",\"version\":1,\"steps\":[{\"name\":\"tfsimple2\"}],\"output\":{\"steps\":[\"tfsimple2.outputs\"]},\"kubernetesMeta\":{},\"input\":{\"externalInputs\":[\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\"],\"tensorMap\":{\"tfsimple-extended.step.tfsimple2.outputs.OUTPUT0\":\"INPUT0\",\"tfsimple-extended2.step.tfsimple2.outputs.OUTPUT0\":\"INPUT1\"}}},\"state\":{\"pipelineVersion\":1,\"status\":\"PipelineReady\",\"reason\":\"created pipeline\",\"lastChangeTimestamp\":\"2023-03-10T10:16:33.017843490Z\",\"modelsReady\":true}}]}\n\n\nseldon pipeline infer tfsimple \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t2,\n\t\t\t\t4,\n\t\t\t\t6,\n\t\t\t\t8,\n\t\t\t\t10,\n\t\t\t\t12,\n\t\t\t\t14,\n\t\t\t\t16,\n\t\t\t\t18,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t24,\n\t\t\t\t26,\n\t\t\t\t28,\n\t\t\t\t30,\n\t\t\t\t32\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t},\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0,\n\t\t\t\t0\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t16\n\t\t\t],\n\t\t\t\"datatype\": \"INT32\"\n\t\t}\n\t]\n}\n\n\nseldon pipeline inspect tfsimple\n\n\nseldon.default.model.tfsimple1.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.model.tfsimple1.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple1_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}}]}\nseldon.default.pipeline.tfsimple.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-extended2\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-extended2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\"]}\nseldon.default.pipeline.tfsimple-extended2.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}]}\n\n\n\nseldon pipeline inspect tfsimple-combined-step\n\n\nseldon.default.model.tfsimple2.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.model.tfsimple2.outputs\tcg5g710fh5ss73a4500g\t{\"modelName\":\"tfsimple2_1\",\"modelVersion\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\nseldon.default.pipeline.tfsimple-combined-step.inputs\tcg5g710fh5ss73a4500g\t{\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]}}],\"rawInputContents\":[\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\",\"AgAAAAQAAAAGAAAACAAAAAoAAAAMAAAADgAAABAAAAASAAAAFAAAABYAAAAYAAAAGgAAABwAAAAeAAAAIAAAAA==\"]}\nseldon.default.pipeline.tfsimple-combined-step.outputs\tcg5g710fh5ss73a4500g\t{\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64]}},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[\"1\",\"16\"],\"contents\":{\"intContents\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}}]}\n\n\n\nseldon pipeline unload tfsimple-extended\nseldon pipeline unload tfsimple-extended2\nseldon pipeline unload tfsimple-combined-step\nseldon pipeline unload tfsimple\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\n{}\n{}\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/pipeline-to-pipeline.html", "key": "examples/pipeline-to-pipeline"}}, "cli/docs/seldon_experiment_status": {"sections": {"seldon-experiment-status": "\nseldon experiment status\u00b6\nget status for experiment\n\nSynopsis\u00b6\nget status for experiment\nseldon experiment status <experimentName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait                    wait for experiment to be active\n\n\n\n\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n\n", "synopsis": "\nSynopsis\u00b6\nget status for experiment\nseldon experiment status <experimentName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait                    wait for experiment to be active\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_experiment_status.html", "key": "cli/docs/seldon_experiment_status"}}, "examples": {"sections": {"examples": "\nExamples\u00b6\nThis section will provide some examples to allow operations with Seldon to be tested so you can run your own models, experiments, pipelines and explainers.\n\nGetting Started Examples\u00b6\n\nLocal examples\nKubernetes examples\n\n\n\nModels\u00b6\n\nHuggingface models\nModel zoo\nArtifact versions\n\n\n\nPipelines\u00b6\n\nPipeline examples\nPipeline to pipeline examples\n\n\n\nExplainers\u00b6\n\nExplainer examples\n\n\n\nServers\u00b6\n\nCustom Servers\n\n\n\nExperiments\u00b6\n\nLocal experiments\nExperiment version examples\n\n\n\nMaking Inference Requests\u00b6\n\nInference examples\nTritonclient examples\nBatch Inference examples (kubernetes)\nBatch Inference examples (local)\n\n\n\nMisc\u00b6\n\nChecking Pipeline readiness\n\n\n\nFurther Kubernetes Examples\u00b6\n\nKubernetes custerwide example\n\n\n\nAdvanced Examples\u00b6\n\nHuggingface speech to sentiment with explanations pipeline\nProduction image classifier with drift and outlier monitoring\nProduction income classifier with drift, outlier and explanations\nConditional pipeline with pandas query model\nKubernetes Server with PVC\n\n\n\n\n", "getting-started-examples": "\nGetting Started Examples\u00b6\n\nLocal examples\nKubernetes examples\n\n", "models": "\nModels\u00b6\n\nHuggingface models\nModel zoo\nArtifact versions\n\n", "pipelines": "\nPipelines\u00b6\n\nPipeline examples\nPipeline to pipeline examples\n\n", "explainers": "\nExplainers\u00b6\n\nExplainer examples\n\n", "servers": "\nServers\u00b6\n\nCustom Servers\n\n", "experiments": "\nExperiments\u00b6\n\nLocal experiments\nExperiment version examples\n\n", "making-inference-requests": "\nMaking Inference Requests\u00b6\n\nInference examples\nTritonclient examples\nBatch Inference examples (kubernetes)\nBatch Inference examples (local)\n\n", "misc": "\nMisc\u00b6\n\nChecking Pipeline readiness\n\n", "further-kubernetes-examples": "\nFurther Kubernetes Examples\u00b6\n\nKubernetes custerwide example\n\n", "advanced-examples": "\nAdvanced Examples\u00b6\n\nHuggingface speech to sentiment with explanations pipeline\nProduction image classifier with drift and outlier monitoring\nProduction income classifier with drift, outlier and explanations\nConditional pipeline with pandas query model\nKubernetes Server with PVC\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/index.html", "key": "examples"}}, "kubernetes/kafka": {"sections": {"kafka": "\nKafka\u00b6\nSeldon Core v2 requires Kafka to implement data-centric inference Pipelines.\nSee our architecture documentation to learn more on how Seldon Core v2 uses Kafka.\n\nNote\nKafka integration is required to enable data-centric inference pipelines feature.\nIt is highly advice to configure Kafka integration to take full advantage of Seldon Core v2 features.\n\nWe list alternatives below.\n\nManaged Kafka\u00b6\nWe recommend to use managed Kafka solution for production installation.\nThis allow to take away all the complexity on running secure and scalable Kafka cluster away.\nWe currently have tested and documented integration with following managed solutions:\n\nConfluent Cloud (security: SASL/PLAIN)\nConfluent Cloud (security: SASL/OAUTHBEARER)\nAmazon MSK (security: mTLS)\nAmazon MSK (security: SASL/SCRAM)\nAzure Event Hub (security: SASL/PLAIN)\n\nSee our Kafka security section for configuration examples.\n\n\nSelf Hosted Kafka\u00b6\n\nStrimzi Kafka\u00b6\nSeldon Core v2 requires Kafka to implement data-centric inference Pipelines.\nTo install Kafka for testing purposed in your k8s cluster, we recommend to use Strimzi Operator.\n\nNote\nThis page discuss how to install Strimzi Operator and create Kafka cluster for trial, dev, or testing purposes.\nFor production grade installation consult Strimzi documentation or use one of managed solutions mentioned here.\n\nYou can install and configure Strimzi using either Helm charts or our Ansible playbooks, both documented below.\n\nHelm\u00b6\nThe installation of a Kafka cluster requires the Strimzi Kafka operator installed in the same namespace.\nThis allows to directly use the mTLS certificates created by Strimzi Operator.\nOne option to install the Strimzi operator is via Helm.\nNote that we are using here KRaft instead of Zookeeper for Kafka.\nYou can enable featureGates during Helm installation via:\nhelm upgrade --install strimzi-kafka-operator \\\n  strimzi/strimzi-kafka-operator \\\n  --namespace seldon-mesh --create-namespace \\\n  --set featureGates='+UseKRaft\\,+UseStrimziPodSets'\n\n\n\nWarning\nUse with caution!\nCurrently Kraft installation of Strimzi is not production ready.\nSee Strimzi documentation and related GitHub issue for further details.\n\nCreate Kafka cluster in seldon-mesh namespace\nhelm upgrade seldon-core-v2-kafka kafka/strimzi -n seldon-mesh --install\n\n\nNote that a specific strimzi operator version is associated with a subset of supported Kafka versions.\n\n\nAnsible\u00b6\nWe provide automation around the installation of a Kafka cluster for Seldon Core v2 to help with development and testing use cases.\nYou can follow the steps defined here to install Kafka via ansible.\nYou can use our Ansible playbooks to install only Strimzi Operator and Kafka cluster by setting extra Ansible vars:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\n\n\nNotes\u00b6\n\nYou can check kafka-examples for more details.\nAs we are using KRaft, use Kafka version 3.3 or above.\nFor security settings check here.\n\n\n\n\n", "managed-kafka": "\nManaged Kafka\u00b6\nWe recommend to use managed Kafka solution for production installation.\nThis allow to take away all the complexity on running secure and scalable Kafka cluster away.\nWe currently have tested and documented integration with following managed solutions:\n\nConfluent Cloud (security: SASL/PLAIN)\nConfluent Cloud (security: SASL/OAUTHBEARER)\nAmazon MSK (security: mTLS)\nAmazon MSK (security: SASL/SCRAM)\nAzure Event Hub (security: SASL/PLAIN)\n\nSee our Kafka security section for configuration examples.\n", "self-hosted-kafka": "\nSelf Hosted Kafka\u00b6\n\nStrimzi Kafka\u00b6\nSeldon Core v2 requires Kafka to implement data-centric inference Pipelines.\nTo install Kafka for testing purposed in your k8s cluster, we recommend to use Strimzi Operator.\n\nNote\nThis page discuss how to install Strimzi Operator and create Kafka cluster for trial, dev, or testing purposes.\nFor production grade installation consult Strimzi documentation or use one of managed solutions mentioned here.\n\nYou can install and configure Strimzi using either Helm charts or our Ansible playbooks, both documented below.\n\nHelm\u00b6\nThe installation of a Kafka cluster requires the Strimzi Kafka operator installed in the same namespace.\nThis allows to directly use the mTLS certificates created by Strimzi Operator.\nOne option to install the Strimzi operator is via Helm.\nNote that we are using here KRaft instead of Zookeeper for Kafka.\nYou can enable featureGates during Helm installation via:\nhelm upgrade --install strimzi-kafka-operator \\\n  strimzi/strimzi-kafka-operator \\\n  --namespace seldon-mesh --create-namespace \\\n  --set featureGates='+UseKRaft\\,+UseStrimziPodSets'\n\n\n\nWarning\nUse with caution!\nCurrently Kraft installation of Strimzi is not production ready.\nSee Strimzi documentation and related GitHub issue for further details.\n\nCreate Kafka cluster in seldon-mesh namespace\nhelm upgrade seldon-core-v2-kafka kafka/strimzi -n seldon-mesh --install\n\n\nNote that a specific strimzi operator version is associated with a subset of supported Kafka versions.\n\n\nAnsible\u00b6\nWe provide automation around the installation of a Kafka cluster for Seldon Core v2 to help with development and testing use cases.\nYou can follow the steps defined here to install Kafka via ansible.\nYou can use our Ansible playbooks to install only Strimzi Operator and Kafka cluster by setting extra Ansible vars:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\n\n\nNotes\u00b6\n\nYou can check kafka-examples for more details.\nAs we are using KRaft, use Kafka version 3.3 or above.\nFor security settings check here.\n\n\n\n", "strimzi-kafka": "\nStrimzi Kafka\u00b6\nSeldon Core v2 requires Kafka to implement data-centric inference Pipelines.\nTo install Kafka for testing purposed in your k8s cluster, we recommend to use Strimzi Operator.\n\nNote\nThis page discuss how to install Strimzi Operator and create Kafka cluster for trial, dev, or testing purposes.\nFor production grade installation consult Strimzi documentation or use one of managed solutions mentioned here.\n\nYou can install and configure Strimzi using either Helm charts or our Ansible playbooks, both documented below.\n\nHelm\u00b6\nThe installation of a Kafka cluster requires the Strimzi Kafka operator installed in the same namespace.\nThis allows to directly use the mTLS certificates created by Strimzi Operator.\nOne option to install the Strimzi operator is via Helm.\nNote that we are using here KRaft instead of Zookeeper for Kafka.\nYou can enable featureGates during Helm installation via:\nhelm upgrade --install strimzi-kafka-operator \\\n  strimzi/strimzi-kafka-operator \\\n  --namespace seldon-mesh --create-namespace \\\n  --set featureGates='+UseKRaft\\,+UseStrimziPodSets'\n\n\n\nWarning\nUse with caution!\nCurrently Kraft installation of Strimzi is not production ready.\nSee Strimzi documentation and related GitHub issue for further details.\n\nCreate Kafka cluster in seldon-mesh namespace\nhelm upgrade seldon-core-v2-kafka kafka/strimzi -n seldon-mesh --install\n\n\nNote that a specific strimzi operator version is associated with a subset of supported Kafka versions.\n\n\nAnsible\u00b6\nWe provide automation around the installation of a Kafka cluster for Seldon Core v2 to help with development and testing use cases.\nYou can follow the steps defined here to install Kafka via ansible.\nYou can use our Ansible playbooks to install only Strimzi Operator and Kafka cluster by setting extra Ansible vars:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\n\n\nNotes\u00b6\n\nYou can check kafka-examples for more details.\nAs we are using KRaft, use Kafka version 3.3 or above.\nFor security settings check here.\n\n\n", "helm": "\nHelm\u00b6\nThe installation of a Kafka cluster requires the Strimzi Kafka operator installed in the same namespace.\nThis allows to directly use the mTLS certificates created by Strimzi Operator.\nOne option to install the Strimzi operator is via Helm.\nNote that we are using here KRaft instead of Zookeeper for Kafka.\nYou can enable featureGates during Helm installation via:\nhelm upgrade --install strimzi-kafka-operator \\\n  strimzi/strimzi-kafka-operator \\\n  --namespace seldon-mesh --create-namespace \\\n  --set featureGates='+UseKRaft\\,+UseStrimziPodSets'\n\n\n\nWarning\nUse with caution!\nCurrently Kraft installation of Strimzi is not production ready.\nSee Strimzi documentation and related GitHub issue for further details.\n\nCreate Kafka cluster in seldon-mesh namespace\nhelm upgrade seldon-core-v2-kafka kafka/strimzi -n seldon-mesh --install\n\n\nNote that a specific strimzi operator version is associated with a subset of supported Kafka versions.\n", "ansible": "\nAnsible\u00b6\nWe provide automation around the installation of a Kafka cluster for Seldon Core v2 to help with development and testing use cases.\nYou can follow the steps defined here to install Kafka via ansible.\nYou can use our Ansible playbooks to install only Strimzi Operator and Kafka cluster by setting extra Ansible vars:\nansible-playbook playbooks/setup-ecosystem.yaml -e full_install=no -e install_kafka=yes\n\n\n", "notes": "\nNotes\u00b6\n\nYou can check kafka-examples for more details.\nAs we are using KRaft, use Kafka version 3.3 or above.\nFor security settings check here.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/kafka/index.html", "key": "kubernetes/kafka"}}, "kubernetes/scaling": {"sections": {"scaling": "\nScaling\u00b6\n\nModels\u00b6\nModels can be scaled by setting their replica count, e.g.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.2.3/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n  replicas: 3\n\n\nCurrently, the number of replicas will need not to exceed the replicas of the Server the model is scheduled to.\n\n\nServers\u00b6\nServers can be scaled by setting their replica count, e.g.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver\n  namespace: seldon\nspec:\n  replicas: 4\n  serverConfig: mlserver\n\n\nCurrently, models scheduled to a server can only scale up to the server replica count.\n\n\nInternal Components\u00b6\nSeldon Core v2 runs with several control and dataplane components. The scaling of these resources is discussed below:\n\nPipeline gateway.\n\nThis pipeline gateway handles REST and gRPC synchronous requests to Pipelines. It is stateless and can be scaled based on traffic demand.\n\n\nModel gateway.\n\nThis component pulls model requests from Kafka and sends them to inference servers. It can be scaled up to the partition factor of your Kafka topics. At present we set a uniform partition factor for all topics in one installation of Seldon Core V2.\n\n\nDataflow engine.\n\nThe dataflow engine runs KStream topologies to manage Pipelines. It can run as multiple replicas and the scheduler will balance Pipelines to run across it with a consistent hashing load balancer. Each Pipeline is managed up to the partition factor of Kafka (presently hardwired to one).\n\n\nScheduler.\n\nThis manages the control plane operations. It is presently required to be one replica as it maintains internal state within a BadgerDB held on local persistent storage (stateful set in Kubernetes). Performance tests have shown this not to be a bottleneck at present.\n\n\nKubernetes Controller.\n\nThe Kubernetes controller manages resources updates on the cluster which it passes on to the Scheduler. It is by default one replica but has the ability to scale.\n\n\nEnvoy\n\nEnvoy replicas get their state from the scheduler for routing information and can be scaled as needed.\n\n\n\n\nFuture Enhancements\u00b6\n\nAllow configuration of partition factor for data plane consistent hashing load balancer.\nAllow Model gateway and Pipeline gateway to use consistent hashing load balancer.\nConsider control plane scaling options.\n\n\n\n", "models": "\nModels\u00b6\nModels can be scaled by setting their replica count, e.g.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.2.3/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n  replicas: 3\n\n\nCurrently, the number of replicas will need not to exceed the replicas of the Server the model is scheduled to.\n", "servers": "\nServers\u00b6\nServers can be scaled by setting their replica count, e.g.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver\n  namespace: seldon\nspec:\n  replicas: 4\n  serverConfig: mlserver\n\n\nCurrently, models scheduled to a server can only scale up to the server replica count.\n", "internal-components": "\nInternal Components\u00b6\nSeldon Core v2 runs with several control and dataplane components. The scaling of these resources is discussed below:\n\nPipeline gateway.\n\nThis pipeline gateway handles REST and gRPC synchronous requests to Pipelines. It is stateless and can be scaled based on traffic demand.\n\n\nModel gateway.\n\nThis component pulls model requests from Kafka and sends them to inference servers. It can be scaled up to the partition factor of your Kafka topics. At present we set a uniform partition factor for all topics in one installation of Seldon Core V2.\n\n\nDataflow engine.\n\nThe dataflow engine runs KStream topologies to manage Pipelines. It can run as multiple replicas and the scheduler will balance Pipelines to run across it with a consistent hashing load balancer. Each Pipeline is managed up to the partition factor of Kafka (presently hardwired to one).\n\n\nScheduler.\n\nThis manages the control plane operations. It is presently required to be one replica as it maintains internal state within a BadgerDB held on local persistent storage (stateful set in Kubernetes). Performance tests have shown this not to be a bottleneck at present.\n\n\nKubernetes Controller.\n\nThe Kubernetes controller manages resources updates on the cluster which it passes on to the Scheduler. It is by default one replica but has the ability to scale.\n\n\nEnvoy\n\nEnvoy replicas get their state from the scheduler for routing information and can be scaled as needed.\n\n\n\n\nFuture Enhancements\u00b6\n\nAllow configuration of partition factor for data plane consistent hashing load balancer.\nAllow Model gateway and Pipeline gateway to use consistent hashing load balancer.\nConsider control plane scaling options.\n\n\n", "future-enhancements": "\nFuture Enhancements\u00b6\n\nAllow configuration of partition factor for data plane consistent hashing load balancer.\nAllow Model gateway and Pipeline gateway to use consistent hashing load balancer.\nConsider control plane scaling options.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/scaling/index.html", "key": "kubernetes/scaling"}}, "cli/docs/seldon_model_status": {"sections": {"seldon-model-status": "\nseldon model status\u00b6\nget status for model\n\nSynopsis\u00b6\nget the status for a model\nseldon model status <modelName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait string             model wait condition\n\n\n\n\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n\n", "synopsis": "\nSynopsis\u00b6\nget the status for a model\nseldon model status <modelName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for status\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -w, --wait string             model wait condition\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_model_status.html", "key": "cli/docs/seldon_model_status"}}, "apis/internal/chainer": {"sections": {"chainer-api": "\nChainer API\u00b6\nThis API is for communication between the Seldon Scheduler and the Seldon dataflow component which manages pipelines.\n\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.chainer;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/chainer\";\noption java_package = \"io.seldon.mlops.chainer\";\n\nmessage PipelineSubscriptionRequest {\n  string name = 1;\n}\n\nmessage PipelineUpdateMessage {\n  enum PipelineOperation {\n    Unknown = 0;\n    Create = 1;\n    Delete = 2;\n  }\n  PipelineOperation op = 1;\n  string pipeline = 2;\n  uint32 version = 3;\n  string uid = 4;\n  repeated PipelineStepUpdate updates = 5;\n}\n\nmessage PipelineStepUpdate {\n  enum PipelineJoinType {\n    Unknown = 0;\n    Inner = 1;\n    Outer = 2;\n    Any = 3;\n  }\n  // https://docs.google.com/document/d/1tX-uaOvngx1RpEyWEZ4EbEcU8D0OgYuRWVb2UAi85n4/edit\n  // Pipeline Resource example, e.g. transform.outputs.traffic\n  //    seldon.<namespace>.<model name>.<inputs|outputs>.<tensor name>\n  repeated PipelineTopic sources = 1;\n  repeated PipelineTopic triggers = 2;\n  PipelineTopic sink = 3;\n  PipelineJoinType inputJoinTy = 4;\n  PipelineJoinType triggersJoinTy = 5;\n  bool passEmptyResponses = 6; // Forward empty response to following steps, default false\n  optional uint32 joinWindowMs = 7; // Join window millisecs, some nozero default (TBD)\n  repeated PipelineTensorMapping tensorMap = 8; // optional list of tensor name mappings\n  Batch batch = 9; // Batch settings\n}\n\nmessage PipelineTensorMapping {\n  string pipelineName = 1;\n  string topicAndTensor = 2;\n  string tensorName = 3;\n}\n\nmessage PipelineTopic {\n  string pipelineName = 1;\n  string topicName = 2;\n  optional string tensor = 3;\n}\n\nmessage Batch {\n  optional uint32 size = 1;\n  optional uint32 windowMs = 2;\n  bool rolling = 3;\n}\n\nmessage PipelineUpdateStatusMessage {\n  // TODO - include `name` to identify transformer message comes from\n  PipelineUpdateMessage update = 1;\n  bool success = 2;\n  string reason = 3;\n}\n\nmessage PipelineUpdateStatusResponse {\n}\n\nservice Chainer {\n  rpc SubscribePipelineUpdates(PipelineSubscriptionRequest) returns (stream PipelineUpdateMessage) {};\n  rpc PipelineUpdateEvent(PipelineUpdateStatusMessage) returns (PipelineUpdateStatusResponse) {};\n}\n\n\n\n", "proto-definition": "\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.chainer;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/chainer\";\noption java_package = \"io.seldon.mlops.chainer\";\n\nmessage PipelineSubscriptionRequest {\n  string name = 1;\n}\n\nmessage PipelineUpdateMessage {\n  enum PipelineOperation {\n    Unknown = 0;\n    Create = 1;\n    Delete = 2;\n  }\n  PipelineOperation op = 1;\n  string pipeline = 2;\n  uint32 version = 3;\n  string uid = 4;\n  repeated PipelineStepUpdate updates = 5;\n}\n\nmessage PipelineStepUpdate {\n  enum PipelineJoinType {\n    Unknown = 0;\n    Inner = 1;\n    Outer = 2;\n    Any = 3;\n  }\n  // https://docs.google.com/document/d/1tX-uaOvngx1RpEyWEZ4EbEcU8D0OgYuRWVb2UAi85n4/edit\n  // Pipeline Resource example, e.g. transform.outputs.traffic\n  //    seldon.<namespace>.<model name>.<inputs|outputs>.<tensor name>\n  repeated PipelineTopic sources = 1;\n  repeated PipelineTopic triggers = 2;\n  PipelineTopic sink = 3;\n  PipelineJoinType inputJoinTy = 4;\n  PipelineJoinType triggersJoinTy = 5;\n  bool passEmptyResponses = 6; // Forward empty response to following steps, default false\n  optional uint32 joinWindowMs = 7; // Join window millisecs, some nozero default (TBD)\n  repeated PipelineTensorMapping tensorMap = 8; // optional list of tensor name mappings\n  Batch batch = 9; // Batch settings\n}\n\nmessage PipelineTensorMapping {\n  string pipelineName = 1;\n  string topicAndTensor = 2;\n  string tensorName = 3;\n}\n\nmessage PipelineTopic {\n  string pipelineName = 1;\n  string topicName = 2;\n  optional string tensor = 3;\n}\n\nmessage Batch {\n  optional uint32 size = 1;\n  optional uint32 windowMs = 2;\n  bool rolling = 3;\n}\n\nmessage PipelineUpdateStatusMessage {\n  // TODO - include `name` to identify transformer message comes from\n  PipelineUpdateMessage update = 1;\n  bool success = 2;\n  string reason = 3;\n}\n\nmessage PipelineUpdateStatusResponse {\n}\n\nservice Chainer {\n  rpc SubscribePipelineUpdates(PipelineSubscriptionRequest) returns (stream PipelineUpdateMessage) {};\n  rpc PipelineUpdateEvent(PipelineUpdateStatusMessage) returns (PipelineUpdateStatusResponse) {};\n}\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/internal/chainer.html", "key": "apis/internal/chainer"}}, "cli/docs/seldon_config_add": {"sections": {"seldon-config-add": "\nseldon config add\u00b6\nadd config\n\nSynopsis\u00b6\nadd config\nseldon config add [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for add\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n\n", "synopsis": "\nSynopsis\u00b6\nadd config\nseldon config add [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for add\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config_add.html", "key": "cli/docs/seldon_config_add"}}, "apis/inference/v2": {"sections": {"open-inference-protocol": "\nOpen Inference Protocol\u00b6\nThis page describes a predict/inference API independent of any\nspecific ML/DL framework and model server. These APIs are\nable to support both easy-to-use and high-performance use cases.\nBy implementing this protocol both\ninference clients and servers will increase their utility and\nportability by being able to operate seamlessly on platforms that have\nstandardized around this API. This protocol is endorsed by NVIDIA\nTriton Inference Server, TensorFlow Serving, and ONNX Runtime\nServer. It is sometimes referred to by its old name \u201cV2 Inference Protocol\u201d.\nFor an inference server to be compliant with this protocol the server\nmust implement all APIs described below, except where an optional\nfeature is explicitly noted. A compliant inference server may choose\nto implement either or both of the HTTP/REST API and the GRPC API.\nThe protocol supports an extension mechanism as a required part of the\nAPI, but this document does not propose any specific extensions. Any\nspecific extensions will be proposed separately.\n\nHTTP/REST\u00b6\nA compliant server must implement the health, metadata, and inference\nAPIs described in this section.\nThe HTTP/REST API uses JSON because it is widely supported and\nlanguage independent. In all JSON schemas shown in this document\n\\(number, \\)string, \\(boolean, \\)object and $array refer to the\nfundamental JSON types. #optional indicates an optional JSON field.\nAll strings in all contexts are case-sensitive.\nFor Seldon a server must recognize the following URLs. The\nversions portion of the URL is shown as optional to allow\nimplementations that don\u2019t support versioning or for cases when the\nuser does not want to specify a specific model version (in which case\nthe server will choose a version based on its own policies).\nHealth:\n GET v2/health/live\n GET v2/health/ready\n GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready\n\n\nServer Metadata:\n GET v2\n\n\nModel Metadata:\n GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]\n\n\nInference:\n POST v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer\n\n\n\nHealth\u00b6\nA health request is made with an HTTP GET to a health endpoint. The\nHTTP response status code indicates a boolean result for the health\nrequest. A 200 status code indicates true and a 4xx status code\nindicates false. The HTTP response body should be empty. There are\nthree health APIs.\n\nServer Live\u00b6\nThe \u201cserver live\u201d API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The \u201cserver\nlive\u201d API can be used directly to implement the Kubernetes\nlivenessProbe.\n\n\nServer Ready\u00b6\nThe \u201cserver ready\u201d health API indicates if all the models are ready\nfor inferencing. The \u201cserver ready\u201d health API can be used directly to\nimplement the Kubernetes readinessProbe.\n\n\nModel Ready\u00b6\nThe \u201cmodel ready\u201d health API indicates if a specific model is ready\nfor inferencing. The model name and (optionally) version must be\navailable in the URL. If a version is not provided the server may\nchoose a version based on its own policies.\n\n\n\nServer Metadata\u00b6\nThe server metadata endpoint provides information about the server. A\nserver metadata request is made with an HTTP GET to a server metadata\nendpoint. In the corresponding response the HTTP body contains the\nServer Metadata Response JSON Object\nor the\nServer Metadata Response JSON Error Object.\n\nServer Metadata Response JSON Object\u00b6\nA successful server metadata request is indicated by a 200 HTTP status\ncode. The server metadata response object, identified as\n$metadata_server_response, is returned in the HTTP body.\n    $metadata_server_response =\n    {\n      \"name\" : $string,\n      \"version\" : $string,\n      \"extensions\" : [ $string, ... ]\n    }\n\n\n\n\u201cname\u201d : A descriptive name for the server.\n\u201cversion\u201d : The server version.\n\u201cextensions\u201d : The extensions supported by the server. Currently no\nstandard extensions are defined. Individual inference servers may\ndefine and document their own extensions.\n\n\n\nServer Metadata Response JSON Error Object\u00b6\nA failed server metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_server_error_response object.\n    $metadata_server_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\n\nModel Metadata\u00b6\nThe per-model metadata endpoint provides information about a model. A\nmodel metadata request is made with an HTTP GET to a model metadata\nendpoint. In the corresponding response the HTTP body contains the\nModel Metadata Response JSON Object\nor the\nModel Metadata Response JSON Error Object.\nThe model name and (optionally) version must be available in the\nURL. If a version is not provided the server may choose a version\nbased on its own policies or return an error.\n\nModel Metadata Response JSON Object\u00b6\nA successful model metadata request is indicated by a 200 HTTP status\ncode. The metadata response object, identified as\n$metadata_model_response, is returned in the HTTP body for every\nsuccessful model metadata request.\n    $metadata_model_response =\n    {\n      \"name\" : $string,\n      \"versions\" : [ $string, ... ] #optional,\n      \"platform\" : $string,\n      \"inputs\" : [ $metadata_tensor, ... ],\n      \"outputs\" : [ $metadata_tensor, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the model.\n\u201cversions\u201d : The model versions that may be explicitly requested via\nthe appropriate endpoint. Optional for servers that don\u2019t support\nversions. Optional for models that don\u2019t allow a version to be\nexplicitly requested.\n\u201cplatform\u201d : The framework/backend for the model. See\nPlatforms.\n\u201cinputs\u201d : The inputs required by the model.\n\u201coutputs\u201d : The outputs produced by the model.\n\nEach model input and output tensors\u2019 metadata is described with a\n$metadata_tensor object.\n    $metadata_tensor =\n    {\n      \"name\" : $string,\n      \"datatype\" : $string,\n      \"shape\" : [ $number, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the tensor.\n\u201cdatatype\u201d : The data-type of the tensor elements as defined in\nTensor Data Types.\n\u201cshape\u201d : The shape of the tensor. Variable-size dimensions are\nspecified as -1.\n\n\n\nModel Metadata Response JSON Error Object\u00b6\nA failed model metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_model_error_response object.\n    $metadata_model_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\n\nInference\u00b6\nAn inference request is made with an HTTP POST to an inference\nendpoint. In the request the HTTP body contains the\nInference Request JSON Object. In\nthe corresponding response the HTTP body contains the\nInference Response JSON Object or\nInference Response JSON Error Object. See\nInference Request Examples for some\nexample HTTP/REST requests and responses.\n\nInference Request JSON Object\u00b6\nThe inference request object, identified as $inference_request, is\nrequired in the HTTP body of the POST request. The model name and\n(optionally) version must be available in the URL. If a version is not\nprovided the server may choose a version based on its own policies or\nreturn an error.\n    $inference_request =\n    {\n      \"id\" : $string #optional,\n      \"parameters\" : $parameters #optional,\n      \"inputs\" : [ $request_input, ... ],\n      \"outputs\" : [ $request_output, ... ] #optional\n    }\n\n\n\n\u201cid\u201d : An identifier for this request. Optional, but if specified\nthis identifier must be returned in the response.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninference request expressed as key/value pairs. See\nParameters for more information.\n\u201cinputs\u201d : The input tensors. Each input is described using the\n$request_input schema defined in Request Input.\n\u201coutputs\u201d : The output tensors requested for this inference. Each\nrequested output is described using the $request_output schema\ndefined in Request Output. Optional, if not\nspecified all outputs produced by the model will be returned using\ndefault $request_output settings.\n\n\nRequest Input\u00b6\nThe $request_input JSON describes an input to the model. If the\ninput is batched, the shape and data must represent the full shape and\ncontents of the entire batch.\n    $request_input =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the input tensor.\n\u201cshape\u201d : The shape of the input tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the input tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\nRequest Output\u00b6\nThe $request_output JSON is used to request which output tensors\nshould be returned from the model.\n    $request_output =\n    {\n      \"name\" : $string,\n      \"parameters\" : $parameters #optional,\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cparameters\u201d : An object containing zero or more parameters for this\noutput expressed as key/value pairs. See Parameters\nfor more information.\n\n\n\n\nInference Response JSON Object\u00b6\nA successful inference request is indicated by a 200 HTTP status\ncode. The inference response object, identified as\n$inference_response, is returned in the HTTP body.\n    $inference_response =\n    {\n      \"model_name\" : $string,\n      \"model_version\" : $string #optional,\n      \"id\" : $string,\n      \"parameters\" : $parameters #optional,\n      \"outputs\" : [ $response_output, ... ]\n    }\n\n\n\n\u201cmodel_name\u201d : The name of the model used for inference.\n\u201cmodel_version\u201d : The specific model version used for\ninference. Inference servers that do not implement versioning should\nnot provide this field in the response.\n\u201cid\u201d : The \u201cid\u201d identifier given in the request, if any.\n\u201cparameters\u201d : An object containing zero or more parameters for this\nresponse expressed as key/value pairs. See Parameters\nfor more information.\n\u201coutputs\u201d : The output tensors. Each output is described using the\n$response_output schema defined in\nResponse Output.\n\n\nResponse Output\u00b6\nThe $response_output JSON describes an output from the model. If the\noutput is batched, the shape and data represents the full shape of the\nentire batch.\n    $response_output =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cshape\u201d : The shape of the output tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the output tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\n\nInference Response JSON Error Object\u00b6\nA failed inference request must be indicated by an HTTP error status\n(typically 400). The HTTP body must contain the\n$inference_error_response object.\n    $inference_error_response =\n    {\n      \"error\": <error message string>\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\nInference Request Examples\u00b6\nThe following example shows an inference request to a model with two\ninputs and one output. The HTTP Content-Length header gives the size\nof the JSON object.\n    POST /v2/models/mymodel/infer HTTP/1.1\n    Host: localhost:8000\n    Content-Type: application/json\n    Content-Length: <xx>\n    {\n      \"id\" : \"42\",\n      \"inputs\" : [\n        {\n          \"name\" : \"input0\",\n          \"shape\" : [ 2, 2 ],\n          \"datatype\" : \"UINT32\",\n          \"data\" : [ 1, 2, 3, 4 ]\n        },\n        {\n          \"name\" : \"input1\",\n          \"shape\" : [ 3 ],\n          \"datatype\" : \"BOOL\",\n          \"data\" : [ true ]\n        }\n      ],\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\"\n        }\n      ]\n    }\n\n\nFor the above request the inference server must return the \u201coutput0\u201d\noutput tensor. Assuming the model returns a [ 3, 2 ] tensor of data\ntype FP32 the following response would be returned.\n    HTTP/1.1 200 OK\n    Content-Type: application/json\n    Content-Length: <yy>\n    {\n      \"id\" : \"42\"\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\",\n          \"shape\" : [ 3, 2 ],\n          \"datatype\"  : \"FP32\",\n          \"data\" : [ 1.0, 1.1, 2.0, 2.1, 3.0, 3.1 ]\n        }\n      ]\n    }\n\n\n\n\n\nParameters\u00b6\nThe *\\(parameters* JSON describes zero or more \u201cname\u201d/\u201dvalue\u201d pairs,\nwhere the \u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a\n\\)string, \\(number, or \\)boolean.\n    $parameters =\n    {\n      $parameter, ...\n    }\n\n    $parameter = $string : $string | $number | $boolean\n\n\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n\n\nTensor Data\u00b6\nTensor data must be presented in row-major order of the tensor\nelements. Element values must be given in \u201clinear\u201d order without any\nstride or padding between elements. Tensor elements may be presented\nin their nature multi-dimensional representation, or as a flattened\none-dimensional representation.\nTensor data given explicitly is provided in a JSON array. Each element\nof the array may be an integer, floating-point number, string or\nboolean value. The server can decide to coerce each element to the\nrequired type or return an error if an unexpected value is\nreceived. Note that fp16 is problematic to communicate explicitly\nsince there is not a standard fp16 representation across backends nor\ntypically the programmatic support to create the fp16 representation\nfor a JSON number.\nFor example, the 2-dimensional matrix:\n[ 1 2\n  4 5 ]\n\n\nCan be represented in its natural format as:\n\"data\" : [ [ 1, 2 ], [ 4, 5 ] ]\n\n\nOr in a flattened one-dimensional representation:\n\"data\" : [ 1, 2, 4, 5 ]\n\n\n\n\n\nGRPC\u00b6\nThe GRPC API closely follows the concepts defined in the\nHTTP/REST API. A compliant server must implement the\nhealth, metadata, and inference APIs described in this section.\nAll strings in all contexts are case-sensitive.\nThe GRPC definition of the service is:\n//\n// Inference Server GRPC endpoints.\n//\nservice GRPCInferenceService\n{\n  // Check liveness of the inference server.\n  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}\n\n  // Check readiness of the inference server.\n  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}\n\n  // Check readiness of a model in the inference server.\n  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}\n\n  // Get server metadata.\n  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}\n\n  // Get model metadata.\n  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}\n\n  // Perform inference using a specific model.\n  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}\n}\n\n\n\nHealth\u00b6\nA health request is made using the ServerLive, ServerReady, or\nModelReady endpoint. For each of these endpoints errors are indicated\nby the google.rpc.Status returned for the request. The OK code\nindicates success and other codes indicate failure.\n\nServer Live\u00b6\nThe ServerLive API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The request\nand response messages for ServerLive are:\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\n\n\n\nServer Ready\u00b6\nThe ServerReady API indicates if the server is ready for\ninferencing. The request and response messages for ServerReady are:\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n\nModel Ready\u00b6\nThe ModelReady API indicates if a specific model is ready for\ninferencing. The request and response messages for ModelReady are:\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n\n\nServer Metadata\u00b6\nThe ServerMetadata API provides information about the server. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ServerMetadata are:\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  // The server name.\n  string name = 1;\n\n  // The server version.\n  string version = 2;\n\n  // The extensions supported by the server.\n  repeated string extensions = 3;\n}\n\n\n\n\nModel Metadata\u00b6\nThe per-model metadata API provides information about a model. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ModelMetadata are:\nmessage ModelMetadataRequest\n{\n  // The name of the model.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  // Metadata for a tensor.\n  message TensorMetadata\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape. A variable-size dimension is represented\n    // by a -1 value.\n    repeated int64 shape = 3;\n  }\n\n  // The model name.\n  string name = 1;\n\n  // The versions of the model available on the server.\n  repeated string versions = 2;\n\n  // The model's platform. See Platforms.\n  string platform = 3;\n\n  // The model's inputs.\n  repeated TensorMetadata inputs = 4;\n\n  // The model's outputs.\n  repeated TensorMetadata outputs = 5;\n}\n\n\n\n\nInference\u00b6\nThe ModelInfer API performs inference using the specified\nmodel. Errors are indicated by the google.rpc.Status returned for the\nrequest. The OK code indicates success and other codes indicate\nfailure. The request and response messages for ModelInfer are:\nmessage ModelInferRequest\n{\n  // An input tensor for an inference request.\n  message InferInputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional inference input tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference request.\n    InferTensorContents contents = 5;\n  }\n\n  // An output tensor requested for an inference request.\n  message InferRequestedOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // Optional requested output tensor parameters.\n    map<string, InferParameter> parameters = 2;\n  }\n\n  // The name of the model to use for inferencing.\n  string model_name = 1;\n\n  // The version of the model to use for inference. If not given the\n  // server will choose a version based on the model and internal policy.\n  string model_version = 2;\n\n  // Optional identifier for the request. If specified will be\n  // returned in the response.\n  string id = 3;\n\n  // Optional inference parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The input tensors for the inference.\n  repeated InferInputTensor inputs = 5;\n\n  // The requested output tensors for the inference. Optional, if not\n  // specified all outputs produced by the model will be returned.\n  repeated InferRequestedOutputTensor outputs = 6;\n\n  // The data contained in an input tensor can be represented in \"raw\"\n  // bytes form or in the repeated type that matches the tensor's data\n  // type. To use the raw representation 'raw_input_contents' must be\n  // initialized with data for each tensor in the same order as\n  // 'inputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferInputTensor::contents must\n  // not be specified for any input tensor.\n  repeated bytes raw_input_contents = 7;\n}\n\nmessage ModelInferResponse\n{\n  // An output tensor returned for an inference request.\n  message InferOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional output tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference response.\n    InferTensorContents contents = 5;\n  }\n\n  // The name of the model used for inference.\n  string model_name = 1;\n\n  // The version of the model used for inference.\n  string model_version = 2;\n\n  // The id of the inference request if one was specified.\n  string id = 3;\n\n  // Optional inference response parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The output tensors holding inference results.\n  repeated InferOutputTensor outputs = 5;\n\n  // The data contained in an output tensor can be represented in\n  // \"raw\" bytes form or in the repeated type that matches the\n  // tensor's data type. To use the raw representation 'raw_output_contents'\n  // must be initialized with data for each tensor in the same order as\n  // 'outputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferOutputTensor::contents must\n  // not be specified for any output tensor.\n  repeated bytes raw_output_contents = 6;\n}\n\n\n\n\nParameters\u00b6\nThe Parameters message describes a \u201cname\u201d/\u201dvalue\u201d pair, where the\n\u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a boolean,\ninteger, or string corresponding to the parameter.\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n//\n// An inference parameter value.\n//\nmessage InferParameter\n{\n  // The parameter value can be a string, an int64, a boolean\n  // or a message specific to a predefined parameter.\n  oneof parameter_choice\n  {\n    // A boolean parameter value.\n    bool bool_param = 1;\n\n    // An int64 parameter value.\n    int64 int64_param = 2;\n\n    // A string parameter value.\n    string string_param = 3;\n  }\n}\n\n\n\n\nTensor Data\u00b6\nIn all representations tensor data must be flattened to a\none-dimensional, row-major order of the tensor elements. Element\nvalues must be given in \u201clinear\u201d order without any stride or padding\nbetween elements.\nUsing a \u201craw\u201d representation of tensors with\nModelInferRequest::raw_input_contents and\nModelInferResponse::raw_output_contents will typically allow higher\nperformance due to the way protobuf allocation and reuse interacts\nwith GRPC. For example, see https://github.com/grpc/grpc/issues/23231.\nAn alternative to the \u201craw\u201d representation is to use\nInferTensorContents to represent the tensor data in a format that\nmatches the tensor\u2019s data type.\n//\n// The data contained in a tensor represented by the repeated type\n// that matches the tensor's data type. Protobuf oneof is not used\n// because oneofs cannot contain repeated fields.\n//\nmessage InferTensorContents\n{\n  // Representation for BOOL data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bool bool_contents = 1;\n\n  // Representation for INT8, INT16, and INT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated int32 int_contents = 2;\n\n  // Representation for INT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated int64 int64_contents = 3;\n\n  // Representation for UINT8, UINT16, and UINT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated uint32 uint_contents = 4;\n\n  // Representation for UINT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated uint64 uint64_contents = 5;\n\n  // Representation for FP32 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated float fp32_contents = 6;\n\n  // Representation for FP64 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated double fp64_contents = 7;\n\n  // Representation for BYTES data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bytes bytes_contents = 8;\n}\n\n\n\n\n\nPlatforms\u00b6\nA platform is a string indicating a DL/ML framework or\nbackend. Platform is returned as part of the response to a\nModel Metadata request but is information only. The\nproposed inference APIs are generic relative to the DL/ML framework\nused by a model and so a client does not need to know the platform of\na given model to use the API. Platform names use the format\n\u201c_\u201d. The following platform names are allowed:\n\ntensorrt_plan : A TensorRT model encoded as a serialized engine or \u201cplan\u201d.\ntensorflow_graphdef : A TensorFlow model encoded as a GraphDef.\ntensorflow_savedmodel : A TensorFlow model encoded as a SavedModel.\nonnx_onnxv1 : A ONNX model encoded for ONNX Runtime.\npytorch_torchscript : A PyTorch model encoded as TorchScript.\nmxnet_mxnet: An MXNet model\ncaffe2_netdef : A Caffe2 model encoded as a NetDef.\n\n\n\nTensor Data Types\u00b6\nTensor data types are shown in the following table along with the size\nof each type, in bytes.\n\n\nData Type\nSize (bytes)\n\n\n\nBOOL\n1\n\nUINT8\n1\n\nUINT16\n2\n\nUINT32\n4\n\nUINT64\n8\n\nINT8\n1\n\nINT16\n2\n\nINT32\n4\n\nINT64\n8\n\nFP16\n2\n\nFP32\n4\n\nFP64\n8\n\nBYTES\nVariable (max 232)\n\n\n\n\n\nReferences\u00b6\nThis document is based on the KServe original created during the lifetime of the KFServing project in Kubeflow by its various contributors including Seldon, NVIDIA, IBM, Bloomberg and others.\n\n", "http-rest": "\nHTTP/REST\u00b6\nA compliant server must implement the health, metadata, and inference\nAPIs described in this section.\nThe HTTP/REST API uses JSON because it is widely supported and\nlanguage independent. In all JSON schemas shown in this document\n\\(number, \\)string, \\(boolean, \\)object and $array refer to the\nfundamental JSON types. #optional indicates an optional JSON field.\nAll strings in all contexts are case-sensitive.\nFor Seldon a server must recognize the following URLs. The\nversions portion of the URL is shown as optional to allow\nimplementations that don\u2019t support versioning or for cases when the\nuser does not want to specify a specific model version (in which case\nthe server will choose a version based on its own policies).\nHealth:\n GET v2/health/live\n GET v2/health/ready\n GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready\n\n\nServer Metadata:\n GET v2\n\n\nModel Metadata:\n GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]\n\n\nInference:\n POST v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer\n\n\n\nHealth\u00b6\nA health request is made with an HTTP GET to a health endpoint. The\nHTTP response status code indicates a boolean result for the health\nrequest. A 200 status code indicates true and a 4xx status code\nindicates false. The HTTP response body should be empty. There are\nthree health APIs.\n\nServer Live\u00b6\nThe \u201cserver live\u201d API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The \u201cserver\nlive\u201d API can be used directly to implement the Kubernetes\nlivenessProbe.\n\n\nServer Ready\u00b6\nThe \u201cserver ready\u201d health API indicates if all the models are ready\nfor inferencing. The \u201cserver ready\u201d health API can be used directly to\nimplement the Kubernetes readinessProbe.\n\n\nModel Ready\u00b6\nThe \u201cmodel ready\u201d health API indicates if a specific model is ready\nfor inferencing. The model name and (optionally) version must be\navailable in the URL. If a version is not provided the server may\nchoose a version based on its own policies.\n\n\n\nServer Metadata\u00b6\nThe server metadata endpoint provides information about the server. A\nserver metadata request is made with an HTTP GET to a server metadata\nendpoint. In the corresponding response the HTTP body contains the\nServer Metadata Response JSON Object\nor the\nServer Metadata Response JSON Error Object.\n\nServer Metadata Response JSON Object\u00b6\nA successful server metadata request is indicated by a 200 HTTP status\ncode. The server metadata response object, identified as\n$metadata_server_response, is returned in the HTTP body.\n    $metadata_server_response =\n    {\n      \"name\" : $string,\n      \"version\" : $string,\n      \"extensions\" : [ $string, ... ]\n    }\n\n\n\n\u201cname\u201d : A descriptive name for the server.\n\u201cversion\u201d : The server version.\n\u201cextensions\u201d : The extensions supported by the server. Currently no\nstandard extensions are defined. Individual inference servers may\ndefine and document their own extensions.\n\n\n\nServer Metadata Response JSON Error Object\u00b6\nA failed server metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_server_error_response object.\n    $metadata_server_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\n\nModel Metadata\u00b6\nThe per-model metadata endpoint provides information about a model. A\nmodel metadata request is made with an HTTP GET to a model metadata\nendpoint. In the corresponding response the HTTP body contains the\nModel Metadata Response JSON Object\nor the\nModel Metadata Response JSON Error Object.\nThe model name and (optionally) version must be available in the\nURL. If a version is not provided the server may choose a version\nbased on its own policies or return an error.\n\nModel Metadata Response JSON Object\u00b6\nA successful model metadata request is indicated by a 200 HTTP status\ncode. The metadata response object, identified as\n$metadata_model_response, is returned in the HTTP body for every\nsuccessful model metadata request.\n    $metadata_model_response =\n    {\n      \"name\" : $string,\n      \"versions\" : [ $string, ... ] #optional,\n      \"platform\" : $string,\n      \"inputs\" : [ $metadata_tensor, ... ],\n      \"outputs\" : [ $metadata_tensor, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the model.\n\u201cversions\u201d : The model versions that may be explicitly requested via\nthe appropriate endpoint. Optional for servers that don\u2019t support\nversions. Optional for models that don\u2019t allow a version to be\nexplicitly requested.\n\u201cplatform\u201d : The framework/backend for the model. See\nPlatforms.\n\u201cinputs\u201d : The inputs required by the model.\n\u201coutputs\u201d : The outputs produced by the model.\n\nEach model input and output tensors\u2019 metadata is described with a\n$metadata_tensor object.\n    $metadata_tensor =\n    {\n      \"name\" : $string,\n      \"datatype\" : $string,\n      \"shape\" : [ $number, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the tensor.\n\u201cdatatype\u201d : The data-type of the tensor elements as defined in\nTensor Data Types.\n\u201cshape\u201d : The shape of the tensor. Variable-size dimensions are\nspecified as -1.\n\n\n\nModel Metadata Response JSON Error Object\u00b6\nA failed model metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_model_error_response object.\n    $metadata_model_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\n\nInference\u00b6\nAn inference request is made with an HTTP POST to an inference\nendpoint. In the request the HTTP body contains the\nInference Request JSON Object. In\nthe corresponding response the HTTP body contains the\nInference Response JSON Object or\nInference Response JSON Error Object. See\nInference Request Examples for some\nexample HTTP/REST requests and responses.\n\nInference Request JSON Object\u00b6\nThe inference request object, identified as $inference_request, is\nrequired in the HTTP body of the POST request. The model name and\n(optionally) version must be available in the URL. If a version is not\nprovided the server may choose a version based on its own policies or\nreturn an error.\n    $inference_request =\n    {\n      \"id\" : $string #optional,\n      \"parameters\" : $parameters #optional,\n      \"inputs\" : [ $request_input, ... ],\n      \"outputs\" : [ $request_output, ... ] #optional\n    }\n\n\n\n\u201cid\u201d : An identifier for this request. Optional, but if specified\nthis identifier must be returned in the response.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninference request expressed as key/value pairs. See\nParameters for more information.\n\u201cinputs\u201d : The input tensors. Each input is described using the\n$request_input schema defined in Request Input.\n\u201coutputs\u201d : The output tensors requested for this inference. Each\nrequested output is described using the $request_output schema\ndefined in Request Output. Optional, if not\nspecified all outputs produced by the model will be returned using\ndefault $request_output settings.\n\n\nRequest Input\u00b6\nThe $request_input JSON describes an input to the model. If the\ninput is batched, the shape and data must represent the full shape and\ncontents of the entire batch.\n    $request_input =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the input tensor.\n\u201cshape\u201d : The shape of the input tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the input tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\nRequest Output\u00b6\nThe $request_output JSON is used to request which output tensors\nshould be returned from the model.\n    $request_output =\n    {\n      \"name\" : $string,\n      \"parameters\" : $parameters #optional,\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cparameters\u201d : An object containing zero or more parameters for this\noutput expressed as key/value pairs. See Parameters\nfor more information.\n\n\n\n\nInference Response JSON Object\u00b6\nA successful inference request is indicated by a 200 HTTP status\ncode. The inference response object, identified as\n$inference_response, is returned in the HTTP body.\n    $inference_response =\n    {\n      \"model_name\" : $string,\n      \"model_version\" : $string #optional,\n      \"id\" : $string,\n      \"parameters\" : $parameters #optional,\n      \"outputs\" : [ $response_output, ... ]\n    }\n\n\n\n\u201cmodel_name\u201d : The name of the model used for inference.\n\u201cmodel_version\u201d : The specific model version used for\ninference. Inference servers that do not implement versioning should\nnot provide this field in the response.\n\u201cid\u201d : The \u201cid\u201d identifier given in the request, if any.\n\u201cparameters\u201d : An object containing zero or more parameters for this\nresponse expressed as key/value pairs. See Parameters\nfor more information.\n\u201coutputs\u201d : The output tensors. Each output is described using the\n$response_output schema defined in\nResponse Output.\n\n\nResponse Output\u00b6\nThe $response_output JSON describes an output from the model. If the\noutput is batched, the shape and data represents the full shape of the\nentire batch.\n    $response_output =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cshape\u201d : The shape of the output tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the output tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\n\nInference Response JSON Error Object\u00b6\nA failed inference request must be indicated by an HTTP error status\n(typically 400). The HTTP body must contain the\n$inference_error_response object.\n    $inference_error_response =\n    {\n      \"error\": <error message string>\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\nInference Request Examples\u00b6\nThe following example shows an inference request to a model with two\ninputs and one output. The HTTP Content-Length header gives the size\nof the JSON object.\n    POST /v2/models/mymodel/infer HTTP/1.1\n    Host: localhost:8000\n    Content-Type: application/json\n    Content-Length: <xx>\n    {\n      \"id\" : \"42\",\n      \"inputs\" : [\n        {\n          \"name\" : \"input0\",\n          \"shape\" : [ 2, 2 ],\n          \"datatype\" : \"UINT32\",\n          \"data\" : [ 1, 2, 3, 4 ]\n        },\n        {\n          \"name\" : \"input1\",\n          \"shape\" : [ 3 ],\n          \"datatype\" : \"BOOL\",\n          \"data\" : [ true ]\n        }\n      ],\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\"\n        }\n      ]\n    }\n\n\nFor the above request the inference server must return the \u201coutput0\u201d\noutput tensor. Assuming the model returns a [ 3, 2 ] tensor of data\ntype FP32 the following response would be returned.\n    HTTP/1.1 200 OK\n    Content-Type: application/json\n    Content-Length: <yy>\n    {\n      \"id\" : \"42\"\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\",\n          \"shape\" : [ 3, 2 ],\n          \"datatype\"  : \"FP32\",\n          \"data\" : [ 1.0, 1.1, 2.0, 2.1, 3.0, 3.1 ]\n        }\n      ]\n    }\n\n\n\n\n\nParameters\u00b6\nThe *\\(parameters* JSON describes zero or more \u201cname\u201d/\u201dvalue\u201d pairs,\nwhere the \u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a\n\\)string, \\(number, or \\)boolean.\n    $parameters =\n    {\n      $parameter, ...\n    }\n\n    $parameter = $string : $string | $number | $boolean\n\n\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n\n\nTensor Data\u00b6\nTensor data must be presented in row-major order of the tensor\nelements. Element values must be given in \u201clinear\u201d order without any\nstride or padding between elements. Tensor elements may be presented\nin their nature multi-dimensional representation, or as a flattened\none-dimensional representation.\nTensor data given explicitly is provided in a JSON array. Each element\nof the array may be an integer, floating-point number, string or\nboolean value. The server can decide to coerce each element to the\nrequired type or return an error if an unexpected value is\nreceived. Note that fp16 is problematic to communicate explicitly\nsince there is not a standard fp16 representation across backends nor\ntypically the programmatic support to create the fp16 representation\nfor a JSON number.\nFor example, the 2-dimensional matrix:\n[ 1 2\n  4 5 ]\n\n\nCan be represented in its natural format as:\n\"data\" : [ [ 1, 2 ], [ 4, 5 ] ]\n\n\nOr in a flattened one-dimensional representation:\n\"data\" : [ 1, 2, 4, 5 ]\n\n\n\n", "health": "\nHealth\u00b6\nA health request is made with an HTTP GET to a health endpoint. The\nHTTP response status code indicates a boolean result for the health\nrequest. A 200 status code indicates true and a 4xx status code\nindicates false. The HTTP response body should be empty. There are\nthree health APIs.\n\nServer Live\u00b6\nThe \u201cserver live\u201d API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The \u201cserver\nlive\u201d API can be used directly to implement the Kubernetes\nlivenessProbe.\n\n\nServer Ready\u00b6\nThe \u201cserver ready\u201d health API indicates if all the models are ready\nfor inferencing. The \u201cserver ready\u201d health API can be used directly to\nimplement the Kubernetes readinessProbe.\n\n\nModel Ready\u00b6\nThe \u201cmodel ready\u201d health API indicates if a specific model is ready\nfor inferencing. The model name and (optionally) version must be\navailable in the URL. If a version is not provided the server may\nchoose a version based on its own policies.\n\n", "server-live": "\nServer Live\u00b6\nThe \u201cserver live\u201d API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The \u201cserver\nlive\u201d API can be used directly to implement the Kubernetes\nlivenessProbe.\n", "server-ready": "\nServer Ready\u00b6\nThe \u201cserver ready\u201d health API indicates if all the models are ready\nfor inferencing. The \u201cserver ready\u201d health API can be used directly to\nimplement the Kubernetes readinessProbe.\n", "model-ready": "\nModel Ready\u00b6\nThe \u201cmodel ready\u201d health API indicates if a specific model is ready\nfor inferencing. The model name and (optionally) version must be\navailable in the URL. If a version is not provided the server may\nchoose a version based on its own policies.\n", "server-metadata": "\nServer Metadata\u00b6\nThe server metadata endpoint provides information about the server. A\nserver metadata request is made with an HTTP GET to a server metadata\nendpoint. In the corresponding response the HTTP body contains the\nServer Metadata Response JSON Object\nor the\nServer Metadata Response JSON Error Object.\n\nServer Metadata Response JSON Object\u00b6\nA successful server metadata request is indicated by a 200 HTTP status\ncode. The server metadata response object, identified as\n$metadata_server_response, is returned in the HTTP body.\n    $metadata_server_response =\n    {\n      \"name\" : $string,\n      \"version\" : $string,\n      \"extensions\" : [ $string, ... ]\n    }\n\n\n\n\u201cname\u201d : A descriptive name for the server.\n\u201cversion\u201d : The server version.\n\u201cextensions\u201d : The extensions supported by the server. Currently no\nstandard extensions are defined. Individual inference servers may\ndefine and document their own extensions.\n\n\n\nServer Metadata Response JSON Error Object\u00b6\nA failed server metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_server_error_response object.\n    $metadata_server_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n", "server-metadata-response-json-object": "\nServer Metadata Response JSON Object\u00b6\nA successful server metadata request is indicated by a 200 HTTP status\ncode. The server metadata response object, identified as\n$metadata_server_response, is returned in the HTTP body.\n    $metadata_server_response =\n    {\n      \"name\" : $string,\n      \"version\" : $string,\n      \"extensions\" : [ $string, ... ]\n    }\n\n\n\n\u201cname\u201d : A descriptive name for the server.\n\u201cversion\u201d : The server version.\n\u201cextensions\u201d : The extensions supported by the server. Currently no\nstandard extensions are defined. Individual inference servers may\ndefine and document their own extensions.\n\n", "server-metadata-response-json-error-object": "\nServer Metadata Response JSON Error Object\u00b6\nA failed server metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_server_error_response object.\n    $metadata_server_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n", "model-metadata": "\nModel Metadata\u00b6\nThe per-model metadata endpoint provides information about a model. A\nmodel metadata request is made with an HTTP GET to a model metadata\nendpoint. In the corresponding response the HTTP body contains the\nModel Metadata Response JSON Object\nor the\nModel Metadata Response JSON Error Object.\nThe model name and (optionally) version must be available in the\nURL. If a version is not provided the server may choose a version\nbased on its own policies or return an error.\n\nModel Metadata Response JSON Object\u00b6\nA successful model metadata request is indicated by a 200 HTTP status\ncode. The metadata response object, identified as\n$metadata_model_response, is returned in the HTTP body for every\nsuccessful model metadata request.\n    $metadata_model_response =\n    {\n      \"name\" : $string,\n      \"versions\" : [ $string, ... ] #optional,\n      \"platform\" : $string,\n      \"inputs\" : [ $metadata_tensor, ... ],\n      \"outputs\" : [ $metadata_tensor, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the model.\n\u201cversions\u201d : The model versions that may be explicitly requested via\nthe appropriate endpoint. Optional for servers that don\u2019t support\nversions. Optional for models that don\u2019t allow a version to be\nexplicitly requested.\n\u201cplatform\u201d : The framework/backend for the model. See\nPlatforms.\n\u201cinputs\u201d : The inputs required by the model.\n\u201coutputs\u201d : The outputs produced by the model.\n\nEach model input and output tensors\u2019 metadata is described with a\n$metadata_tensor object.\n    $metadata_tensor =\n    {\n      \"name\" : $string,\n      \"datatype\" : $string,\n      \"shape\" : [ $number, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the tensor.\n\u201cdatatype\u201d : The data-type of the tensor elements as defined in\nTensor Data Types.\n\u201cshape\u201d : The shape of the tensor. Variable-size dimensions are\nspecified as -1.\n\n\n\nModel Metadata Response JSON Error Object\u00b6\nA failed model metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_model_error_response object.\n    $metadata_model_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n", "model-metadata-response-json-object": "\nModel Metadata Response JSON Object\u00b6\nA successful model metadata request is indicated by a 200 HTTP status\ncode. The metadata response object, identified as\n$metadata_model_response, is returned in the HTTP body for every\nsuccessful model metadata request.\n    $metadata_model_response =\n    {\n      \"name\" : $string,\n      \"versions\" : [ $string, ... ] #optional,\n      \"platform\" : $string,\n      \"inputs\" : [ $metadata_tensor, ... ],\n      \"outputs\" : [ $metadata_tensor, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the model.\n\u201cversions\u201d : The model versions that may be explicitly requested via\nthe appropriate endpoint. Optional for servers that don\u2019t support\nversions. Optional for models that don\u2019t allow a version to be\nexplicitly requested.\n\u201cplatform\u201d : The framework/backend for the model. See\nPlatforms.\n\u201cinputs\u201d : The inputs required by the model.\n\u201coutputs\u201d : The outputs produced by the model.\n\nEach model input and output tensors\u2019 metadata is described with a\n$metadata_tensor object.\n    $metadata_tensor =\n    {\n      \"name\" : $string,\n      \"datatype\" : $string,\n      \"shape\" : [ $number, ... ]\n    }\n\n\n\n\u201cname\u201d : The name of the tensor.\n\u201cdatatype\u201d : The data-type of the tensor elements as defined in\nTensor Data Types.\n\u201cshape\u201d : The shape of the tensor. Variable-size dimensions are\nspecified as -1.\n\n", "model-metadata-response-json-error-object": "\nModel Metadata Response JSON Error Object\u00b6\nA failed model metadata request must be indicated by an HTTP error\nstatus (typically 400). The HTTP body must contain the\n$metadata_model_error_response object.\n    $metadata_model_error_response =\n    {\n      \"error\": $string\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n", "inference": "\nInference\u00b6\nAn inference request is made with an HTTP POST to an inference\nendpoint. In the request the HTTP body contains the\nInference Request JSON Object. In\nthe corresponding response the HTTP body contains the\nInference Response JSON Object or\nInference Response JSON Error Object. See\nInference Request Examples for some\nexample HTTP/REST requests and responses.\n\nInference Request JSON Object\u00b6\nThe inference request object, identified as $inference_request, is\nrequired in the HTTP body of the POST request. The model name and\n(optionally) version must be available in the URL. If a version is not\nprovided the server may choose a version based on its own policies or\nreturn an error.\n    $inference_request =\n    {\n      \"id\" : $string #optional,\n      \"parameters\" : $parameters #optional,\n      \"inputs\" : [ $request_input, ... ],\n      \"outputs\" : [ $request_output, ... ] #optional\n    }\n\n\n\n\u201cid\u201d : An identifier for this request. Optional, but if specified\nthis identifier must be returned in the response.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninference request expressed as key/value pairs. See\nParameters for more information.\n\u201cinputs\u201d : The input tensors. Each input is described using the\n$request_input schema defined in Request Input.\n\u201coutputs\u201d : The output tensors requested for this inference. Each\nrequested output is described using the $request_output schema\ndefined in Request Output. Optional, if not\nspecified all outputs produced by the model will be returned using\ndefault $request_output settings.\n\n\nRequest Input\u00b6\nThe $request_input JSON describes an input to the model. If the\ninput is batched, the shape and data must represent the full shape and\ncontents of the entire batch.\n    $request_input =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the input tensor.\n\u201cshape\u201d : The shape of the input tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the input tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\nRequest Output\u00b6\nThe $request_output JSON is used to request which output tensors\nshould be returned from the model.\n    $request_output =\n    {\n      \"name\" : $string,\n      \"parameters\" : $parameters #optional,\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cparameters\u201d : An object containing zero or more parameters for this\noutput expressed as key/value pairs. See Parameters\nfor more information.\n\n\n\n\nInference Response JSON Object\u00b6\nA successful inference request is indicated by a 200 HTTP status\ncode. The inference response object, identified as\n$inference_response, is returned in the HTTP body.\n    $inference_response =\n    {\n      \"model_name\" : $string,\n      \"model_version\" : $string #optional,\n      \"id\" : $string,\n      \"parameters\" : $parameters #optional,\n      \"outputs\" : [ $response_output, ... ]\n    }\n\n\n\n\u201cmodel_name\u201d : The name of the model used for inference.\n\u201cmodel_version\u201d : The specific model version used for\ninference. Inference servers that do not implement versioning should\nnot provide this field in the response.\n\u201cid\u201d : The \u201cid\u201d identifier given in the request, if any.\n\u201cparameters\u201d : An object containing zero or more parameters for this\nresponse expressed as key/value pairs. See Parameters\nfor more information.\n\u201coutputs\u201d : The output tensors. Each output is described using the\n$response_output schema defined in\nResponse Output.\n\n\nResponse Output\u00b6\nThe $response_output JSON describes an output from the model. If the\noutput is batched, the shape and data represents the full shape of the\nentire batch.\n    $response_output =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cshape\u201d : The shape of the output tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the output tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\n\nInference Response JSON Error Object\u00b6\nA failed inference request must be indicated by an HTTP error status\n(typically 400). The HTTP body must contain the\n$inference_error_response object.\n    $inference_error_response =\n    {\n      \"error\": <error message string>\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n\n\nInference Request Examples\u00b6\nThe following example shows an inference request to a model with two\ninputs and one output. The HTTP Content-Length header gives the size\nof the JSON object.\n    POST /v2/models/mymodel/infer HTTP/1.1\n    Host: localhost:8000\n    Content-Type: application/json\n    Content-Length: <xx>\n    {\n      \"id\" : \"42\",\n      \"inputs\" : [\n        {\n          \"name\" : \"input0\",\n          \"shape\" : [ 2, 2 ],\n          \"datatype\" : \"UINT32\",\n          \"data\" : [ 1, 2, 3, 4 ]\n        },\n        {\n          \"name\" : \"input1\",\n          \"shape\" : [ 3 ],\n          \"datatype\" : \"BOOL\",\n          \"data\" : [ true ]\n        }\n      ],\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\"\n        }\n      ]\n    }\n\n\nFor the above request the inference server must return the \u201coutput0\u201d\noutput tensor. Assuming the model returns a [ 3, 2 ] tensor of data\ntype FP32 the following response would be returned.\n    HTTP/1.1 200 OK\n    Content-Type: application/json\n    Content-Length: <yy>\n    {\n      \"id\" : \"42\"\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\",\n          \"shape\" : [ 3, 2 ],\n          \"datatype\"  : \"FP32\",\n          \"data\" : [ 1.0, 1.1, 2.0, 2.1, 3.0, 3.1 ]\n        }\n      ]\n    }\n\n\n\n", "inference-request-json-object": "\nInference Request JSON Object\u00b6\nThe inference request object, identified as $inference_request, is\nrequired in the HTTP body of the POST request. The model name and\n(optionally) version must be available in the URL. If a version is not\nprovided the server may choose a version based on its own policies or\nreturn an error.\n    $inference_request =\n    {\n      \"id\" : $string #optional,\n      \"parameters\" : $parameters #optional,\n      \"inputs\" : [ $request_input, ... ],\n      \"outputs\" : [ $request_output, ... ] #optional\n    }\n\n\n\n\u201cid\u201d : An identifier for this request. Optional, but if specified\nthis identifier must be returned in the response.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninference request expressed as key/value pairs. See\nParameters for more information.\n\u201cinputs\u201d : The input tensors. Each input is described using the\n$request_input schema defined in Request Input.\n\u201coutputs\u201d : The output tensors requested for this inference. Each\nrequested output is described using the $request_output schema\ndefined in Request Output. Optional, if not\nspecified all outputs produced by the model will be returned using\ndefault $request_output settings.\n\n\nRequest Input\u00b6\nThe $request_input JSON describes an input to the model. If the\ninput is batched, the shape and data must represent the full shape and\ncontents of the entire batch.\n    $request_input =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the input tensor.\n\u201cshape\u201d : The shape of the input tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the input tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n\nRequest Output\u00b6\nThe $request_output JSON is used to request which output tensors\nshould be returned from the model.\n    $request_output =\n    {\n      \"name\" : $string,\n      \"parameters\" : $parameters #optional,\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cparameters\u201d : An object containing zero or more parameters for this\noutput expressed as key/value pairs. See Parameters\nfor more information.\n\n\n", "request-input": "\nRequest Input\u00b6\nThe $request_input JSON describes an input to the model. If the\ninput is batched, the shape and data must represent the full shape and\ncontents of the entire batch.\n    $request_input =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the input tensor.\n\u201cshape\u201d : The shape of the input tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the input tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n", "request-output": "\nRequest Output\u00b6\nThe $request_output JSON is used to request which output tensors\nshould be returned from the model.\n    $request_output =\n    {\n      \"name\" : $string,\n      \"parameters\" : $parameters #optional,\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cparameters\u201d : An object containing zero or more parameters for this\noutput expressed as key/value pairs. See Parameters\nfor more information.\n\n", "inference-response-json-object": "\nInference Response JSON Object\u00b6\nA successful inference request is indicated by a 200 HTTP status\ncode. The inference response object, identified as\n$inference_response, is returned in the HTTP body.\n    $inference_response =\n    {\n      \"model_name\" : $string,\n      \"model_version\" : $string #optional,\n      \"id\" : $string,\n      \"parameters\" : $parameters #optional,\n      \"outputs\" : [ $response_output, ... ]\n    }\n\n\n\n\u201cmodel_name\u201d : The name of the model used for inference.\n\u201cmodel_version\u201d : The specific model version used for\ninference. Inference servers that do not implement versioning should\nnot provide this field in the response.\n\u201cid\u201d : The \u201cid\u201d identifier given in the request, if any.\n\u201cparameters\u201d : An object containing zero or more parameters for this\nresponse expressed as key/value pairs. See Parameters\nfor more information.\n\u201coutputs\u201d : The output tensors. Each output is described using the\n$response_output schema defined in\nResponse Output.\n\n\nResponse Output\u00b6\nThe $response_output JSON describes an output from the model. If the\noutput is batched, the shape and data represents the full shape of the\nentire batch.\n    $response_output =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cshape\u201d : The shape of the output tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the output tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n\n", "response-output": "\nResponse Output\u00b6\nThe $response_output JSON describes an output from the model. If the\noutput is batched, the shape and data represents the full shape of the\nentire batch.\n    $response_output =\n    {\n      \"name\" : $string,\n      \"shape\" : [ $number, ... ],\n      \"datatype\"  : $string,\n      \"parameters\" : $parameters #optional,\n      \"data\" : $tensor_data\n    }\n\n\n\n\u201cname\u201d : The name of the output tensor.\n\u201cshape\u201d : The shape of the output tensor. Each dimension must be an\ninteger representable as an unsigned 64-bit integer value.\n\u201cdatatype\u201d : The data-type of the output tensor elements as defined\nin Tensor Data Types.\n\u201cparameters\u201d : An object containing zero or more parameters for this\ninput expressed as key/value pairs. See Parameters\nfor more information.\n\u201cdata\u201d: The contents of the tensor. See Tensor Data\nfor more information.\n\n", "inference-response-json-error-object": "\nInference Response JSON Error Object\u00b6\nA failed inference request must be indicated by an HTTP error status\n(typically 400). The HTTP body must contain the\n$inference_error_response object.\n    $inference_error_response =\n    {\n      \"error\": <error message string>\n    }\n\n\n\n\u201cerror\u201d : The descriptive message for the error.\n\n", "inference-request-examples": "\nInference Request Examples\u00b6\nThe following example shows an inference request to a model with two\ninputs and one output. The HTTP Content-Length header gives the size\nof the JSON object.\n    POST /v2/models/mymodel/infer HTTP/1.1\n    Host: localhost:8000\n    Content-Type: application/json\n    Content-Length: <xx>\n    {\n      \"id\" : \"42\",\n      \"inputs\" : [\n        {\n          \"name\" : \"input0\",\n          \"shape\" : [ 2, 2 ],\n          \"datatype\" : \"UINT32\",\n          \"data\" : [ 1, 2, 3, 4 ]\n        },\n        {\n          \"name\" : \"input1\",\n          \"shape\" : [ 3 ],\n          \"datatype\" : \"BOOL\",\n          \"data\" : [ true ]\n        }\n      ],\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\"\n        }\n      ]\n    }\n\n\nFor the above request the inference server must return the \u201coutput0\u201d\noutput tensor. Assuming the model returns a [ 3, 2 ] tensor of data\ntype FP32 the following response would be returned.\n    HTTP/1.1 200 OK\n    Content-Type: application/json\n    Content-Length: <yy>\n    {\n      \"id\" : \"42\"\n      \"outputs\" : [\n        {\n          \"name\" : \"output0\",\n          \"shape\" : [ 3, 2 ],\n          \"datatype\"  : \"FP32\",\n          \"data\" : [ 1.0, 1.1, 2.0, 2.1, 3.0, 3.1 ]\n        }\n      ]\n    }\n\n\n", "parameters": "\nParameters\u00b6\nThe *\\(parameters* JSON describes zero or more \u201cname\u201d/\u201dvalue\u201d pairs,\nwhere the \u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a\n\\)string, \\(number, or \\)boolean.\n    $parameters =\n    {\n      $parameter, ...\n    }\n\n    $parameter = $string : $string | $number | $boolean\n\n\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n", "tensor-data": "\nTensor Data\u00b6\nTensor data must be presented in row-major order of the tensor\nelements. Element values must be given in \u201clinear\u201d order without any\nstride or padding between elements. Tensor elements may be presented\nin their nature multi-dimensional representation, or as a flattened\none-dimensional representation.\nTensor data given explicitly is provided in a JSON array. Each element\nof the array may be an integer, floating-point number, string or\nboolean value. The server can decide to coerce each element to the\nrequired type or return an error if an unexpected value is\nreceived. Note that fp16 is problematic to communicate explicitly\nsince there is not a standard fp16 representation across backends nor\ntypically the programmatic support to create the fp16 representation\nfor a JSON number.\nFor example, the 2-dimensional matrix:\n[ 1 2\n  4 5 ]\n\n\nCan be represented in its natural format as:\n\"data\" : [ [ 1, 2 ], [ 4, 5 ] ]\n\n\nOr in a flattened one-dimensional representation:\n\"data\" : [ 1, 2, 4, 5 ]\n\n\n", "grpc": "\nGRPC\u00b6\nThe GRPC API closely follows the concepts defined in the\nHTTP/REST API. A compliant server must implement the\nhealth, metadata, and inference APIs described in this section.\nAll strings in all contexts are case-sensitive.\nThe GRPC definition of the service is:\n//\n// Inference Server GRPC endpoints.\n//\nservice GRPCInferenceService\n{\n  // Check liveness of the inference server.\n  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}\n\n  // Check readiness of the inference server.\n  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}\n\n  // Check readiness of a model in the inference server.\n  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}\n\n  // Get server metadata.\n  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}\n\n  // Get model metadata.\n  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}\n\n  // Perform inference using a specific model.\n  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}\n}\n\n\n\nHealth\u00b6\nA health request is made using the ServerLive, ServerReady, or\nModelReady endpoint. For each of these endpoints errors are indicated\nby the google.rpc.Status returned for the request. The OK code\nindicates success and other codes indicate failure.\n\nServer Live\u00b6\nThe ServerLive API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The request\nand response messages for ServerLive are:\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\n\n\n\nServer Ready\u00b6\nThe ServerReady API indicates if the server is ready for\ninferencing. The request and response messages for ServerReady are:\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n\nModel Ready\u00b6\nThe ModelReady API indicates if a specific model is ready for\ninferencing. The request and response messages for ModelReady are:\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n\n\nServer Metadata\u00b6\nThe ServerMetadata API provides information about the server. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ServerMetadata are:\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  // The server name.\n  string name = 1;\n\n  // The server version.\n  string version = 2;\n\n  // The extensions supported by the server.\n  repeated string extensions = 3;\n}\n\n\n\n\nModel Metadata\u00b6\nThe per-model metadata API provides information about a model. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ModelMetadata are:\nmessage ModelMetadataRequest\n{\n  // The name of the model.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  // Metadata for a tensor.\n  message TensorMetadata\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape. A variable-size dimension is represented\n    // by a -1 value.\n    repeated int64 shape = 3;\n  }\n\n  // The model name.\n  string name = 1;\n\n  // The versions of the model available on the server.\n  repeated string versions = 2;\n\n  // The model's platform. See Platforms.\n  string platform = 3;\n\n  // The model's inputs.\n  repeated TensorMetadata inputs = 4;\n\n  // The model's outputs.\n  repeated TensorMetadata outputs = 5;\n}\n\n\n\n\nInference\u00b6\nThe ModelInfer API performs inference using the specified\nmodel. Errors are indicated by the google.rpc.Status returned for the\nrequest. The OK code indicates success and other codes indicate\nfailure. The request and response messages for ModelInfer are:\nmessage ModelInferRequest\n{\n  // An input tensor for an inference request.\n  message InferInputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional inference input tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference request.\n    InferTensorContents contents = 5;\n  }\n\n  // An output tensor requested for an inference request.\n  message InferRequestedOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // Optional requested output tensor parameters.\n    map<string, InferParameter> parameters = 2;\n  }\n\n  // The name of the model to use for inferencing.\n  string model_name = 1;\n\n  // The version of the model to use for inference. If not given the\n  // server will choose a version based on the model and internal policy.\n  string model_version = 2;\n\n  // Optional identifier for the request. If specified will be\n  // returned in the response.\n  string id = 3;\n\n  // Optional inference parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The input tensors for the inference.\n  repeated InferInputTensor inputs = 5;\n\n  // The requested output tensors for the inference. Optional, if not\n  // specified all outputs produced by the model will be returned.\n  repeated InferRequestedOutputTensor outputs = 6;\n\n  // The data contained in an input tensor can be represented in \"raw\"\n  // bytes form or in the repeated type that matches the tensor's data\n  // type. To use the raw representation 'raw_input_contents' must be\n  // initialized with data for each tensor in the same order as\n  // 'inputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferInputTensor::contents must\n  // not be specified for any input tensor.\n  repeated bytes raw_input_contents = 7;\n}\n\nmessage ModelInferResponse\n{\n  // An output tensor returned for an inference request.\n  message InferOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional output tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference response.\n    InferTensorContents contents = 5;\n  }\n\n  // The name of the model used for inference.\n  string model_name = 1;\n\n  // The version of the model used for inference.\n  string model_version = 2;\n\n  // The id of the inference request if one was specified.\n  string id = 3;\n\n  // Optional inference response parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The output tensors holding inference results.\n  repeated InferOutputTensor outputs = 5;\n\n  // The data contained in an output tensor can be represented in\n  // \"raw\" bytes form or in the repeated type that matches the\n  // tensor's data type. To use the raw representation 'raw_output_contents'\n  // must be initialized with data for each tensor in the same order as\n  // 'outputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferOutputTensor::contents must\n  // not be specified for any output tensor.\n  repeated bytes raw_output_contents = 6;\n}\n\n\n\n\nParameters\u00b6\nThe Parameters message describes a \u201cname\u201d/\u201dvalue\u201d pair, where the\n\u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a boolean,\ninteger, or string corresponding to the parameter.\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n//\n// An inference parameter value.\n//\nmessage InferParameter\n{\n  // The parameter value can be a string, an int64, a boolean\n  // or a message specific to a predefined parameter.\n  oneof parameter_choice\n  {\n    // A boolean parameter value.\n    bool bool_param = 1;\n\n    // An int64 parameter value.\n    int64 int64_param = 2;\n\n    // A string parameter value.\n    string string_param = 3;\n  }\n}\n\n\n\n\nTensor Data\u00b6\nIn all representations tensor data must be flattened to a\none-dimensional, row-major order of the tensor elements. Element\nvalues must be given in \u201clinear\u201d order without any stride or padding\nbetween elements.\nUsing a \u201craw\u201d representation of tensors with\nModelInferRequest::raw_input_contents and\nModelInferResponse::raw_output_contents will typically allow higher\nperformance due to the way protobuf allocation and reuse interacts\nwith GRPC. For example, see https://github.com/grpc/grpc/issues/23231.\nAn alternative to the \u201craw\u201d representation is to use\nInferTensorContents to represent the tensor data in a format that\nmatches the tensor\u2019s data type.\n//\n// The data contained in a tensor represented by the repeated type\n// that matches the tensor's data type. Protobuf oneof is not used\n// because oneofs cannot contain repeated fields.\n//\nmessage InferTensorContents\n{\n  // Representation for BOOL data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bool bool_contents = 1;\n\n  // Representation for INT8, INT16, and INT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated int32 int_contents = 2;\n\n  // Representation for INT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated int64 int64_contents = 3;\n\n  // Representation for UINT8, UINT16, and UINT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated uint32 uint_contents = 4;\n\n  // Representation for UINT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated uint64 uint64_contents = 5;\n\n  // Representation for FP32 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated float fp32_contents = 6;\n\n  // Representation for FP64 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated double fp64_contents = 7;\n\n  // Representation for BYTES data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bytes bytes_contents = 8;\n}\n\n\n\n", "id1": "\nHealth\u00b6\nA health request is made using the ServerLive, ServerReady, or\nModelReady endpoint. For each of these endpoints errors are indicated\nby the google.rpc.Status returned for the request. The OK code\nindicates success and other codes indicate failure.\n\nServer Live\u00b6\nThe ServerLive API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The request\nand response messages for ServerLive are:\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\n\n\n\nServer Ready\u00b6\nThe ServerReady API indicates if the server is ready for\ninferencing. The request and response messages for ServerReady are:\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n\nModel Ready\u00b6\nThe ModelReady API indicates if a specific model is ready for\ninferencing. The request and response messages for ModelReady are:\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n\n", "id2": "\nServer Live\u00b6\nThe ServerLive API indicates if the inference server is able to\nreceive and respond to metadata and inference requests. The request\nand response messages for ServerLive are:\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\n\n", "id3": "\nServer Ready\u00b6\nThe ServerReady API indicates if the server is ready for\ninferencing. The request and response messages for ServerReady are:\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n", "id4": "\nModel Ready\u00b6\nThe ModelReady API indicates if a specific model is ready for\ninferencing. The request and response messages for ModelReady are:\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\n\n", "id5": "\nServer Metadata\u00b6\nThe ServerMetadata API provides information about the server. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ServerMetadata are:\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  // The server name.\n  string name = 1;\n\n  // The server version.\n  string version = 2;\n\n  // The extensions supported by the server.\n  repeated string extensions = 3;\n}\n\n\n", "id6": "\nModel Metadata\u00b6\nThe per-model metadata API provides information about a model. Errors\nare indicated by the google.rpc.Status returned for the request. The\nOK code indicates success and other codes indicate failure. The\nrequest and response messages for ModelMetadata are:\nmessage ModelMetadataRequest\n{\n  // The name of the model.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  // Metadata for a tensor.\n  message TensorMetadata\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape. A variable-size dimension is represented\n    // by a -1 value.\n    repeated int64 shape = 3;\n  }\n\n  // The model name.\n  string name = 1;\n\n  // The versions of the model available on the server.\n  repeated string versions = 2;\n\n  // The model's platform. See Platforms.\n  string platform = 3;\n\n  // The model's inputs.\n  repeated TensorMetadata inputs = 4;\n\n  // The model's outputs.\n  repeated TensorMetadata outputs = 5;\n}\n\n\n", "id7": "\nInference\u00b6\nThe ModelInfer API performs inference using the specified\nmodel. Errors are indicated by the google.rpc.Status returned for the\nrequest. The OK code indicates success and other codes indicate\nfailure. The request and response messages for ModelInfer are:\nmessage ModelInferRequest\n{\n  // An input tensor for an inference request.\n  message InferInputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional inference input tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference request.\n    InferTensorContents contents = 5;\n  }\n\n  // An output tensor requested for an inference request.\n  message InferRequestedOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // Optional requested output tensor parameters.\n    map<string, InferParameter> parameters = 2;\n  }\n\n  // The name of the model to use for inferencing.\n  string model_name = 1;\n\n  // The version of the model to use for inference. If not given the\n  // server will choose a version based on the model and internal policy.\n  string model_version = 2;\n\n  // Optional identifier for the request. If specified will be\n  // returned in the response.\n  string id = 3;\n\n  // Optional inference parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The input tensors for the inference.\n  repeated InferInputTensor inputs = 5;\n\n  // The requested output tensors for the inference. Optional, if not\n  // specified all outputs produced by the model will be returned.\n  repeated InferRequestedOutputTensor outputs = 6;\n\n  // The data contained in an input tensor can be represented in \"raw\"\n  // bytes form or in the repeated type that matches the tensor's data\n  // type. To use the raw representation 'raw_input_contents' must be\n  // initialized with data for each tensor in the same order as\n  // 'inputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferInputTensor::contents must\n  // not be specified for any input tensor.\n  repeated bytes raw_input_contents = 7;\n}\n\nmessage ModelInferResponse\n{\n  // An output tensor returned for an inference request.\n  message InferOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional output tensor parameters.\n    map<string, InferParameter> parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference response.\n    InferTensorContents contents = 5;\n  }\n\n  // The name of the model used for inference.\n  string model_name = 1;\n\n  // The version of the model used for inference.\n  string model_version = 2;\n\n  // The id of the inference request if one was specified.\n  string id = 3;\n\n  // Optional inference response parameters.\n  map<string, InferParameter> parameters = 4;\n\n  // The output tensors holding inference results.\n  repeated InferOutputTensor outputs = 5;\n\n  // The data contained in an output tensor can be represented in\n  // \"raw\" bytes form or in the repeated type that matches the\n  // tensor's data type. To use the raw representation 'raw_output_contents'\n  // must be initialized with data for each tensor in the same order as\n  // 'outputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 data type must be represented as raw\n  // content as there is no specific data type for a 16-bit float\n  // type.\n  //\n  // If this field is specified then InferOutputTensor::contents must\n  // not be specified for any output tensor.\n  repeated bytes raw_output_contents = 6;\n}\n\n\n", "id8": "\nParameters\u00b6\nThe Parameters message describes a \u201cname\u201d/\u201dvalue\u201d pair, where the\n\u201cname\u201d is the name of the parameter and the \u201cvalue\u201d is a boolean,\ninteger, or string corresponding to the parameter.\nCurrently no parameters are defined. As required a future proposal may\ndefine one or more standard parameters to allow portable functionality\nacross different inference servers. A server can implement\nserver-specific parameters to provide non-standard capabilities.\n//\n// An inference parameter value.\n//\nmessage InferParameter\n{\n  // The parameter value can be a string, an int64, a boolean\n  // or a message specific to a predefined parameter.\n  oneof parameter_choice\n  {\n    // A boolean parameter value.\n    bool bool_param = 1;\n\n    // An int64 parameter value.\n    int64 int64_param = 2;\n\n    // A string parameter value.\n    string string_param = 3;\n  }\n}\n\n\n", "id9": "\nTensor Data\u00b6\nIn all representations tensor data must be flattened to a\none-dimensional, row-major order of the tensor elements. Element\nvalues must be given in \u201clinear\u201d order without any stride or padding\nbetween elements.\nUsing a \u201craw\u201d representation of tensors with\nModelInferRequest::raw_input_contents and\nModelInferResponse::raw_output_contents will typically allow higher\nperformance due to the way protobuf allocation and reuse interacts\nwith GRPC. For example, see https://github.com/grpc/grpc/issues/23231.\nAn alternative to the \u201craw\u201d representation is to use\nInferTensorContents to represent the tensor data in a format that\nmatches the tensor\u2019s data type.\n//\n// The data contained in a tensor represented by the repeated type\n// that matches the tensor's data type. Protobuf oneof is not used\n// because oneofs cannot contain repeated fields.\n//\nmessage InferTensorContents\n{\n  // Representation for BOOL data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bool bool_contents = 1;\n\n  // Representation for INT8, INT16, and INT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated int32 int_contents = 2;\n\n  // Representation for INT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated int64 int64_contents = 3;\n\n  // Representation for UINT8, UINT16, and UINT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated uint32 uint_contents = 4;\n\n  // Representation for UINT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated uint64 uint64_contents = 5;\n\n  // Representation for FP32 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated float fp32_contents = 6;\n\n  // Representation for FP64 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated double fp64_contents = 7;\n\n  // Representation for BYTES data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bytes bytes_contents = 8;\n}\n\n\n", "platforms": "\nPlatforms\u00b6\nA platform is a string indicating a DL/ML framework or\nbackend. Platform is returned as part of the response to a\nModel Metadata request but is information only. The\nproposed inference APIs are generic relative to the DL/ML framework\nused by a model and so a client does not need to know the platform of\na given model to use the API. Platform names use the format\n\u201c_\u201d. The following platform names are allowed:\n\ntensorrt_plan : A TensorRT model encoded as a serialized engine or \u201cplan\u201d.\ntensorflow_graphdef : A TensorFlow model encoded as a GraphDef.\ntensorflow_savedmodel : A TensorFlow model encoded as a SavedModel.\nonnx_onnxv1 : A ONNX model encoded for ONNX Runtime.\npytorch_torchscript : A PyTorch model encoded as TorchScript.\nmxnet_mxnet: An MXNet model\ncaffe2_netdef : A Caffe2 model encoded as a NetDef.\n\n", "tensor-data-types": "\nTensor Data Types\u00b6\nTensor data types are shown in the following table along with the size\nof each type, in bytes.\n\n\nData Type\nSize (bytes)\n\n\n\nBOOL\n1\n\nUINT8\n1\n\nUINT16\n2\n\nUINT32\n4\n\nUINT64\n8\n\nINT8\n1\n\nINT16\n2\n\nINT32\n4\n\nINT64\n8\n\nFP16\n2\n\nFP32\n4\n\nFP64\n8\n\nBYTES\nVariable (max 232)\n\n\n\n", "references": "\nReferences\u00b6\nThis document is based on the KServe original created during the lifetime of the KFServing project in Kubeflow by its various contributors including Seldon, NVIDIA, IBM, Bloomberg and others.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/inference/v2.html", "key": "apis/inference/v2"}}, "apis/internal/agent": {"sections": {"agent-api": "\nAgent API\u00b6\nThis API is for communication between the Seldon Scheduler and the Seldon Agent which runs next to each inference server and manages the loading and unloading of models onto the server as well as acting as a reverse proxy in the data plane for handling requests to the inference server.\n\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.agent;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/agent\";\n\nimport \"mlops/scheduler/scheduler.proto\";\n\n// [START Messages]\n\nmessage ModelEventMessage {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n  string modelName = 3;\n  uint32 modelVersion = 4;\n  enum Event {\n      UNKNOWN_EVENT = 0;\n      LOAD_FAIL_MEMORY = 1;\n      LOADED = 2;\n      LOAD_FAILED = 3;\n      UNLOADED = 4;\n      UNLOAD_FAILED = 5;\n      REMOVED = 6; // unloaded and removed from local PVC\n      REMOVE_FAILED = 7;\n      RSYNC = 9; // Ask server for all models that need to be loaded\n      }\n  Event event = 5;\n  string message = 6;\n  uint64 availableMemoryBytes = 7;\n}\n\nmessage ModelEventResponse {\n\n}\n\nmessage ModelScalingTriggerMessage {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n  string modelName = 3;\n  uint32 modelVersion = 4;\n  enum Trigger {\n      SCALE_UP = 0;\n      SCALE_DOWN = 1;\n      }\n  Trigger trigger = 5;\n  uint32 amount = 6;  // number of replicas required\n  map<string,uint32> metrics = 7;  // optional metrics to expose to the scheduler\n}\n\nmessage ModelScalingTriggerResponse {\n\n}\n\nmessage AgentDrainRequest {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n}\n\nmessage AgentDrainResponse {\n  bool success = 1;\n}\n\nmessage AgentSubscribeRequest {\n  string serverName = 1;\n  bool shared = 2;\n  uint32 replicaIdx = 3;\n  ReplicaConfig replicaConfig = 4;\n  repeated ModelVersion loadedModels = 5;\n  uint64 availableMemoryBytes = 6;\n}\n\nmessage ReplicaConfig {\n  string inferenceSvc = 1; // inference DNS service name\n  int32 inferenceHttpPort = 2; // inference HTTP port\n  int32 inferenceGrpcPort = 3; // Inference grpc port\n  uint64 memoryBytes = 4; // The memory capacity of the server replica\n  repeated string capabilities = 5; // The list of capabilities of the server, e.g. sklearn, pytorch, xgboost, mlflow\n  uint32 overCommitPercentage = 6; // The percentage of over commit to allow, set to 0 (%) to disable over commit\n}\n\nmessage ModelOperationMessage {\n  enum Operation {\n    UNKNOWN_EVENT = 0;\n    LOAD_MODEL = 1;\n    UNLOAD_MODEL = 2;\n  }\n  Operation operation = 1;\n  ModelVersion modelVersion = 2;\n  bool autoscalingEnabled = 3;\n}\n\nmessage ModelVersion {\n  scheduler.Model model = 1;\n  uint32 version = 2;\n}\n\n// [END Messages]\n\n// [START Services]\n\nservice AgentService {\n  rpc AgentEvent(ModelEventMessage) returns (ModelEventResponse) {};\n  rpc Subscribe(AgentSubscribeRequest) returns (stream ModelOperationMessage) {};\n  rpc ModelScalingTrigger(stream ModelScalingTriggerMessage) returns (ModelScalingTriggerResponse) {};\n  rpc AgentDrain(AgentDrainRequest) returns (AgentDrainResponse) {};\n}\n\n// [END Services]\n\n\n\n", "proto-definition": "\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.agent;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/agent\";\n\nimport \"mlops/scheduler/scheduler.proto\";\n\n// [START Messages]\n\nmessage ModelEventMessage {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n  string modelName = 3;\n  uint32 modelVersion = 4;\n  enum Event {\n      UNKNOWN_EVENT = 0;\n      LOAD_FAIL_MEMORY = 1;\n      LOADED = 2;\n      LOAD_FAILED = 3;\n      UNLOADED = 4;\n      UNLOAD_FAILED = 5;\n      REMOVED = 6; // unloaded and removed from local PVC\n      REMOVE_FAILED = 7;\n      RSYNC = 9; // Ask server for all models that need to be loaded\n      }\n  Event event = 5;\n  string message = 6;\n  uint64 availableMemoryBytes = 7;\n}\n\nmessage ModelEventResponse {\n\n}\n\nmessage ModelScalingTriggerMessage {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n  string modelName = 3;\n  uint32 modelVersion = 4;\n  enum Trigger {\n      SCALE_UP = 0;\n      SCALE_DOWN = 1;\n      }\n  Trigger trigger = 5;\n  uint32 amount = 6;  // number of replicas required\n  map<string,uint32> metrics = 7;  // optional metrics to expose to the scheduler\n}\n\nmessage ModelScalingTriggerResponse {\n\n}\n\nmessage AgentDrainRequest {\n  string serverName = 1;\n  uint32 replicaIdx = 2;\n}\n\nmessage AgentDrainResponse {\n  bool success = 1;\n}\n\nmessage AgentSubscribeRequest {\n  string serverName = 1;\n  bool shared = 2;\n  uint32 replicaIdx = 3;\n  ReplicaConfig replicaConfig = 4;\n  repeated ModelVersion loadedModels = 5;\n  uint64 availableMemoryBytes = 6;\n}\n\nmessage ReplicaConfig {\n  string inferenceSvc = 1; // inference DNS service name\n  int32 inferenceHttpPort = 2; // inference HTTP port\n  int32 inferenceGrpcPort = 3; // Inference grpc port\n  uint64 memoryBytes = 4; // The memory capacity of the server replica\n  repeated string capabilities = 5; // The list of capabilities of the server, e.g. sklearn, pytorch, xgboost, mlflow\n  uint32 overCommitPercentage = 6; // The percentage of over commit to allow, set to 0 (%) to disable over commit\n}\n\nmessage ModelOperationMessage {\n  enum Operation {\n    UNKNOWN_EVENT = 0;\n    LOAD_MODEL = 1;\n    UNLOAD_MODEL = 2;\n  }\n  Operation operation = 1;\n  ModelVersion modelVersion = 2;\n  bool autoscalingEnabled = 3;\n}\n\nmessage ModelVersion {\n  scheduler.Model model = 1;\n  uint32 version = 2;\n}\n\n// [END Messages]\n\n// [START Services]\n\nservice AgentService {\n  rpc AgentEvent(ModelEventMessage) returns (ModelEventResponse) {};\n  rpc Subscribe(AgentSubscribeRequest) returns (stream ModelOperationMessage) {};\n  rpc ModelScalingTrigger(stream ModelScalingTriggerMessage) returns (ModelScalingTriggerResponse) {};\n  rpc AgentDrain(AgentDrainRequest) returns (AgentDrainResponse) {};\n}\n\n// [END Services]\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/internal/agent.html", "key": "apis/internal/agent"}}, "cli/docs/seldon_config": {"sections": {"seldon-config": "\nseldon config\u00b6\nmanage configs\n\nSynopsis\u00b6\nManage and activate configuration files for the CLI\nseldon config <subcomand> [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for config\n\n\n\n\nSEE ALSO\u00b6\n\nseldon\t -\nseldon config activate\t - activate config\nseldon config add\t - add config\nseldon config deactivate\t - deactivate config\nseldon config list\t - list configs\nseldon config remove\t - remove config\n\n\n", "synopsis": "\nSynopsis\u00b6\nManage and activate configuration files for the CLI\nseldon config <subcomand> [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for config\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon\t -\nseldon config activate\t - activate config\nseldon config add\t - add config\nseldon config deactivate\t - deactivate config\nseldon config list\t - list configs\nseldon config remove\t - remove config\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config.html", "key": "cli/docs/seldon_config"}}, "cli/docs/seldon_config_deactivate": {"sections": {"seldon-config-deactivate": "\nseldon config deactivate\u00b6\ndeactivate config\n\nSynopsis\u00b6\ndeactivate config\nseldon config deactivate [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for deactivate\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n\n", "synopsis": "\nSynopsis\u00b6\ndeactivate config\nseldon config deactivate [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for deactivate\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config_deactivate.html", "key": "cli/docs/seldon_config_deactivate"}}, "cli": {"sections": {"cli": "\nCLI\u00b6\nSeldon provides a CLI to allow easy management and testing of Model, Experiment, and Pipeline resources.\nAt present this needs to be built by hand from the operator folder.\nmake build-seldon     # for linux/macOS amd64\nmake build-seldon-arm # for macOS ARM\n\n\nThen place the bin/seldon executable in your path.\n\ncli docs\n\n\nEnvironment Variables and Services\u00b6\nThe CLI talks to 3 backend services on default endpoints:\n\nThe Seldon Core V2 Scheduler: default 0.0.0.0:9004\nThe Seldon Core inference endpoint: default 0.0.0.0:9000\nThe Seldon Kafka broker: default: 0.0.0.0:9092\n\nThese defaults will be correct when Seldon Core v2 is installed locally as per the docs. For Kubernetes, you will need to change these by defining environment variables.\nconst (\n\tdefaultInferHost     = \"0.0.0.0:9000\"\n\tdefaultKafkaHost     = \"0.0.0.0:9092\"\n\tdefaultSchedulerHost = \"0.0.0.0:9004\"\n)\n\n\n\n\nKubernetes Usage\u00b6\n\nInference Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the inference svc as a loadbalancer you will find it at:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 80:\nexport SELDON_INFER_HOST=<ip>:80\n\n\n\n\nScheduler Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the scheduler svc as a loadbalancer you will find it at:\nkubectl get svc seldon-scheduler -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 9004:\nexport SELDON_SCHEDULE_HOST=<ip>:9004\n\n\n\n\nKafka Broker\u00b6\nThe Kafka broker will depend on how you have installed Kafka into your Kubernetes cluster. Find the broker IP and use:\nexport SELDON_KAFKA_BROKER=<ip>:<port>\n\n\n\n\n\nConfig file\u00b6\nYou can create a config file to manage connections to running seldon core v2 installs. The settings will override any environment variable settings.\nThe definition is shown below:\ntype SeldonCLIConfig struct {\n\tDataplane    *Dataplane    `json:\"dataplane,omitempty\"`\n\tControlplane *ControlPlane `json:\"controlplane,omitempty\"`\n\tKafka        *KafkaConfig  `json:\"kafka,omitempty\"`\n}\n\ntype Dataplane struct {\n\tInferHost     string `json:\"inferHost,omitempty\"`\n\tTls           bool   `json:\"tls,omitempty\"`\n\tSkipSSLVerify bool   `json:\"skipSSLVerify,omitempty\"`\n\tKeyPath       string `json:\"keyPath,omitempty\"`\n\tCrtPath       string `json:\"crtPath,omitempty\"`\n\tCaPath        string `json:\"caPath,omitempty\"`\n}\n\ntype ControlPlane struct {\n\tSchedulerHost string `json:\"schedulerHost,omitempty\"`\n\tTls           bool   `json:\"tls,omitempty\"`\n\tKeyPath       string `json:\"keyPath,omitempty\"`\n\tCrtPath       string `json:\"crtPath,omitempty\"`\n\tCaPath        string `json:\"caPath,omitempty\"`\n}\n\nconst (\n\tKafkaConfigProtocolSSL          = \"ssl\"\n\tKafkaConfigProtocolSASLSSL      = \"sasl_ssl\"\n\tKafkaConfigProtocolSASLPlaintxt = \"sasl_plaintxt\"\n)\n\ntype KafkaConfig struct {\n\tBootstrap    string `json:\"bootstrap,omitempty\"`\n\tNamespace    string `json:\"namespace,omitempty\"`\n\tProtocol     string `json:\"protocol,omitempty\"`\n\tKeyPath      string `json:\"keyPath,omitempty\"`\n\tCrtPath      string `json:\"crtPath,omitempty\"`\n\tCaPath       string `json:\"caPath,omitempty\"`\n\tSaslUsername string `json:\"saslUsername,omitempty\"`\n\tSaslPassword string `json:\"saslPassword,omitempty\"`\n\tTopicPrefix  string `json:\"topicPrefix,omitempty\"`\n}\n\n\nAn example below shows an example where we connect via TLS to the Seldon scheduler using our scheduler client certificate:\n{\n    \"controlplane\":{\n\t\"schedulerHost\": \"seldon-scheduler.svc:9044\",\n\t\"tls\"; true,\n\t\"keyPath\": \"/home/certs/seldon-scheduler-client/tls.key\",\n\t\"crtPath\": \"/home/certs/seldon-scheduler-client/tls.crt\",\n\t\"caPath\": \"/home/certs/seldon-scheduler-client/ca.crt\"\n    }\n}\n\n\n\nTo manage config files and activate them you can use the CLI command seldon config which has subcommands to list, add, remove, activate and decative configs.\nFor example:\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\t\t*\n\n$ seldon config deactivate kind-sasl\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config add gcp-scv2 ~/seldon/cli/gcp.json\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\ngcp-scv2\t/home/work/seldon/cli/gcp.json\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config activate gcp-scv2\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\ngcp-scv2\t/home/work/seldon/cli/gcp.json\t    \t\t*\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config list kind-sasl\n{\n  \"controlplane\": {\n    \"schedulerHost\": \"172.19.255.2:9004\"\n  },\n  \"kafka\": {\n    \"bootstrap\": \"172.19.255.3:9093\",\n    \"caPath\": \"/home/work/gcp/scv2/certs/seldon-cluster-ca-cert/ca.crt\"\n  }\n}\n\n\n\n\nTLS Certificates for Local Use\u00b6\nFor running with Kubernetes TLS connections on the control and/or data plane, certificates will need to be downloaded locally. We provide an example script which will download certificates from a Kubernetes secret and store them in a folder. It can be found in hack/download-k8s-certs.sh and takes 2 or 3 arguments:\n./download-k8s-certs.sh <namespace> <secret> [<folder>]\n\n\ne.g.:\n./download-k8s-certs.sh seldon-mesh seldon-scheduler-client\n\n\n\n\n\n", "environment-variables-and-services": "\nEnvironment Variables and Services\u00b6\nThe CLI talks to 3 backend services on default endpoints:\n\nThe Seldon Core V2 Scheduler: default 0.0.0.0:9004\nThe Seldon Core inference endpoint: default 0.0.0.0:9000\nThe Seldon Kafka broker: default: 0.0.0.0:9092\n\nThese defaults will be correct when Seldon Core v2 is installed locally as per the docs. For Kubernetes, you will need to change these by defining environment variables.\nconst (\n\tdefaultInferHost     = \"0.0.0.0:9000\"\n\tdefaultKafkaHost     = \"0.0.0.0:9092\"\n\tdefaultSchedulerHost = \"0.0.0.0:9004\"\n)\n\n\n", "kubernetes-usage": "\nKubernetes Usage\u00b6\n\nInference Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the inference svc as a loadbalancer you will find it at:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 80:\nexport SELDON_INFER_HOST=<ip>:80\n\n\n\n\nScheduler Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the scheduler svc as a loadbalancer you will find it at:\nkubectl get svc seldon-scheduler -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 9004:\nexport SELDON_SCHEDULE_HOST=<ip>:9004\n\n\n\n\nKafka Broker\u00b6\nThe Kafka broker will depend on how you have installed Kafka into your Kubernetes cluster. Find the broker IP and use:\nexport SELDON_KAFKA_BROKER=<ip>:<port>\n\n\n\n", "inference-service": "\nInference Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the inference svc as a loadbalancer you will find it at:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 80:\nexport SELDON_INFER_HOST=<ip>:80\n\n\n", "scheduler-service": "\nScheduler Service\u00b6\nFor a default install into the seldon-mesh namespace if you have exposed the scheduler svc as a loadbalancer you will find it at:\nkubectl get svc seldon-scheduler -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nUse above IP at port 9004:\nexport SELDON_SCHEDULE_HOST=<ip>:9004\n\n\n", "kafka-broker": "\nKafka Broker\u00b6\nThe Kafka broker will depend on how you have installed Kafka into your Kubernetes cluster. Find the broker IP and use:\nexport SELDON_KAFKA_BROKER=<ip>:<port>\n\n\n", "config-file": "\nConfig file\u00b6\nYou can create a config file to manage connections to running seldon core v2 installs. The settings will override any environment variable settings.\nThe definition is shown below:\ntype SeldonCLIConfig struct {\n\tDataplane    *Dataplane    `json:\"dataplane,omitempty\"`\n\tControlplane *ControlPlane `json:\"controlplane,omitempty\"`\n\tKafka        *KafkaConfig  `json:\"kafka,omitempty\"`\n}\n\ntype Dataplane struct {\n\tInferHost     string `json:\"inferHost,omitempty\"`\n\tTls           bool   `json:\"tls,omitempty\"`\n\tSkipSSLVerify bool   `json:\"skipSSLVerify,omitempty\"`\n\tKeyPath       string `json:\"keyPath,omitempty\"`\n\tCrtPath       string `json:\"crtPath,omitempty\"`\n\tCaPath        string `json:\"caPath,omitempty\"`\n}\n\ntype ControlPlane struct {\n\tSchedulerHost string `json:\"schedulerHost,omitempty\"`\n\tTls           bool   `json:\"tls,omitempty\"`\n\tKeyPath       string `json:\"keyPath,omitempty\"`\n\tCrtPath       string `json:\"crtPath,omitempty\"`\n\tCaPath        string `json:\"caPath,omitempty\"`\n}\n\nconst (\n\tKafkaConfigProtocolSSL          = \"ssl\"\n\tKafkaConfigProtocolSASLSSL      = \"sasl_ssl\"\n\tKafkaConfigProtocolSASLPlaintxt = \"sasl_plaintxt\"\n)\n\ntype KafkaConfig struct {\n\tBootstrap    string `json:\"bootstrap,omitempty\"`\n\tNamespace    string `json:\"namespace,omitempty\"`\n\tProtocol     string `json:\"protocol,omitempty\"`\n\tKeyPath      string `json:\"keyPath,omitempty\"`\n\tCrtPath      string `json:\"crtPath,omitempty\"`\n\tCaPath       string `json:\"caPath,omitempty\"`\n\tSaslUsername string `json:\"saslUsername,omitempty\"`\n\tSaslPassword string `json:\"saslPassword,omitempty\"`\n\tTopicPrefix  string `json:\"topicPrefix,omitempty\"`\n}\n\n\nAn example below shows an example where we connect via TLS to the Seldon scheduler using our scheduler client certificate:\n{\n    \"controlplane\":{\n\t\"schedulerHost\": \"seldon-scheduler.svc:9044\",\n\t\"tls\"; true,\n\t\"keyPath\": \"/home/certs/seldon-scheduler-client/tls.key\",\n\t\"crtPath\": \"/home/certs/seldon-scheduler-client/tls.crt\",\n\t\"caPath\": \"/home/certs/seldon-scheduler-client/ca.crt\"\n    }\n}\n\n\n\nTo manage config files and activate them you can use the CLI command seldon config which has subcommands to list, add, remove, activate and decative configs.\nFor example:\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\t\t*\n\n$ seldon config deactivate kind-sasl\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config add gcp-scv2 ~/seldon/cli/gcp.json\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\ngcp-scv2\t/home/work/seldon/cli/gcp.json\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config activate gcp-scv2\n\n$ seldon config list\nconfig\t\tpath\t\t\t\t\t\tactive\n------\t\t----\t\t\t\t\t\t------\ngcp-scv2\t/home/work/seldon/cli/gcp.json\t    \t\t*\nkind-sasl\t/home/work/seldon/cli/config-sasl.json\n\n$ seldon config list kind-sasl\n{\n  \"controlplane\": {\n    \"schedulerHost\": \"172.19.255.2:9004\"\n  },\n  \"kafka\": {\n    \"bootstrap\": \"172.19.255.3:9093\",\n    \"caPath\": \"/home/work/gcp/scv2/certs/seldon-cluster-ca-cert/ca.crt\"\n  }\n}\n\n\n", "tls-certificates-for-local-use": "\nTLS Certificates for Local Use\u00b6\nFor running with Kubernetes TLS connections on the control and/or data plane, certificates will need to be downloaded locally. We provide an example script which will download certificates from a Kubernetes secret and store them in a folder. It can be found in hack/download-k8s-certs.sh and takes 2 or 3 arguments:\n./download-k8s-certs.sh <namespace> <secret> [<folder>]\n\n\ne.g.:\n./download-k8s-certs.sh seldon-mesh seldon-scheduler-client\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/index.html", "key": "cli"}}, "cli/docs/seldon_pipeline_inspect": {"sections": {"seldon-pipeline-inspect": "\nseldon pipeline inspect\u00b6\ninspect data in a pipeline\n\nSynopsis\u00b6\ninspect data in a pipeline. Specify as pipelineName or pipelineName.(inputs|outputs) or pipelineName.stepName or pipelineName.stepName.(inputs|outputs) or pipelineName.stepName.(inputs|outputs).tensorName\nseldon pipeline inspect <expression> [flags]\n\n\n\n\nOptions\u00b6\n      --format string           inspect output format: raw or json. Default raw (default \"raw\")\n  -h, --help                    help for inspect\n      --kafka-broker string     kafka broker (default \"0.0.0.0:9092\")\n      --namespace string        Kubernetes namespace. Default default (default \"default\")\n      --offset int              message offset to start reading from, i.e. default 1 is the last message only (default 1)\n      --request-id string       request id to show, if not specified will be all messages in offset range\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n      --verbose                 display more details, such as headers\n\n\n\n\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n\n", "synopsis": "\nSynopsis\u00b6\ninspect data in a pipeline. Specify as pipelineName or pipelineName.(inputs|outputs) or pipelineName.stepName or pipelineName.stepName.(inputs|outputs) or pipelineName.stepName.(inputs|outputs).tensorName\nseldon pipeline inspect <expression> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --format string           inspect output format: raw or json. Default raw (default \"raw\")\n  -h, --help                    help for inspect\n      --kafka-broker string     kafka broker (default \"0.0.0.0:9092\")\n      --namespace string        Kubernetes namespace. Default default (default \"default\")\n      --offset int              message offset to start reading from, i.e. default 1 is the last message only (default 1)\n      --request-id string       request id to show, if not specified will be all messages in offset range\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n      --verbose                 display more details, such as headers\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline_inspect.html", "key": "cli/docs/seldon_pipeline_inspect"}}, "examples/local-examples": {"sections": {"local-examples": "\nLocal Examples\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Non Kubernetes Local Examples\u00b6\n\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nLoad the model\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nWait for the model to be ready\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nDo a REST inference call\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"983bd95f-4b4d-4ff1-95b2-df9d6d089164\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\nseldon model infer iris --inference-mode grpc \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload iris\n\n\n\n\nTensorflow Model\u00b6\nWe run a simple tensorflow model. Note the requirements section specifying tensorflow.\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nGet model metadata\nseldon model metadata tfsimple1\n\n\n{\n\t\"name\": \"tfsimple1_1\",\n\t\"versions\": [\n\t\t\"1\"\n\t],\n\t\"platform\": \"tensorflow_graphdef\",\n\t\"inputs\": [\n\t\t{\n\t\t\t\"name\": \"INPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"INPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t],\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a REST inference call.\nseldon model infer tfsimple1 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"tfsimple1_1\",\n  \"model_version\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ]\n    }\n  ]\n}\n\n\nDo a gRPC inference call\nseldon model infer tfsimple1 --inference-mode grpc \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload tfsimple1\n\n\n\n\nExperiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate an experiment.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris | jq -M .\nseldon model status iris2 | jq -M .\n\n\n{\n  \"modelName\": \"iris\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n{\n  \"modelName\": \"iris2\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris2\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::57 :iris_1::43]\n\n\n\nRun one more request\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"fa425bdf-737c-41fe-894d-58868f70fe5d\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nWe will test REST and gRPC.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nShow the requests all go to original model now.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\n\n\n\n", "seldon-v2-non-kubernetes-local-examples": "\nSeldon V2 Non Kubernetes Local Examples\u00b6\n\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nLoad the model\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nWait for the model to be ready\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nDo a REST inference call\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"983bd95f-4b4d-4ff1-95b2-df9d6d089164\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\nseldon model infer iris --inference-mode grpc \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload iris\n\n\n\n\nTensorflow Model\u00b6\nWe run a simple tensorflow model. Note the requirements section specifying tensorflow.\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nGet model metadata\nseldon model metadata tfsimple1\n\n\n{\n\t\"name\": \"tfsimple1_1\",\n\t\"versions\": [\n\t\t\"1\"\n\t],\n\t\"platform\": \"tensorflow_graphdef\",\n\t\"inputs\": [\n\t\t{\n\t\t\t\"name\": \"INPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"INPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t],\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a REST inference call.\nseldon model infer tfsimple1 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"tfsimple1_1\",\n  \"model_version\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ]\n    }\n  ]\n}\n\n\nDo a gRPC inference call\nseldon model infer tfsimple1 --inference-mode grpc \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload tfsimple1\n\n\n\n\nExperiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate an experiment.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris | jq -M .\nseldon model status iris2 | jq -M .\n\n\n{\n  \"modelName\": \"iris\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n{\n  \"modelName\": \"iris2\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris2\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::57 :iris_1::43]\n\n\n\nRun one more request\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"fa425bdf-737c-41fe-894d-58868f70fe5d\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nWe will test REST and gRPC.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nShow the requests all go to original model now.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\n\n\n", "sklearn-model": "\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nLoad the model\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nWait for the model to be ready\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nDo a REST inference call\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"983bd95f-4b4d-4ff1-95b2-df9d6d089164\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\nseldon model infer iris --inference-mode grpc \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload iris\n\n\n", "tensorflow-model": "\nTensorflow Model\u00b6\nWe run a simple tensorflow model. Note the requirements section specifying tensorflow.\ncat ./models/tfsimple1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nLoad the model.\nseldon model load -f ./models/tfsimple1.yaml\n\n\n{}\n\n\nWait for the model to be ready.\nseldon model status tfsimple1 -w ModelAvailable | jq -M .\n\n\n{}\n\n\nGet model metadata\nseldon model metadata tfsimple1\n\n\n{\n\t\"name\": \"tfsimple1_1\",\n\t\"versions\": [\n\t\t\"1\"\n\t],\n\t\"platform\": \"tensorflow_graphdef\",\n\t\"inputs\": [\n\t\t{\n\t\t\t\"name\": \"INPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"INPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t],\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"OUTPUT0\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"OUTPUT1\",\n\t\t\t\"datatype\": \"INT32\",\n\t\t\t\"shape\": [\n\t\t\t\t-1,\n\t\t\t\t16\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a REST inference call.\nseldon model infer tfsimple1 \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"model_name\": \"tfsimple1_1\",\n  \"model_version\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        2,\n        4,\n        6,\n        8,\n        10,\n        12,\n        14,\n        16,\n        18,\n        20,\n        22,\n        24,\n        26,\n        28,\n        30,\n        32\n      ]\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        1,\n        16\n      ],\n      \"data\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ]\n    }\n  ]\n}\n\n\nDo a gRPC inference call\nseldon model infer tfsimple1 --inference-mode grpc \\\n    '{\"model_name\":\"tfsimple1\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"tfsimple1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0,\n          0\n        ]\n      }\n    }\n  ]\n}\n\n\nUnload the model\nseldon model unload tfsimple1\n\n\n", "experiment": "\nExperiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate an experiment.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris | jq -M .\nseldon model status iris2 | jq -M .\n\n\n{\n  \"modelName\": \"iris\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362720538Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n{\n  \"modelName\": \"iris2\",\n  \"versions\": [\n    {\n      \"version\": 1,\n      \"serverName\": \"mlserver\",\n      \"kubernetesMeta\": {},\n      \"modelReplicaState\": {\n        \"0\": {\n          \"state\": \"Available\",\n          \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n        }\n      },\n      \"state\": {\n        \"state\": \"ModelAvailable\",\n        \"availableReplicas\": 1,\n        \"lastChangeTimestamp\": \"2023-06-29T14:01:41.362845079Z\"\n      },\n      \"modelDefn\": {\n        \"meta\": {\n          \"name\": \"iris2\",\n          \"kubernetesMeta\": {}\n        },\n        \"modelSpec\": {\n          \"uri\": \"gs://seldon-models/mlserver/iris\",\n          \"requirements\": [\n            \"sklearn\"\n          ]\n        },\n        \"deploymentSpec\": {\n          \"replicas\": 1\n        }\n      }\n    }\n  ]\n}\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::57 :iris_1::43]\n\n\n\nRun one more request\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"fa425bdf-737c-41fe-894d-58868f70fe5d\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nWe will test REST and gRPC.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nShow the requests all go to original model now.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::100]\n\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/local-examples.html", "key": "examples/local-examples"}}, "models/parameterized-models": {"sections": {"parameterized-models": "\nParameterized Models\u00b6\nThe Model specification allows parameters to be passed to the loaded model to allow customization. For example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-one\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 1\"\n\n\nThis capability is only available for MLServer custom model runtimes. The named keys and values will be added to the model-settings.json file for the provided model in the\nparameters.extra Dict. MLServer models are able to read these values in their load method.\n\nExample Parameterized Models\u00b6\n\nPandas Query\n\n\n\n\n", "example-parameterized-models": "\nExample Parameterized Models\u00b6\n\nPandas Query\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/models/parameterized-models/index.html", "key": "models/parameterized-models"}}, "cli/docs/seldon_model_unload": {"sections": {"seldon-model-unload": "\nseldon model unload\u00b6\nunload a model\n\nSynopsis\u00b6\nunload a model\nseldon model unload <modelName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for unload\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n\n", "synopsis": "\nSynopsis\u00b6\nunload a model\nseldon model unload <modelName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for unload\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_model_unload.html", "key": "cli/docs/seldon_model_unload"}}, "cli/docs/seldon_pipeline_unload": {"sections": {"seldon-pipeline-unload": "\nseldon pipeline unload\u00b6\nunload a pipeline\n\nSynopsis\u00b6\nunload a pipeline\nseldon pipeline unload <pipelineName> [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for unload\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n\n", "synopsis": "\nSynopsis\u00b6\nunload a pipeline\nseldon pipeline unload <pipelineName> [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -h, --help                    help for unload\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline_unload.html", "key": "cli/docs/seldon_pipeline_unload"}}, "kubernetes/service-meshes/ambassador": {"sections": {"ambassador": "\nAmbassador\u00b6\nAmbassador provides service mesh and ingress products. Our examples here are based on the Emissary ingress.\nWe will run through some examples as shown in the notebook service-meshes/ambassador/ambassador.ipynb\n\nSingle Model\u00b6\n\nSeldon Iris classifier model\nDefault Ambassador Host and Listener\nAmbassador Mappings for REST and gRPC endpoints\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  hostname: '*'\n  prefix: /v2/\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n\n\nTraffic Split\u00b6\n\nWarning\nTraffic splitting does not presently work due to this issue. We recommend you use a Seldon Experiment instead.\n\nSeldon provides an Experiment resource for service mesh agnostic traffic splitting but if you wish to control this via Ambassador and example is shown below to split traffic between two models.\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  add_response_headers:\n    seldon_model:\n      value: iris1\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  add_response_headers:\n    seldon_model:\n      value: iris2\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n\n\nAmbassador Notebook Example\u00b6\nAssumes\n\nYou have installed emissary as per their docs\n\nTested with\nemissary-ingress-7.3.2 insatlled via helm\nINGRESS_IP=!kubectl get svc emissary-ingress -n emissary -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nAmbassador Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  hostname: '*'\n  prefix: /v2/\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/single-model | kubectl apply --validate=false -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris-grpc created\nmapping.getambassador.io/iris-http created\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\"\\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:45:43 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 792\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"72ac79f5-b355-4be3-b8c5-2ebedaa39f60\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris-grpc\" deleted\nmapping.getambassador.io \"iris-http\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\nTraffic Split Two Models\u00b6\nCurrently not working due to this issue\n!kustomize build config/traffic-split\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  add_response_headers:\n    seldon_model:\n      value: iris1\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  add_response_headers:\n    seldon_model:\n      value: iris2\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris1-grpc created\nmapping.getambassador.io/iris1-http created\nmapping.getambassador.io/iris2-grpc created\nmapping.getambassador.io/iris2-http created\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:46:17 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 920\n< seldon-route: iris2_1\n< seldon_model: iris2\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris2_1\",\"model_version\":\"1\",\"id\":\"ed521c32-cd85-4cb8-90eb-7c896803f271\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris2_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris1-grpc\" deleted\nmapping.getambassador.io \"iris1-http\" deleted\nmapping.getambassador.io \"iris2-grpc\" deleted\nmapping.getambassador.io \"iris2-http\" deleted\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n\n\n", "single-model": "\nSingle Model\u00b6\n\nSeldon Iris classifier model\nDefault Ambassador Host and Listener\nAmbassador Mappings for REST and gRPC endpoints\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  hostname: '*'\n  prefix: /v2/\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n", "traffic-split": "\nTraffic Split\u00b6\n\nWarning\nTraffic splitting does not presently work due to this issue. We recommend you use a Seldon Experiment instead.\n\nSeldon provides an Experiment resource for service mesh agnostic traffic splitting but if you wish to control this via Ambassador and example is shown below to split traffic between two models.\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  add_response_headers:\n    seldon_model:\n      value: iris1\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  add_response_headers:\n    seldon_model:\n      value: iris2\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n", "ambassador-notebook-example": "\nAmbassador Notebook Example\u00b6\nAssumes\n\nYou have installed emissary as per their docs\n\nTested with\nemissary-ingress-7.3.2 insatlled via helm\nINGRESS_IP=!kubectl get svc emissary-ingress -n emissary -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nAmbassador Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  hostname: '*'\n  prefix: /v2/\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/single-model | kubectl apply --validate=false -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris-grpc created\nmapping.getambassador.io/iris-http created\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\"\\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:45:43 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 792\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"72ac79f5-b355-4be3-b8c5-2ebedaa39f60\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris-grpc\" deleted\nmapping.getambassador.io \"iris-http\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\nTraffic Split Two Models\u00b6\nCurrently not working due to this issue\n!kustomize build config/traffic-split\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  add_response_headers:\n    seldon_model:\n      value: iris1\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  add_response_headers:\n    seldon_model:\n      value: iris2\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris1-grpc created\nmapping.getambassador.io/iris1-http created\nmapping.getambassador.io/iris2-grpc created\nmapping.getambassador.io/iris2-http created\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:46:17 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 920\n< seldon-route: iris2_1\n< seldon_model: iris2\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris2_1\",\"model_version\":\"1\",\"id\":\"ed521c32-cd85-4cb8-90eb-7c896803f271\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris2_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris1-grpc\" deleted\nmapping.getambassador.io \"iris1-http\" deleted\nmapping.getambassador.io \"iris2-grpc\" deleted\nmapping.getambassador.io \"iris2-http\" deleted\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n\n", "ambassador-single-model-example": "\nAmbassador Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris\n  hostname: '*'\n  prefix: /v2/\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/single-model | kubectl apply --validate=false -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris-grpc created\nmapping.getambassador.io/iris-http created\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\"\\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:45:43 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 792\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"72ac79f5-b355-4be3-b8c5-2ebedaa39f60\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris-grpc\" deleted\nmapping.getambassador.io \"iris-http\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n", "traffic-split-two-models": "\nTraffic Split Two Models\u00b6\nCurrently not working due to this issue\n!kustomize build config/traffic-split\n\n\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard\n  namespace: seldon-mesh\nspec:\n  hostname: '*'\n  requestPolicy:\n    insecure:\n      action: Route\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-listener-8080\n  namespace: seldon-mesh\nspec:\n  hostBinding:\n    namespace:\n      from: ALL\n  port: 8080\n  protocol: HTTP\n  securityModel: INSECURE\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris1-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris1\n  add_response_headers:\n    seldon_model:\n      value: iris1\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-grpc\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  grpc: true\n  hostname: '*'\n  prefix: /inference.GRPCInferenceService\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: iris2-http\n  namespace: seldon-mesh\nspec:\n  add_request_headers:\n    seldon-model:\n      value: iris2\n  add_response_headers:\n    seldon_model:\n      value: iris2\n  hostname: '*'\n  prefix: /v2\n  rewrite: \"\"\n  service: seldon-mesh:80\n  weight: 50\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nhost.getambassador.io/wildcard created\nlistener.getambassador.io/emissary-ingress-listener-8080 created\nmapping.getambassador.io/iris1-grpc created\nmapping.getambassador.io/iris1-http created\nmapping.getambassador.io/iris2-grpc created\nmapping.getambassador.io/iris2-http created\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:46:17 GMT\n< server: envoy\n< x-envoy-upstream-service-time: 920\n< seldon-route: iris2_1\n< seldon_model: iris2\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris2_1\",\"model_version\":\"1\",\"id\":\"ed521c32-cd85-4cb8-90eb-7c896803f271\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris2_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nhost.getambassador.io \"wildcard\" deleted\nlistener.getambassador.io \"emissary-ingress-listener-8080\" deleted\nmapping.getambassador.io \"iris1-grpc\" deleted\nmapping.getambassador.io \"iris1-http\" deleted\nmapping.getambassador.io \"iris2-grpc\" deleted\nmapping.getambassador.io \"iris2-http\" deleted\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/service-meshes/ambassador/index.html", "key": "kubernetes/service-meshes/ambassador"}}, "cli/docs/seldon_config_list": {"sections": {"seldon-config-list": "\nseldon config list\u00b6\nlist configs\n\nSynopsis\u00b6\nlist configs\nseldon config list [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for list\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n\n", "synopsis": "\nSynopsis\u00b6\nlist configs\nseldon config list [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for list\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config_list.html", "key": "cli/docs/seldon_config_list"}}, "upgrading": {"sections": {"upgrading": "\nUpgrading\u00b6\n\nUpgrading from 2.6 - 2.7\u00b6\nAll pods provisioned through the operator i.e. SeldonRuntime and Server resources now have the label app.kubernetes.io/name for identifying the pods.\nPreviously, the labelling has been inconsistent across different versions of Seldon Core v2, with mixture of app and app.kubernetes.io/name used.\nIf using the Prometheus operator (\u201cKube Prometheus\u201d), please apply the v2.7.0 manifests for Core v2 according to the metrics documentation.\nNote that these manifests need to be adjusted to discover metrics endpoints based on the existing setup.\nIf previous pod monitors had namespaceSelector fields set, these should be copied over and applied to the new manifests.\nIf namespaces do not matter, cluster-wide metrics endpoint discovery can be setup by modifying the namespaceSelector field in the pod monitors:\nspec:\n  namespaceSelector:\n    any: true\n\n\n\n\nUpgrading from 2.5 - 2.6\u00b6\nRelease 2.6 brings with it new custom resources SeldonConfig and SeldonRuntime, which provide a new way to install Seldon Core V2 in Kubernetes. Upgrading in the same namespace will cause downtime while the pods are being recreated. Alternatively  users can have an external service mesh or other means to be used over multiple namespaces to bring up the system in a new namespace and redeploy models before switch traffic between them.\nIf the new 2.6 charts are used to upgrade in an existing namespace models will eventually be redeloyed but there will be service downtime as the core components are redeployed.\n\n", "upgrading-from-2-6-2-7": "\nUpgrading from 2.6 - 2.7\u00b6\nAll pods provisioned through the operator i.e. SeldonRuntime and Server resources now have the label app.kubernetes.io/name for identifying the pods.\nPreviously, the labelling has been inconsistent across different versions of Seldon Core v2, with mixture of app and app.kubernetes.io/name used.\nIf using the Prometheus operator (\u201cKube Prometheus\u201d), please apply the v2.7.0 manifests for Core v2 according to the metrics documentation.\nNote that these manifests need to be adjusted to discover metrics endpoints based on the existing setup.\nIf previous pod monitors had namespaceSelector fields set, these should be copied over and applied to the new manifests.\nIf namespaces do not matter, cluster-wide metrics endpoint discovery can be setup by modifying the namespaceSelector field in the pod monitors:\nspec:\n  namespaceSelector:\n    any: true\n\n\n", "upgrading-from-2-5-2-6": "\nUpgrading from 2.5 - 2.6\u00b6\nRelease 2.6 brings with it new custom resources SeldonConfig and SeldonRuntime, which provide a new way to install Seldon Core V2 in Kubernetes. Upgrading in the same namespace will cause downtime while the pods are being recreated. Alternatively  users can have an external service mesh or other means to be used over multiple namespaces to bring up the system in a new namespace and redeploy models before switch traffic between them.\nIf the new 2.6 charts are used to upgrade in an existing namespace models will eventually be redeloyed but there will be service downtime as the core components are redeployed.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/upgrading/index.html", "key": "upgrading"}}, "models/inference-artifacts": {"sections": {"inference-artifacts": "\nInference Artifacts\u00b6\nTo run your model inside Seldon you must supply an inference artifact that can be downloaded and run on one of MLServer or Triton inference servers. We list artifacts below by alphabetical order below.\n\n\nType\nServer\nTag\nExample\n\n\n\nAlibi-Detect\nMLServer\nalibi-detect\nexample\n\nAlibi-Explain\nMLServer\nalibi-explain\nexample\n\nDALI\nTriton\ndali\nTBC\n\nHuggingface\nMLServer\nhuggingface\nexample\n\nLightGBM\nMLServer\nlightgbm\nexample\n\nMLFlow\nMLServer\nmlflow\nexample\n\nONNX\nTriton\nonnx\nexample\n\nOpenVino\nTriton\nopenvino\nTBC\n\nCustom Python\nMLServer\npython, mlserver\nexample\n\nCustom Python\nTriton\npython, triton\nexample\n\nPyTorch\nTriton\npytorch\nexample\n\nSKLearn\nMLServer\nsklearn\nexample\n\nSpark Mlib\nMLServer\nspark-mlib\nTBC\n\nTensorflow\nTriton\ntensorflow\nexample\n\nTensorRT\nTriton\ntensorrt\nTBC\n\nTriton FIL\nTriton\nfil\nTBC\n\nXGBoost\nMLServer\nxgboost\nexample\n\n\n\n\nSaving Model artifacts\u00b6\nFor many machine learning artifacts you can simply save them to a folder and load them into seldon core v2. Details are given below as well as a link to creating a custom model settings file if needed.\n\n\nType\nNotes\nCustom Model Settings\n\n\n\nAlibi-Detect\nSave model using Alibi-Detect.\ndocs\n\nAlibi-Explain\nSave model using Alibi-Explain.\ndocs\n\nDALI\nFollow the Triton docs to create a config.pbtxt and model folder with artifact.\ndocs\n\nHuggingface\nCreate an MLServer model-settings.json with the Huggingface model required\ndocs\n\nLightGBM\nSave model to file with extension.bst.\ndocs\n\nMLFlow\nUse the created artifacts/model folder from your training run.\ndocs\n\nONNX\nSave you model with name model.onnx.\ndocs\n\nOpenVino\nFollow the Triton docs to create your model artifacts.\ndocs\n\nCustom MLServer Python\nCreate a python file with a class that extends MLModel.\ndocs\n\nCustom Triton Python\nFollow the Triton docs to create your config.pbtxt and associated python files.\ndocs\n\nPyTorch\nCreate a Triton config.pbtxt describing inputs and outputs and place traced torchscript in folder as model.pt.\ndocs\n\nSKLearn\nSave model via joblib to a file with extension .joblib or with pickle to a file with extension .pkl or .pickle.\ndocs\n\nSpark Mlib\nFollow the MLServer docs.\ndocs\n\nTensorflow\nSave model in \u201cSaved Model\u201d format as model.savedodel. If using graphdef format you will need to create Triton config.pbtxt and place your model in a numbered sub folder. HDF5 is not supported.\ndocs\n\nTensorRT\nFollow the Triton docs to create your model artifacts.\ndocs\n\nTriton FIL\nFollow the Triton docs to create your model artifacts.\ndocs\n\nXGBoost\nSave model to file with extension.bst or .json.\ndocs\n\n\n\n\n\nCustom MLServer Model Settings\u00b6\nFor MLServer targeted models you can create a model-settings.json file to help MLServer load your model and place this alongside your artifact. See the MLServer project  for details.\n\n\nCustom Triton Configuration\u00b6\nFor Triton inference server models you can create a configuration config.pbtxt file alongside your artifact.\n\n\nNotes\u00b6\n\nThe tag field represents the tag you need to add to the requirements part of the Model spec for your artifact to be loaded on a compatible server. e.g. for an sklearn model:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\n\n\n\n", "saving-model-artifacts": "\nSaving Model artifacts\u00b6\nFor many machine learning artifacts you can simply save them to a folder and load them into seldon core v2. Details are given below as well as a link to creating a custom model settings file if needed.\n\n\nType\nNotes\nCustom Model Settings\n\n\n\nAlibi-Detect\nSave model using Alibi-Detect.\ndocs\n\nAlibi-Explain\nSave model using Alibi-Explain.\ndocs\n\nDALI\nFollow the Triton docs to create a config.pbtxt and model folder with artifact.\ndocs\n\nHuggingface\nCreate an MLServer model-settings.json with the Huggingface model required\ndocs\n\nLightGBM\nSave model to file with extension.bst.\ndocs\n\nMLFlow\nUse the created artifacts/model folder from your training run.\ndocs\n\nONNX\nSave you model with name model.onnx.\ndocs\n\nOpenVino\nFollow the Triton docs to create your model artifacts.\ndocs\n\nCustom MLServer Python\nCreate a python file with a class that extends MLModel.\ndocs\n\nCustom Triton Python\nFollow the Triton docs to create your config.pbtxt and associated python files.\ndocs\n\nPyTorch\nCreate a Triton config.pbtxt describing inputs and outputs and place traced torchscript in folder as model.pt.\ndocs\n\nSKLearn\nSave model via joblib to a file with extension .joblib or with pickle to a file with extension .pkl or .pickle.\ndocs\n\nSpark Mlib\nFollow the MLServer docs.\ndocs\n\nTensorflow\nSave model in \u201cSaved Model\u201d format as model.savedodel. If using graphdef format you will need to create Triton config.pbtxt and place your model in a numbered sub folder. HDF5 is not supported.\ndocs\n\nTensorRT\nFollow the Triton docs to create your model artifacts.\ndocs\n\nTriton FIL\nFollow the Triton docs to create your model artifacts.\ndocs\n\nXGBoost\nSave model to file with extension.bst or .json.\ndocs\n\n\n\n", "custom-mlserver-model-settings": "\nCustom MLServer Model Settings\u00b6\nFor MLServer targeted models you can create a model-settings.json file to help MLServer load your model and place this alongside your artifact. See the MLServer project  for details.\n", "custom-triton-configuration": "\nCustom Triton Configuration\u00b6\nFor Triton inference server models you can create a configuration config.pbtxt file alongside your artifact.\n", "notes": "\nNotes\u00b6\n\nThe tag field represents the tag you need to add to the requirements part of the Model spec for your artifact to be loaded on a compatible server. e.g. for an sklearn model:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/models/inference-artifacts/index.html", "key": "models/inference-artifacts"}}, "kubernetes/tracing": {"sections": {"tracing": "\nTracing\u00b6\nWe support Open Telemetry tracing. By default all components will attempt to send OLTP events to seldon-collector.seldon-mesh:4317 which will export to Jaeger at simplest-collector.seldon-mesh:4317.\nThe components can be installed from the tracing/k8s folder. In future an Ansible playbook will be created. This installs a Open Telemetry collector and a simple Jaeger install with a service that can be port forwarded to at simplest.seldon-mesh:16686.\nAn example Jaeger trace is show below:\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/tracing/index.html", "key": "kubernetes/tracing"}}, "architecture": {"sections": {"architecture": "\nArchitecture\u00b6\nThe current set of components used in Seldon Core V2 is shown below:\n\nThe core components are:\n\nScheduler : manages the load and unload of models, pipelines, explainers and experiments.\nPipeline gateway : handles REST/gRPC calls to pipelines.\nDataflow engine : handles the flow of data between components in a pipeline.\nModel gateway : handles the flow of data from models to inference requests on servers and passes on the responses.\nAgent : manages the loading and unloading of models on a server and access to the server over REST/gRPC.\nEnvoy : manages the proxying of requests to the correct servers including load balancing.\n\nAll the above are Kubernetes agnostic and can run locally, e.g. on Docker Compose.\nWe also provide a Kubernetes Operator to allow Kubernetes usage.\nKafka is used as the backbone for Pipelines allowing a decentralized, synchronous and asynchronous usage.\n\nKafka\u00b6\nKafka is used as the backbone for allowing Pipelines of Models to be connected together into arbitrary directed acyclic graphs. Models can be reused in different Pipelines. The flow of data between models is handled by the dataflow engine using KStreams.\n\n\n\nDataflow Architecture\u00b6\nSeldon V2 follows a dataflow design paradigm and it\u2019s part of the current movement for data centric machine learning. By taking a decentralized route that focuses on the flow of data users can have more flexibility and insight in building complex applications containing machine learning and traditional components. This contrasts with a more centralized orchestration more traditional in service orientated architectures.\n\nBy focusing on the data we allow users to join various flows together using stream joining concepts as shown below.\n\nWe support several types of joins:\n\ninner joins, where all inputs need to be present for a transaction to join the tensors passed through the Pipeline;\nouter joins, where only a subset needs to be available during the join window\ntriggers, in which data flows need to wait until records on one or more trigger data flows appear. The data in these triggers is not passed onwards from the join.\n\nThese techniques allow users to create complex pipeline flows of data between machine learning components.\nMore discussion on the data flow view of machine learning and its effect on v2 design can be found here.\n\n\n\n", "kafka": "\nKafka\u00b6\nKafka is used as the backbone for allowing Pipelines of Models to be connected together into arbitrary directed acyclic graphs. Models can be reused in different Pipelines. The flow of data between models is handled by the dataflow engine using KStreams.\n\n", "dataflow-architecture": "\nDataflow Architecture\u00b6\nSeldon V2 follows a dataflow design paradigm and it\u2019s part of the current movement for data centric machine learning. By taking a decentralized route that focuses on the flow of data users can have more flexibility and insight in building complex applications containing machine learning and traditional components. This contrasts with a more centralized orchestration more traditional in service orientated architectures.\n\nBy focusing on the data we allow users to join various flows together using stream joining concepts as shown below.\n\nWe support several types of joins:\n\ninner joins, where all inputs need to be present for a transaction to join the tensors passed through the Pipeline;\nouter joins, where only a subset needs to be available during the join window\ntriggers, in which data flows need to wait until records on one or more trigger data flows appear. The data in these triggers is not passed onwards from the join.\n\nThese techniques allow users to create complex pipeline flows of data between machine learning components.\nMore discussion on the data flow view of machine learning and its effect on v2 design can be found here.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/architecture/index.html", "key": "architecture"}}, "examples/speech-to-sentiment": {"sections": {"huggingface-speech-to-sentiment-pipeline": "\nHuggingface speech to sentiment pipeline\u00b6\nRun these examples from the samples/examples/huggingface folder.\n\nHuggingface Speech to Sentiment Pipeline Example\u00b6\nIn this example we create a Pipeline to chain two huggingface models to allow speech to sentiment functionalityand add an explainer to understand the result.\nThis example also illustrates how explainers can target pipelines to allow complex explanations flows.\n\nThis example requires ffmpeg package to be installed locally. run make install-requirements for the Python dependencies.\nfrom ipywebrtc import AudioRecorder, CameraStream\nimport torchaudio\nfrom IPython.display import Audio\nimport base64\nimport json\nimport requests\nimport os\nimport time\n\n\nCreate a method to load speech from recorder; transform into mp3 and send at base64 data. On return of the result extract and show the text and sentiment.\nreqJson = json.loads('{\"inputs\":[{\"name\":\"args\", \"parameters\": {\"content_type\": \"base64\"}, \"data\":[],\"datatype\":\"BYTES\",\"shape\":[1]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndef infer(resource: str):\n    with open('recording.webm', 'wb') as f:\n        f.write(recorder.audio.value)\n    !ffmpeg -i recording.webm -vn -ab 128k -ar 44100 file.mp3 -y -hide_banner -loglevel panic\n    with open(\"file.mp3\", mode='rb') as file:\n        fileContent = file.read()\n        encoded = base64.b64encode(fileContent)\n        base64_message = encoded.decode('utf-8')\n    reqJson[\"inputs\"][0][\"data\"] = [str(base64_message)]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\": resource}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    j = response_raw.json()\n    sentiment = j[\"outputs\"][0][\"data\"][0]\n    text = j[\"outputs\"][1][\"data\"][0]\n    reqId = response_raw.headers[\"x-request-id\"]\n    print(reqId)\n    os.environ[\"REQUEST_ID\"]=reqId\n    print(base64.b64decode(text))\n    print(base64.b64decode(sentiment))\n\n\n\nLoad Huggingface Models\u00b6\nWe will load two Huggingface models for speech to text and text to sentiment.\ncat ../../models/hf-whisper.yaml\necho \"---\"\ncat ../../models/hf-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: whisper\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/whisper\"\n  requirements:\n  - huggingface\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/sentiment\"\n  requirements:\n  - huggingface\n\n\nseldon model load -f ../../models/hf-whisper.yaml\nseldon model load -f ../../models/hf-sentiment.yaml\n\n\n{}\n{}\n\n\nseldon model status whisper -w ModelAvailable | jq -M .\nseldon model status sentiment -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\n\n\nCreate Explain Pipeline\u00b6\nTo allow Alibi-Explain to more easily explain the sentiment we will need:\n\ninput and output transfrorms that take the Dict values input and output by the Huggingface sentiment model and turn them into values that Alibi-Explain can easily understand with the core values we want to explain and the outputs from the sentiment model.\nA separate Pipeline to allow us to join the sentiment model with the output transform\n\nThese transform models are MLServer custom runtimes as shown below:\ncat ./sentiment-input-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.logging import logger\nimport json\n\n\nclass SentimentInputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (input-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (input-transform): %s\",res_list)\n    texts = []\n    for res in res_list:\n      logger.info(\"decoded data (input-transform): %s\", res)\n      #text = json.loads(res)\n      text = res\n      texts.append(text[\"text\"])\n\n    logger.info(\"transformed data (input-transform): %s\", texts)\n    response =  StringRequestCodec.encode_response(\n      model_name=\"sentiment\",\n      payload=texts\n    )\n    logger.info(\"response (input-transform): %s\", response)\n    return response\n\n\n\ncat ./sentiment-output-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs import StringCodec, Base64Codec, NumpyRequestCodec\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.codecs.numpy import NumpyRequestCodec\nimport base64\nfrom mlserver.logging import logger\nimport numpy as np\nimport json\n\nclass SentimentOutputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (output-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (output-transform): %s\",res_list)\n    scores = []\n    for res in res_list:\n      logger.debug(\"decoded data (output transform): %s\",res)\n      #sentiment = json.loads(res)\n      sentiment = res\n      if sentiment[\"label\"] == \"POSITIVE\":\n        scores.append(1)\n      else:\n        scores.append(0)\n    response =  NumpyRequestCodec.encode_response(\n      model_name=\"sentiments\",\n      payload=np.array(scores)\n    )\n    logger.info(\"response (output-transform): %s\", response)\n    return response\n\n\n\ncat ../../models/hf-sentiment-input-transform.yaml\necho \"---\"\ncat ../../models/hf-sentiment-output-transform.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-input-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-input-transform\"\n  requirements:\n  - mlserver\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-output-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-output-transform\"\n  requirements:\n  - mlserver\n  - python\n\n\nseldon model load -f ../../models/hf-sentiment-input-transform.yaml\nseldon model load -f ../../models/hf-sentiment-output-transform.yaml\n\n\n{}\n{}\n\n\nseldon model status sentiment-input-transform -w ModelAvailable | jq -M .\nseldon model status sentiment-output-transform -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ../../pipelines/sentiment-explain.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: sentiment-explain\nspec:\n  steps:\n    - name: sentiment\n      tensorMap:\n        sentiment-explain.inputs.predict: array_inputs\n    - name: sentiment-output-transform\n      inputs:\n      - sentiment\n  output:\n    steps:\n    - sentiment-output-transform\n\n\nseldon pipeline load -f ../../pipelines/sentiment-explain.yaml\n\n\nseldon pipeline status sentiment-explain -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"sentiment-explain\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"sentiment-explain\",\n        \"uid\": \"cihuo3svgtec73bj6ncg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"tensorMap\": {\n              \"sentiment-explain.inputs.predict\": \"array_inputs\"\n            }\n          },\n          {\n            \"name\": \"sentiment-output-transform\",\n            \"inputs\": [\n              \"sentiment.outputs\"\n            ]\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment-output-transform.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:53:19.250753906Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ncat ../../models/hf-sentiment-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/speech-sentiment/explainer\"\n  explainer:\n    type: anchor_text\n    pipelineRef: sentiment-explain\n\n\nseldon model load -f ../../models/hf-sentiment-explainer.yaml\n\n\n{}\n\n\nseldon model status sentiment-explainer -w ModelAvailable | jq -M .\n\n\nError: Model wait status timeout\n\n\n\n\n\nSpeech to Sentiment Pipeline with Explanation\u00b6\nWe can now create the final pipeline that will take speech and generate sentiment alongwith an explanation of why that sentiment was predicted.\ncat ../../pipelines/speech-to-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: speech-to-sentiment\nspec:\n  steps:\n    - name: whisper\n    - name: sentiment\n      inputs:\n      - whisper\n      tensorMap:\n        whisper.outputs.output: args\n    - name: sentiment-input-transform\n      inputs:\n      - whisper\n    - name: sentiment-explainer\n      inputs:\n      - sentiment-input-transform\n  output:\n    steps:\n    - sentiment\n    - whisper\n\n\nseldon pipeline load -f ../../pipelines/speech-to-sentiment.yaml\n\n\nseldon pipeline status speech-to-sentiment -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"speech-to-sentiment\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"speech-to-sentiment\",\n        \"uid\": \"cihuqb4vgtec73bj6nd0\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ],\n            \"tensorMap\": {\n              \"whisper.outputs.output\": \"args\"\n            }\n          },\n          {\n            \"name\": \"sentiment-explainer\",\n            \"inputs\": [\n              \"sentiment-input-transform.outputs\"\n            ]\n          },\n          {\n            \"name\": \"sentiment-input-transform\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ]\n          },\n          {\n            \"name\": \"whisper\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment.outputs\",\n            \"whisper.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:58:04.277171896Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\n\n\nTest\u00b6\ncamera = CameraStream(constraints={'audio': True,'video':False})\nrecorder = AudioRecorder(stream=camera)\nrecorder\n\n\nAudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': \u2026\n\n\n\ninfer(\"speech-to-sentiment.pipeline\")\n\n\ncihuqm8fh5ss73der5gg\nb'{\"text\": \" Cambridge is a great place.\"}'\nb'{\"label\": \"POSITIVE\", \"score\": 0.9998548030853271}'\n\n\n\nWe will wait for the explanation which is run asynchronously to the functional output from the Pipeline above.\nwhile True:\n    base64Res = !seldon pipeline inspect speech-to-sentiment.sentiment-explainer.outputs --format json \\\n          --request-id ${REQUEST_ID}\n    j = json.loads(base64Res[0])\n    if j[\"topics\"][0][\"msgs\"] is not None:\n        expBase64 = j[\"topics\"][0][\"msgs\"][0][\"value\"][\"outputs\"][0][\"contents\"][\"bytesContents\"][0]\n        expRaw = base64.b64decode(expBase64)\n        exp = json.loads(expRaw)\n        print(\"\")\n        print(\"Explanation anchors:\",exp[\"data\"][\"anchor\"])\n        break\n    else:\n        print(\".\",end='')\n        time.sleep(1)\n\n\n\n......\nExplanation anchors: ['great']\n\n\n\n\n\nCleanup\u00b6\nseldon pipeline unload speech-to-sentiment\nseldon pipeline unload sentiment-explain\n\n\nseldon model unload whisper\nseldon model unload sentiment\nseldon model unload sentiment-explainer\nseldon model unload sentiment-output-transform\nseldon model unload sentiment-input-transform\n\n\n\n\n", "huggingface-speech-to-sentiment-pipeline-example": "\nHuggingface Speech to Sentiment Pipeline Example\u00b6\nIn this example we create a Pipeline to chain two huggingface models to allow speech to sentiment functionalityand add an explainer to understand the result.\nThis example also illustrates how explainers can target pipelines to allow complex explanations flows.\n\nThis example requires ffmpeg package to be installed locally. run make install-requirements for the Python dependencies.\nfrom ipywebrtc import AudioRecorder, CameraStream\nimport torchaudio\nfrom IPython.display import Audio\nimport base64\nimport json\nimport requests\nimport os\nimport time\n\n\nCreate a method to load speech from recorder; transform into mp3 and send at base64 data. On return of the result extract and show the text and sentiment.\nreqJson = json.loads('{\"inputs\":[{\"name\":\"args\", \"parameters\": {\"content_type\": \"base64\"}, \"data\":[],\"datatype\":\"BYTES\",\"shape\":[1]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndef infer(resource: str):\n    with open('recording.webm', 'wb') as f:\n        f.write(recorder.audio.value)\n    !ffmpeg -i recording.webm -vn -ab 128k -ar 44100 file.mp3 -y -hide_banner -loglevel panic\n    with open(\"file.mp3\", mode='rb') as file:\n        fileContent = file.read()\n        encoded = base64.b64encode(fileContent)\n        base64_message = encoded.decode('utf-8')\n    reqJson[\"inputs\"][0][\"data\"] = [str(base64_message)]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\": resource}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    j = response_raw.json()\n    sentiment = j[\"outputs\"][0][\"data\"][0]\n    text = j[\"outputs\"][1][\"data\"][0]\n    reqId = response_raw.headers[\"x-request-id\"]\n    print(reqId)\n    os.environ[\"REQUEST_ID\"]=reqId\n    print(base64.b64decode(text))\n    print(base64.b64decode(sentiment))\n\n\n\nLoad Huggingface Models\u00b6\nWe will load two Huggingface models for speech to text and text to sentiment.\ncat ../../models/hf-whisper.yaml\necho \"---\"\ncat ../../models/hf-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: whisper\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/whisper\"\n  requirements:\n  - huggingface\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/sentiment\"\n  requirements:\n  - huggingface\n\n\nseldon model load -f ../../models/hf-whisper.yaml\nseldon model load -f ../../models/hf-sentiment.yaml\n\n\n{}\n{}\n\n\nseldon model status whisper -w ModelAvailable | jq -M .\nseldon model status sentiment -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\n\n\nCreate Explain Pipeline\u00b6\nTo allow Alibi-Explain to more easily explain the sentiment we will need:\n\ninput and output transfrorms that take the Dict values input and output by the Huggingface sentiment model and turn them into values that Alibi-Explain can easily understand with the core values we want to explain and the outputs from the sentiment model.\nA separate Pipeline to allow us to join the sentiment model with the output transform\n\nThese transform models are MLServer custom runtimes as shown below:\ncat ./sentiment-input-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.logging import logger\nimport json\n\n\nclass SentimentInputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (input-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (input-transform): %s\",res_list)\n    texts = []\n    for res in res_list:\n      logger.info(\"decoded data (input-transform): %s\", res)\n      #text = json.loads(res)\n      text = res\n      texts.append(text[\"text\"])\n\n    logger.info(\"transformed data (input-transform): %s\", texts)\n    response =  StringRequestCodec.encode_response(\n      model_name=\"sentiment\",\n      payload=texts\n    )\n    logger.info(\"response (input-transform): %s\", response)\n    return response\n\n\n\ncat ./sentiment-output-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs import StringCodec, Base64Codec, NumpyRequestCodec\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.codecs.numpy import NumpyRequestCodec\nimport base64\nfrom mlserver.logging import logger\nimport numpy as np\nimport json\n\nclass SentimentOutputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (output-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (output-transform): %s\",res_list)\n    scores = []\n    for res in res_list:\n      logger.debug(\"decoded data (output transform): %s\",res)\n      #sentiment = json.loads(res)\n      sentiment = res\n      if sentiment[\"label\"] == \"POSITIVE\":\n        scores.append(1)\n      else:\n        scores.append(0)\n    response =  NumpyRequestCodec.encode_response(\n      model_name=\"sentiments\",\n      payload=np.array(scores)\n    )\n    logger.info(\"response (output-transform): %s\", response)\n    return response\n\n\n\ncat ../../models/hf-sentiment-input-transform.yaml\necho \"---\"\ncat ../../models/hf-sentiment-output-transform.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-input-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-input-transform\"\n  requirements:\n  - mlserver\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-output-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-output-transform\"\n  requirements:\n  - mlserver\n  - python\n\n\nseldon model load -f ../../models/hf-sentiment-input-transform.yaml\nseldon model load -f ../../models/hf-sentiment-output-transform.yaml\n\n\n{}\n{}\n\n\nseldon model status sentiment-input-transform -w ModelAvailable | jq -M .\nseldon model status sentiment-output-transform -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ../../pipelines/sentiment-explain.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: sentiment-explain\nspec:\n  steps:\n    - name: sentiment\n      tensorMap:\n        sentiment-explain.inputs.predict: array_inputs\n    - name: sentiment-output-transform\n      inputs:\n      - sentiment\n  output:\n    steps:\n    - sentiment-output-transform\n\n\nseldon pipeline load -f ../../pipelines/sentiment-explain.yaml\n\n\nseldon pipeline status sentiment-explain -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"sentiment-explain\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"sentiment-explain\",\n        \"uid\": \"cihuo3svgtec73bj6ncg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"tensorMap\": {\n              \"sentiment-explain.inputs.predict\": \"array_inputs\"\n            }\n          },\n          {\n            \"name\": \"sentiment-output-transform\",\n            \"inputs\": [\n              \"sentiment.outputs\"\n            ]\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment-output-transform.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:53:19.250753906Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ncat ../../models/hf-sentiment-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/speech-sentiment/explainer\"\n  explainer:\n    type: anchor_text\n    pipelineRef: sentiment-explain\n\n\nseldon model load -f ../../models/hf-sentiment-explainer.yaml\n\n\n{}\n\n\nseldon model status sentiment-explainer -w ModelAvailable | jq -M .\n\n\nError: Model wait status timeout\n\n\n\n\n\nSpeech to Sentiment Pipeline with Explanation\u00b6\nWe can now create the final pipeline that will take speech and generate sentiment alongwith an explanation of why that sentiment was predicted.\ncat ../../pipelines/speech-to-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: speech-to-sentiment\nspec:\n  steps:\n    - name: whisper\n    - name: sentiment\n      inputs:\n      - whisper\n      tensorMap:\n        whisper.outputs.output: args\n    - name: sentiment-input-transform\n      inputs:\n      - whisper\n    - name: sentiment-explainer\n      inputs:\n      - sentiment-input-transform\n  output:\n    steps:\n    - sentiment\n    - whisper\n\n\nseldon pipeline load -f ../../pipelines/speech-to-sentiment.yaml\n\n\nseldon pipeline status speech-to-sentiment -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"speech-to-sentiment\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"speech-to-sentiment\",\n        \"uid\": \"cihuqb4vgtec73bj6nd0\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ],\n            \"tensorMap\": {\n              \"whisper.outputs.output\": \"args\"\n            }\n          },\n          {\n            \"name\": \"sentiment-explainer\",\n            \"inputs\": [\n              \"sentiment-input-transform.outputs\"\n            ]\n          },\n          {\n            \"name\": \"sentiment-input-transform\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ]\n          },\n          {\n            \"name\": \"whisper\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment.outputs\",\n            \"whisper.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:58:04.277171896Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\n\n\nTest\u00b6\ncamera = CameraStream(constraints={'audio': True,'video':False})\nrecorder = AudioRecorder(stream=camera)\nrecorder\n\n\nAudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': \u2026\n\n\n\ninfer(\"speech-to-sentiment.pipeline\")\n\n\ncihuqm8fh5ss73der5gg\nb'{\"text\": \" Cambridge is a great place.\"}'\nb'{\"label\": \"POSITIVE\", \"score\": 0.9998548030853271}'\n\n\n\nWe will wait for the explanation which is run asynchronously to the functional output from the Pipeline above.\nwhile True:\n    base64Res = !seldon pipeline inspect speech-to-sentiment.sentiment-explainer.outputs --format json \\\n          --request-id ${REQUEST_ID}\n    j = json.loads(base64Res[0])\n    if j[\"topics\"][0][\"msgs\"] is not None:\n        expBase64 = j[\"topics\"][0][\"msgs\"][0][\"value\"][\"outputs\"][0][\"contents\"][\"bytesContents\"][0]\n        expRaw = base64.b64decode(expBase64)\n        exp = json.loads(expRaw)\n        print(\"\")\n        print(\"Explanation anchors:\",exp[\"data\"][\"anchor\"])\n        break\n    else:\n        print(\".\",end='')\n        time.sleep(1)\n\n\n\n......\nExplanation anchors: ['great']\n\n\n\n\n\nCleanup\u00b6\nseldon pipeline unload speech-to-sentiment\nseldon pipeline unload sentiment-explain\n\n\nseldon model unload whisper\nseldon model unload sentiment\nseldon model unload sentiment-explainer\nseldon model unload sentiment-output-transform\nseldon model unload sentiment-input-transform\n\n\n\n", "load-huggingface-models": "\nLoad Huggingface Models\u00b6\nWe will load two Huggingface models for speech to text and text to sentiment.\ncat ../../models/hf-whisper.yaml\necho \"---\"\ncat ../../models/hf-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: whisper\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/whisper\"\n  requirements:\n  - huggingface\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/sentiment\"\n  requirements:\n  - huggingface\n\n\nseldon model load -f ../../models/hf-whisper.yaml\nseldon model load -f ../../models/hf-sentiment.yaml\n\n\n{}\n{}\n\n\nseldon model status whisper -w ModelAvailable | jq -M .\nseldon model status sentiment -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\n", "create-explain-pipeline": "\nCreate Explain Pipeline\u00b6\nTo allow Alibi-Explain to more easily explain the sentiment we will need:\n\ninput and output transfrorms that take the Dict values input and output by the Huggingface sentiment model and turn them into values that Alibi-Explain can easily understand with the core values we want to explain and the outputs from the sentiment model.\nA separate Pipeline to allow us to join the sentiment model with the output transform\n\nThese transform models are MLServer custom runtimes as shown below:\ncat ./sentiment-input-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.logging import logger\nimport json\n\n\nclass SentimentInputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (input-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (input-transform): %s\",res_list)\n    texts = []\n    for res in res_list:\n      logger.info(\"decoded data (input-transform): %s\", res)\n      #text = json.loads(res)\n      text = res\n      texts.append(text[\"text\"])\n\n    logger.info(\"transformed data (input-transform): %s\", texts)\n    response =  StringRequestCodec.encode_response(\n      model_name=\"sentiment\",\n      payload=texts\n    )\n    logger.info(\"response (input-transform): %s\", response)\n    return response\n\n\n\ncat ./sentiment-output-transform/model.py | pygmentize\n\n\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\nfrom mlserver.codecs import StringCodec, Base64Codec, NumpyRequestCodec\nfrom mlserver.codecs.string import StringRequestCodec\nfrom mlserver.codecs.numpy import NumpyRequestCodec\nimport base64\nfrom mlserver.logging import logger\nimport numpy as np\nimport json\n\nclass SentimentOutputTransformRuntime(MLModel):\n\n  async def load(self) -> bool:\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    logger.info(\"payload (output-transform): %s\",payload)\n    res_list = self.decode_request(payload, default_codec=StringRequestCodec)\n    logger.info(\"res list (output-transform): %s\",res_list)\n    scores = []\n    for res in res_list:\n      logger.debug(\"decoded data (output transform): %s\",res)\n      #sentiment = json.loads(res)\n      sentiment = res\n      if sentiment[\"label\"] == \"POSITIVE\":\n        scores.append(1)\n      else:\n        scores.append(0)\n    response =  NumpyRequestCodec.encode_response(\n      model_name=\"sentiments\",\n      payload=np.array(scores)\n    )\n    logger.info(\"response (output-transform): %s\", response)\n    return response\n\n\n\ncat ../../models/hf-sentiment-input-transform.yaml\necho \"---\"\ncat ../../models/hf-sentiment-output-transform.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-input-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-input-transform\"\n  requirements:\n  - mlserver\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-output-transform\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/mlserver_1.3.5/sentiment-output-transform\"\n  requirements:\n  - mlserver\n  - python\n\n\nseldon model load -f ../../models/hf-sentiment-input-transform.yaml\nseldon model load -f ../../models/hf-sentiment-output-transform.yaml\n\n\n{}\n{}\n\n\nseldon model status sentiment-input-transform -w ModelAvailable | jq -M .\nseldon model status sentiment-output-transform -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncat ../../pipelines/sentiment-explain.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: sentiment-explain\nspec:\n  steps:\n    - name: sentiment\n      tensorMap:\n        sentiment-explain.inputs.predict: array_inputs\n    - name: sentiment-output-transform\n      inputs:\n      - sentiment\n  output:\n    steps:\n    - sentiment-output-transform\n\n\nseldon pipeline load -f ../../pipelines/sentiment-explain.yaml\n\n\nseldon pipeline status sentiment-explain -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"sentiment-explain\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"sentiment-explain\",\n        \"uid\": \"cihuo3svgtec73bj6ncg\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"tensorMap\": {\n              \"sentiment-explain.inputs.predict\": \"array_inputs\"\n            }\n          },\n          {\n            \"name\": \"sentiment-output-transform\",\n            \"inputs\": [\n              \"sentiment.outputs\"\n            ]\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment-output-transform.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:53:19.250753906Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\ncat ../../models/hf-sentiment-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/speech-sentiment/explainer\"\n  explainer:\n    type: anchor_text\n    pipelineRef: sentiment-explain\n\n\nseldon model load -f ../../models/hf-sentiment-explainer.yaml\n\n\n{}\n\n\nseldon model status sentiment-explainer -w ModelAvailable | jq -M .\n\n\nError: Model wait status timeout\n\n\n\n", "speech-to-sentiment-pipeline-with-explanation": "\nSpeech to Sentiment Pipeline with Explanation\u00b6\nWe can now create the final pipeline that will take speech and generate sentiment alongwith an explanation of why that sentiment was predicted.\ncat ../../pipelines/speech-to-sentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: speech-to-sentiment\nspec:\n  steps:\n    - name: whisper\n    - name: sentiment\n      inputs:\n      - whisper\n      tensorMap:\n        whisper.outputs.output: args\n    - name: sentiment-input-transform\n      inputs:\n      - whisper\n    - name: sentiment-explainer\n      inputs:\n      - sentiment-input-transform\n  output:\n    steps:\n    - sentiment\n    - whisper\n\n\nseldon pipeline load -f ../../pipelines/speech-to-sentiment.yaml\n\n\nseldon pipeline status speech-to-sentiment -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"speech-to-sentiment\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"speech-to-sentiment\",\n        \"uid\": \"cihuqb4vgtec73bj6nd0\",\n        \"version\": 2,\n        \"steps\": [\n          {\n            \"name\": \"sentiment\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ],\n            \"tensorMap\": {\n              \"whisper.outputs.output\": \"args\"\n            }\n          },\n          {\n            \"name\": \"sentiment-explainer\",\n            \"inputs\": [\n              \"sentiment-input-transform.outputs\"\n            ]\n          },\n          {\n            \"name\": \"sentiment-input-transform\",\n            \"inputs\": [\n              \"whisper.outputs\"\n            ]\n          },\n          {\n            \"name\": \"whisper\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"sentiment.outputs\",\n            \"whisper.outputs\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 2,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-07-04T09:58:04.277171896Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\n", "test": "\nTest\u00b6\ncamera = CameraStream(constraints={'audio': True,'video':False})\nrecorder = AudioRecorder(stream=camera)\nrecorder\n\n\nAudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': \u2026\n\n\n\ninfer(\"speech-to-sentiment.pipeline\")\n\n\ncihuqm8fh5ss73der5gg\nb'{\"text\": \" Cambridge is a great place.\"}'\nb'{\"label\": \"POSITIVE\", \"score\": 0.9998548030853271}'\n\n\n\nWe will wait for the explanation which is run asynchronously to the functional output from the Pipeline above.\nwhile True:\n    base64Res = !seldon pipeline inspect speech-to-sentiment.sentiment-explainer.outputs --format json \\\n          --request-id ${REQUEST_ID}\n    j = json.loads(base64Res[0])\n    if j[\"topics\"][0][\"msgs\"] is not None:\n        expBase64 = j[\"topics\"][0][\"msgs\"][0][\"value\"][\"outputs\"][0][\"contents\"][\"bytesContents\"][0]\n        expRaw = base64.b64decode(expBase64)\n        exp = json.loads(expRaw)\n        print(\"\")\n        print(\"Explanation anchors:\",exp[\"data\"][\"anchor\"])\n        break\n    else:\n        print(\".\",end='')\n        time.sleep(1)\n\n\n\n......\nExplanation anchors: ['great']\n\n\n\n", "cleanup": "\nCleanup\u00b6\nseldon pipeline unload speech-to-sentiment\nseldon pipeline unload sentiment-explain\n\n\nseldon model unload whisper\nseldon model unload sentiment\nseldon model unload sentiment-explainer\nseldon model unload sentiment-output-transform\nseldon model unload sentiment-input-transform\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/speech-to-sentiment.html", "key": "examples/speech-to-sentiment"}}, "examples/pandasquery": {"sections": {"conditional-pipeline-with-pandas-query-model": "\nConditional Pipeline with Pandas Query Model\u00b6\nThe model is defined as an MLServer custom runtime and allows the user to pass in a custom pandas query as a parameter defined at model creation to be used to filter the data passed to the model.\n# Copyright (c) 2024 Seldon Technologies Ltd.\n\n# Use of this software is governed BY\n# (1) the license included in the LICENSE file or\n# (2) if the license included in the LICENSE file is the Business Source License 1.1,\n# the Change License after the Change Date as each is defined in accordance with the LICENSE file.\n\nfrom mlserver import MLModel\nfrom mlserver.types import InferenceRequest, InferenceResponse\nfrom mlserver.codecs import PandasCodec\nfrom mlserver.errors import MLServerError\nimport pandas as pd\nfrom fastapi import status\nfrom mlserver.logging import logger\n\nQUERY_KEY = \"query\"\n\n\nclass ModelParametersMissing(MLServerError):\n  def __init__(self, model_name: str, reason: str):\n    super().__init__(\n      f\"Parameters missing for model {model_name} {reason}\", status.HTTP_400_BAD_REQUEST\n    )\n\nclass PandasQueryRuntime(MLModel):\n\n  async def load(self) -> bool:\n    logger.info(\"Loading with settings %s\", self.settings)\n    if self.settings.parameters is None or \\\n      self.settings.parameters.extra is None:\n      raise ModelParametersMissing(self.name, \"no settings.parameters.extra found\")\n    self.query = self.settings.parameters.extra[QUERY_KEY]\n    if self.query is None:\n      raise ModelParametersMissing(self.name, \"no settings.parameters.extra.query found\")\n    self.ready = True\n\n    return self.ready\n\n  async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n    input_df: pd.DataFrame = PandasCodec.decode_request(payload)\n    # run query on input_df and save in output_df\n    output_df = input_df.query(self.query)\n    if output_df.empty:\n      output_df = pd.DataFrame({'status':[\"no rows satisfied \" + self.query]})\n    else:\n      output_df[\"status\"] = \"row satisfied \" + self.query\n    return PandasCodec.encode_response(self.name, output_df, self.version)\n\n\n\nConditional Pipeline using PandasQuery\u00b6\ncat ../../models/choice1.yaml\necho \"---\"\ncat ../../models/choice2.yaml\necho \"---\"\ncat ../../models/add10.yaml\necho \"---\"\ncat ../../models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-one\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 1\"\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-two\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 2\"\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ../../models/choice1.yaml\nseldon model load -f ../../models/choice2.yaml\nseldon model load -f ../../models/add10.yaml\nseldon model load -f ../../models/mul10.yaml\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model status choice-is-one -w ModelAvailable\nseldon model status choice-is-two -w ModelAvailable\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n{}\n{}\n\n\ncat ../../pipelines/choice.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: choice\nspec:\n  steps:\n  - name: choice-is-one\n  - name: mul10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-one.outputs.choice\n  - name: choice-is-two\n  - name: add10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-two.outputs.choice\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ../../pipelines/choice.yaml\n\n\nseldon pipeline status choice -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"choice\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"choice\",\n        \"uid\": \"cifel9aufmbc73e5intg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"choice.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"choice-is-two.outputs.choice\"\n            ]\n          },\n          {\n            \"name\": \"choice-is-one\"\n          },\n          {\n            \"name\": \"choice-is-two\"\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"choice.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"choice-is-one.outputs.choice\"\n            ]\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:45:57.284684328Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer choice --inference-mode grpc \\\n '{\"model_name\":\"choice\",\"inputs\":[{\"name\":\"choice\",\"contents\":{\"int_contents\":[1]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[5,6,7,8]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          50,\n          60,\n          70,\n          80\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer choice --inference-mode grpc \\\n '{\"model_name\":\"choice\",\"inputs\":[{\"name\":\"choice\",\"contents\":{\"int_contents\":[2]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[5,6,7,8]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          15,\n          16,\n          17,\n          18\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon model unload choice-is-one\nseldon model unload choice-is-two\nseldon model unload add10\nseldon model unload mul10\nseldon pipeline unload choice\n\n\n\n\n\n\n", "conditional-pipeline-using-pandasquery": "\nConditional Pipeline using PandasQuery\u00b6\ncat ../../models/choice1.yaml\necho \"---\"\ncat ../../models/choice2.yaml\necho \"---\"\ncat ../../models/add10.yaml\necho \"---\"\ncat ../../models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-one\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 1\"\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: choice-is-two\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/pandasquery\"\n  requirements:\n  - mlserver\n  - python\n  parameters:\n  - name: query\n    value: \"choice == 2\"\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ../../models/choice1.yaml\nseldon model load -f ../../models/choice2.yaml\nseldon model load -f ../../models/add10.yaml\nseldon model load -f ../../models/mul10.yaml\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model status choice-is-one -w ModelAvailable\nseldon model status choice-is-two -w ModelAvailable\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n{}\n{}\n\n\ncat ../../pipelines/choice.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: choice\nspec:\n  steps:\n  - name: choice-is-one\n  - name: mul10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-one.outputs.choice\n  - name: choice-is-two\n  - name: add10\n    inputs:\n    - choice.inputs.INPUT\n    triggers:\n    - choice-is-two.outputs.choice\n  output:\n    steps:\n    - mul10\n    - add10\n    stepsJoin: any\n\n\nseldon pipeline load -f ../../pipelines/choice.yaml\n\n\nseldon pipeline status choice -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"choice\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"choice\",\n        \"uid\": \"cifel9aufmbc73e5intg\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"add10\",\n            \"inputs\": [\n              \"choice.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"choice-is-two.outputs.choice\"\n            ]\n          },\n          {\n            \"name\": \"choice-is-one\"\n          },\n          {\n            \"name\": \"choice-is-two\"\n          },\n          {\n            \"name\": \"mul10\",\n            \"inputs\": [\n              \"choice.inputs.INPUT\"\n            ],\n            \"triggers\": [\n              \"choice-is-one.outputs.choice\"\n            ]\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"mul10.outputs\",\n            \"add10.outputs\"\n          ],\n          \"stepsJoin\": \"ANY\"\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:45:57.284684328Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer choice --inference-mode grpc \\\n '{\"model_name\":\"choice\",\"inputs\":[{\"name\":\"choice\",\"contents\":{\"int_contents\":[1]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[5,6,7,8]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          50,\n          60,\n          70,\n          80\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer choice --inference-mode grpc \\\n '{\"model_name\":\"choice\",\"inputs\":[{\"name\":\"choice\",\"contents\":{\"int_contents\":[2]},\"datatype\":\"INT32\",\"shape\":[1]},{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[5,6,7,8]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          15,\n          16,\n          17,\n          18\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon model unload choice-is-one\nseldon model unload choice-is-two\nseldon model unload add10\nseldon model unload mul10\nseldon pipeline unload choice\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/pandasquery.html", "key": "examples/pandasquery"}}, "examples/k8s-examples": {"sections": {"kubernetes-examples": "\nKubernetes Examples\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Kubernetes Examples\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.18.255.2'\n\n\n\n\nModel\u00b6\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nkubectl create -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n\nkubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"7fd401e1-3dce-46f5-9668-902aea652b89\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl get server mlserver -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"reason\": \"StatefulSet replicas matches desired replicas\",\n      \"status\": \"True\",\n      \"type\": \"StatefulSetReady\"\n    }\n  ],\n  \"loadedModels\": 1,\n  \"replicas\": 1,\n  \"selector\": \"seldon-server-name=mlserver\"\n}\n\n\nkubectl delete -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\n\nExperiment\u00b6\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nkubectl create -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\nmodel.mlops.seldon.io/iris2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nkubectl create -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample created\n\n\n\nkubectl wait --for condition=ready --timeout=300s experiment --all -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample condition met\n\n\n\nseldon model infer --inference-host ${MESH_IP}:80 -i 50 iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::29 :iris_1::21]\n\n\n\nkubectl delete -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io \"experiment-sample\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n\n\n\nPipeline - model chain\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\n\n\n\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nseldon pipeline infer tfsimples --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\n\n\n\n\n\nPipeline - model join\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\nmodel.mlops.seldon.io/tfsimple3 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\nmodel.mlops.seldon.io/tfsimple3 condition met\n\n\n\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nkubectl create -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join condition met\n\n\n\nseldon pipeline infer join --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"join\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\nmodel.mlops.seldon.io \"tfsimple3\" deleted\n\n\n\n\n\n\nExplainer\u00b6\ncat ./models/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n\n\nkubectl create -f ./models/income.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income condition met\n\n\n\nkubectl get model income -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:02:53Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:02:53Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer income --inference-host ${MESH_IP}:80 \\\n     '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"f52acfeb-0f22-429f-8c7a-785ef17cd470\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nkubectl create -f ./models/income-explainer.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income-explainer created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income condition met\nmodel.mlops.seldon.io/income-explainer condition met\n\n\n\nkubectl get model income-explainer -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:03:07Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:03:07Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer income-explainer --inference-host ${MESH_IP}:80 \\\n     '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income-explainer_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"3028a904-9bb3-42d7-bdb7-6e6993323ed7\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"explanation\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"str\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"meta\\\": {\\\"name\\\": \\\"AnchorTabular\\\", \\\"type\\\": [\\\"blackbox\\\"], \\\"explanations\\\": [\\\"local\\\"], \\\"params\\\": {\\\"seed\\\": 1, \\\"disc_perc\\\": [25, 50, 75], \\\"threshold\\\": 0.95, \\\"delta\\\": 0.1, \\\"tau\\\": 0.15, \\\"batch_size\\\": 100, \\\"coverage_samples\\\": 10000, \\\"beam_size\\\": 1, \\\"stop_on_first\\\": false, \\\"max_anchor_size\\\": null, \\\"min_samples_start\\\": 100, \\\"n_covered_ex\\\": 10, \\\"binary_cache_size\\\": 10000, \\\"cache_margin\\\": 1000, \\\"verbose\\\": false, \\\"verbose_every\\\": 1, \\\"kwargs\\\": {}}, \\\"version\\\": \\\"0.9.1\\\"}, \\\"data\\\": {\\\"anchor\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"precision\\\": 0.9705882352941176, \\\"coverage\\\": 0.0699, \\\"raw\\\": {\\\"feature\\\": [3, 5], \\\"mean\\\": [0.8094218415417559, 0.9705882352941176], \\\"precision\\\": [0.8094218415417559, 0.9705882352941176], \\\"coverage\\\": [0.3036, 0.0699], \\\"examples\\\": [{\\\"covered_true\\\": [[23, 4, 1, 1, 5, 1, 4, 0, 0, 0, 40, 9], [44, 4, 1, 1, 8, 0, 4, 1, 0, 0, 40, 9], [60, 2, 5, 1, 5, 1, 4, 0, 0, 0, 25, 9], [52, 4, 1, 1, 2, 0, 4, 1, 0, 0, 50, 9], [66, 6, 1, 1, 8, 0, 4, 1, 0, 0, 8, 9], [52, 4, 1, 1, 8, 0, 4, 1, 0, 0, 40, 9], [27, 4, 1, 1, 1, 1, 4, 1, 0, 0, 35, 9], [48, 4, 1, 1, 6, 0, 4, 1, 0, 0, 45, 9], [45, 6, 1, 1, 5, 0, 4, 1, 0, 0, 40, 9], [40, 2, 1, 1, 5, 4, 4, 0, 0, 0, 45, 9]], \\\"covered_false\\\": [[42, 6, 5, 1, 6, 0, 4, 1, 99999, 0, 80, 9], [29, 4, 1, 1, 8, 1, 4, 1, 0, 0, 50, 9], [49, 4, 1, 1, 8, 0, 4, 1, 0, 0, 50, 9], [34, 4, 5, 1, 8, 0, 4, 1, 0, 0, 40, 9], [38, 2, 1, 1, 5, 5, 4, 0, 7688, 0, 40, 9], [45, 7, 5, 1, 5, 0, 4, 1, 0, 0, 45, 9], [43, 4, 2, 1, 5, 0, 4, 1, 99999, 0, 55, 9], [47, 4, 5, 1, 6, 1, 4, 1, 27828, 0, 60, 9], [42, 6, 1, 1, 2, 0, 4, 1, 15024, 0, 60, 9], [56, 4, 1, 1, 6, 0, 2, 1, 7688, 0, 45, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}, {\\\"covered_true\\\": [[23, 4, 1, 1, 4, 3, 4, 1, 0, 0, 40, 9], [50, 2, 5, 1, 8, 3, 2, 1, 0, 0, 45, 9], [24, 4, 1, 1, 7, 3, 4, 0, 0, 0, 40, 3], [62, 4, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [22, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9], [44, 4, 1, 1, 1, 3, 4, 0, 0, 0, 40, 9], [46, 4, 1, 1, 4, 3, 4, 1, 0, 0, 40, 9], [44, 4, 1, 1, 2, 3, 4, 1, 0, 0, 40, 9], [25, 4, 5, 1, 5, 3, 4, 1, 0, 0, 35, 9], [32, 2, 5, 1, 5, 3, 4, 1, 0, 0, 50, 9]], \\\"covered_false\\\": [[57, 5, 5, 1, 6, 3, 4, 1, 99999, 0, 40, 9], [44, 4, 1, 1, 8, 3, 4, 1, 7688, 0, 60, 9], [43, 2, 5, 1, 4, 3, 2, 0, 8614, 0, 47, 9], [56, 5, 2, 1, 5, 3, 4, 1, 99999, 0, 70, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}], \\\"all_precision\\\": 0, \\\"num_preds\\\": 1000000, \\\"success\\\": true, \\\"names\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"prediction\\\": [0], \\\"instance\\\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \\\"instances\\\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nkubectl delete -f ./models/income.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/income-explainer.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"income\" deleted\nmodel.mlops.seldon.io \"income-explainer\" deleted\n\n\n\n\n\n\n\n", "seldon-v2-kubernetes-examples": "\nSeldon V2 Kubernetes Examples\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.18.255.2'\n\n\n\n\nModel\u00b6\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nkubectl create -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n\nkubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"7fd401e1-3dce-46f5-9668-902aea652b89\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl get server mlserver -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"reason\": \"StatefulSet replicas matches desired replicas\",\n      \"status\": \"True\",\n      \"type\": \"StatefulSetReady\"\n    }\n  ],\n  \"loadedModels\": 1,\n  \"replicas\": 1,\n  \"selector\": \"seldon-server-name=mlserver\"\n}\n\n\nkubectl delete -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\n\nExperiment\u00b6\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nkubectl create -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\nmodel.mlops.seldon.io/iris2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nkubectl create -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample created\n\n\n\nkubectl wait --for condition=ready --timeout=300s experiment --all -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample condition met\n\n\n\nseldon model infer --inference-host ${MESH_IP}:80 -i 50 iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::29 :iris_1::21]\n\n\n\nkubectl delete -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io \"experiment-sample\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n\n\n\nPipeline - model chain\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\n\n\n\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nseldon pipeline infer tfsimples --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\n\n\n\n\n\nPipeline - model join\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\nmodel.mlops.seldon.io/tfsimple3 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\nmodel.mlops.seldon.io/tfsimple3 condition met\n\n\n\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nkubectl create -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join condition met\n\n\n\nseldon pipeline infer join --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"join\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\nmodel.mlops.seldon.io \"tfsimple3\" deleted\n\n\n\n\n", "model": "\nModel\u00b6\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nkubectl create -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n\nkubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:01:52Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"7fd401e1-3dce-46f5-9668-902aea652b89\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"parameters\": {\n        \"content_type\": {\n          \"stringParam\": \"np\"\n        }\n      },\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl get server mlserver -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T09:59:12Z\",\n      \"reason\": \"StatefulSet replicas matches desired replicas\",\n      \"status\": \"True\",\n      \"type\": \"StatefulSetReady\"\n    }\n  ],\n  \"loadedModels\": 1,\n  \"replicas\": 1,\n  \"selector\": \"seldon-server-name=mlserver\"\n}\n\n\nkubectl delete -f ./models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n", "experiment": "\nExperiment\u00b6\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nkubectl create -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\nmodel.mlops.seldon.io/iris2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nkubectl create -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample created\n\n\n\nkubectl wait --for condition=ready --timeout=300s experiment --all -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io/experiment-sample condition met\n\n\n\nseldon model infer --inference-host ${MESH_IP}:80 -i 50 iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::29 :iris_1::21]\n\n\n\nkubectl delete -f ./experiments/ab-default-model.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/sklearn2.yaml -n ${NAMESPACE}\n\n\nexperiment.mlops.seldon.io \"experiment-sample\" deleted\nmodel.mlops.seldon.io \"iris\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\n\n\n\n", "pipeline-model-chain": "\nPipeline - model chain\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\n\n\n\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nseldon pipeline infer tfsimples --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\n\n\n\n", "pipeline-model-join": "\nPipeline - model join\u00b6\ncat ./models/tfsimple1.yaml\ncat ./models/tfsimple2.yaml\ncat ./models/tfsimple3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple1\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple2\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple3\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\nmodel.mlops.seldon.io/tfsimple3 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 condition met\nmodel.mlops.seldon.io/tfsimple2 condition met\nmodel.mlops.seldon.io/tfsimple3 condition met\n\n\n\ncat ./pipelines/tfsimples-join.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3\n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nkubectl create -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/join condition met\n\n\n\nseldon pipeline infer join --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n    '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT0\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    },\n    {\n      \"name\": \"OUTPUT1\",\n      \"datatype\": \"INT32\",\n      \"shape\": [\n        \"1\",\n        \"16\"\n      ],\n      \"contents\": {\n        \"intContents\": [\n          2,\n          4,\n          6,\n          8,\n          10,\n          12,\n          14,\n          16,\n          18,\n          20,\n          22,\n          24,\n          26,\n          28,\n          30,\n          32\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./pipelines/tfsimples-join.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io \"join\" deleted\n\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple3.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\nmodel.mlops.seldon.io \"tfsimple3\" deleted\n\n\n\n", "explainer": "\nExplainer\u00b6\ncat ./models/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n\n\nkubectl create -f ./models/income.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income condition met\n\n\n\nkubectl get model income -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:02:53Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:02:53Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer income --inference-host ${MESH_IP}:80 \\\n     '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"f52acfeb-0f22-429f-8c7a-785ef17cd470\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nkubectl create -f ./models/income-explainer.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income-explainer created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/income condition met\nmodel.mlops.seldon.io/income-explainer condition met\n\n\n\nkubectl get model income-explainer -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:03:07Z\",\n      \"message\": \"ModelAvailable\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2023-06-30T10:03:07Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\nseldon model infer income-explainer --inference-host ${MESH_IP}:80 \\\n     '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income-explainer_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"3028a904-9bb3-42d7-bdb7-6e6993323ed7\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"explanation\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"str\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"meta\\\": {\\\"name\\\": \\\"AnchorTabular\\\", \\\"type\\\": [\\\"blackbox\\\"], \\\"explanations\\\": [\\\"local\\\"], \\\"params\\\": {\\\"seed\\\": 1, \\\"disc_perc\\\": [25, 50, 75], \\\"threshold\\\": 0.95, \\\"delta\\\": 0.1, \\\"tau\\\": 0.15, \\\"batch_size\\\": 100, \\\"coverage_samples\\\": 10000, \\\"beam_size\\\": 1, \\\"stop_on_first\\\": false, \\\"max_anchor_size\\\": null, \\\"min_samples_start\\\": 100, \\\"n_covered_ex\\\": 10, \\\"binary_cache_size\\\": 10000, \\\"cache_margin\\\": 1000, \\\"verbose\\\": false, \\\"verbose_every\\\": 1, \\\"kwargs\\\": {}}, \\\"version\\\": \\\"0.9.1\\\"}, \\\"data\\\": {\\\"anchor\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"precision\\\": 0.9705882352941176, \\\"coverage\\\": 0.0699, \\\"raw\\\": {\\\"feature\\\": [3, 5], \\\"mean\\\": [0.8094218415417559, 0.9705882352941176], \\\"precision\\\": [0.8094218415417559, 0.9705882352941176], \\\"coverage\\\": [0.3036, 0.0699], \\\"examples\\\": [{\\\"covered_true\\\": [[23, 4, 1, 1, 5, 1, 4, 0, 0, 0, 40, 9], [44, 4, 1, 1, 8, 0, 4, 1, 0, 0, 40, 9], [60, 2, 5, 1, 5, 1, 4, 0, 0, 0, 25, 9], [52, 4, 1, 1, 2, 0, 4, 1, 0, 0, 50, 9], [66, 6, 1, 1, 8, 0, 4, 1, 0, 0, 8, 9], [52, 4, 1, 1, 8, 0, 4, 1, 0, 0, 40, 9], [27, 4, 1, 1, 1, 1, 4, 1, 0, 0, 35, 9], [48, 4, 1, 1, 6, 0, 4, 1, 0, 0, 45, 9], [45, 6, 1, 1, 5, 0, 4, 1, 0, 0, 40, 9], [40, 2, 1, 1, 5, 4, 4, 0, 0, 0, 45, 9]], \\\"covered_false\\\": [[42, 6, 5, 1, 6, 0, 4, 1, 99999, 0, 80, 9], [29, 4, 1, 1, 8, 1, 4, 1, 0, 0, 50, 9], [49, 4, 1, 1, 8, 0, 4, 1, 0, 0, 50, 9], [34, 4, 5, 1, 8, 0, 4, 1, 0, 0, 40, 9], [38, 2, 1, 1, 5, 5, 4, 0, 7688, 0, 40, 9], [45, 7, 5, 1, 5, 0, 4, 1, 0, 0, 45, 9], [43, 4, 2, 1, 5, 0, 4, 1, 99999, 0, 55, 9], [47, 4, 5, 1, 6, 1, 4, 1, 27828, 0, 60, 9], [42, 6, 1, 1, 2, 0, 4, 1, 15024, 0, 60, 9], [56, 4, 1, 1, 6, 0, 2, 1, 7688, 0, 45, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}, {\\\"covered_true\\\": [[23, 4, 1, 1, 4, 3, 4, 1, 0, 0, 40, 9], [50, 2, 5, 1, 8, 3, 2, 1, 0, 0, 45, 9], [24, 4, 1, 1, 7, 3, 4, 0, 0, 0, 40, 3], [62, 4, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [22, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9], [44, 4, 1, 1, 1, 3, 4, 0, 0, 0, 40, 9], [46, 4, 1, 1, 4, 3, 4, 1, 0, 0, 40, 9], [44, 4, 1, 1, 2, 3, 4, 1, 0, 0, 40, 9], [25, 4, 5, 1, 5, 3, 4, 1, 0, 0, 35, 9], [32, 2, 5, 1, 5, 3, 4, 1, 0, 0, 50, 9]], \\\"covered_false\\\": [[57, 5, 5, 1, 6, 3, 4, 1, 99999, 0, 40, 9], [44, 4, 1, 1, 8, 3, 4, 1, 7688, 0, 60, 9], [43, 2, 5, 1, 4, 3, 2, 0, 8614, 0, 47, 9], [56, 5, 2, 1, 5, 3, 4, 1, 99999, 0, 70, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}], \\\"all_precision\\\": 0, \\\"num_preds\\\": 1000000, \\\"success\\\": true, \\\"names\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"prediction\\\": [0], \\\"instance\\\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \\\"instances\\\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nkubectl delete -f ./models/income.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/income-explainer.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"income\" deleted\nmodel.mlops.seldon.io \"income-explainer\" deleted\n\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/k8s-examples.html", "key": "examples/k8s-examples"}}, "experiments": {"sections": {"experiments": "\nExperiments\u00b6\nAn Experiment defines an http traffic split between Models or Pipelines.\nExperiments also allow a mirror model or pipeline to be tested where some percentage of the traffic to the main model is sent to the mirror but the result is not returned.\nFurther details are given here.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/experiments/index.html", "key": "experiments"}}, "cli/docs/seldon_experiment_start": {"sections": {"seldon-experiment-start": "\nseldon experiment start\u00b6\nstart an experiment\n\nSynopsis\u00b6\nstart an experiment\nseldon experiment start [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        experiment manifest file (YAML)\n  -h, --help                    help for start\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n\n", "synopsis": "\nSynopsis\u00b6\nstart an experiment\nseldon experiment start [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        experiment manifest file (YAML)\n  -h, --help                    help for start\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon experiment\t - manage experiments\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_experiment_start.html", "key": "cli/docs/seldon_experiment_start"}}, "cli/docs/seldon_config_remove": {"sections": {"seldon-config-remove": "\nseldon config remove\u00b6\nremove config\n\nSynopsis\u00b6\nremove config\nseldon config remove [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for remove\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n\n", "synopsis": "\nSynopsis\u00b6\nremove config\nseldon config remove [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for remove\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_config_remove.html", "key": "cli/docs/seldon_config_remove"}}, "examples/huggingface": {"sections": {"huggingface-models": "\nHuggingface Models\u00b6\nRun these examples from the samples folder.\n\nHuggingface Examples\u00b6\n\nText Generation Model\u00b6\ncat ./models/hf-text-gen.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: text-gen\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/text-generation\"\n  requirements:\n  - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen.yaml\n\n\n{}\n\n\nseldon model status text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"121ff5f4-1d4a-46d0-9a5e-4cd3b11040df\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away, the planet is full of strange little creatures. A very strange combination of creatures in that universe, that is. A strange combination of creatures in that universe, that is. A kind of creature that is\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\n\\\\nThe Universe is a big and massive place. How can you feel any of this? Your body doesn\\'t make sense if the Universe is in full swing \\\\u2014 you don\\'t have to remember whether the\"}'\n\n\n\nUnload the model\nseldon model unload text-gen\n\n\n\n\nCustom Text Generation Model\u00b6\ncat ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: custom-tiny-stories-text-gen\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/huggingface-text-gen-custom-tiny-stories\"\n  requirements:\n    - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\n{}\n\n\nseldon model status custom-tiny-stories-text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer custom-tiny-stories-text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"custom-tiny-stories-text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"d0fce59c-76e2-4f81-9711-1c93d08bcbf9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away. It was a very special place to live.\\\\n\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer custom-tiny-stories-text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\nOne night, a little girl named Lily went to\"}'\n\n\n\nUnload the model\nseldon model unload custom-tiny-stories-text-gen\n\n\nAs a next step, why not try running a larger-scale model? You can find a definition for one in ./models/hf-text-gen-custom-gpt2.yaml. However, you may need to request and allocate more memory!\n\n\n\n\n", "huggingface-examples": "\nHuggingface Examples\u00b6\n\nText Generation Model\u00b6\ncat ./models/hf-text-gen.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: text-gen\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/text-generation\"\n  requirements:\n  - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen.yaml\n\n\n{}\n\n\nseldon model status text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"121ff5f4-1d4a-46d0-9a5e-4cd3b11040df\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away, the planet is full of strange little creatures. A very strange combination of creatures in that universe, that is. A strange combination of creatures in that universe, that is. A kind of creature that is\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\n\\\\nThe Universe is a big and massive place. How can you feel any of this? Your body doesn\\'t make sense if the Universe is in full swing \\\\u2014 you don\\'t have to remember whether the\"}'\n\n\n\nUnload the model\nseldon model unload text-gen\n\n\n\n\nCustom Text Generation Model\u00b6\ncat ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: custom-tiny-stories-text-gen\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/huggingface-text-gen-custom-tiny-stories\"\n  requirements:\n    - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\n{}\n\n\nseldon model status custom-tiny-stories-text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer custom-tiny-stories-text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"custom-tiny-stories-text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"d0fce59c-76e2-4f81-9711-1c93d08bcbf9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away. It was a very special place to live.\\\\n\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer custom-tiny-stories-text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\nOne night, a little girl named Lily went to\"}'\n\n\n\nUnload the model\nseldon model unload custom-tiny-stories-text-gen\n\n\nAs a next step, why not try running a larger-scale model? You can find a definition for one in ./models/hf-text-gen-custom-gpt2.yaml. However, you may need to request and allocate more memory!\n\n\n\n", "text-generation-model": "\nText Generation Model\u00b6\ncat ./models/hf-text-gen.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: text-gen\nspec:\n  storageUri: \"gs://seldon-models/mlserver/huggingface/text-generation\"\n  requirements:\n  - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen.yaml\n\n\n{}\n\n\nseldon model status text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"121ff5f4-1d4a-46d0-9a5e-4cd3b11040df\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away, the planet is full of strange little creatures. A very strange combination of creatures in that universe, that is. A strange combination of creatures in that universe, that is. A kind of creature that is\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\n\\\\nThe Universe is a big and massive place. How can you feel any of this? Your body doesn\\'t make sense if the Universe is in full swing \\\\u2014 you don\\'t have to remember whether the\"}'\n\n\n\nUnload the model\nseldon model unload text-gen\n\n\n", "custom-text-generation-model": "\nCustom Text Generation Model\u00b6\ncat ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: custom-tiny-stories-text-gen\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/huggingface-text-gen-custom-tiny-stories\"\n  requirements:\n    - huggingface\n\n\nLoad the model\nseldon model load -f ./models/hf-text-gen-custom-tiny-stories.yaml\n\n\n{}\n\n\nseldon model status custom-tiny-stories-text-gen -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer custom-tiny-stories-text-gen \\\n  '{\"inputs\": [{\"name\": \"args\",\"shape\": [1],\"datatype\": \"BYTES\",\"data\": [\"Once upon a time in a galaxy far away\"]}]}'\n\n\n{\n\t\"model_name\": \"custom-tiny-stories-text-gen_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"d0fce59c-76e2-4f81-9711-1c93d08bcbf9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"output\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"hg_jsonlist\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"generated_text\\\": \\\"Once upon a time in a galaxy far away. It was a very special place to live.\\\\n\\\"}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nres = !seldon model infer custom-tiny-stories-text-gen --inference-mode grpc \\\n   '{\"inputs\":[{\"name\":\"args\",\"contents\":{\"bytes_contents\":[\"T25jZSB1cG9uIGEgdGltZSBpbiBhIGdhbGF4eSBmYXIgYXdheQo=\"]},\"datatype\":\"BYTES\",\"shape\":[1]}]}'\n\n\nimport json\nimport base64\nr = json.loads(res[0])\nbase64.b64decode(r[\"outputs\"][0][\"contents\"][\"bytesContents\"][0])\n\n\nb'{\"generated_text\": \"Once upon a time in a galaxy far away\\\\nOne night, a little girl named Lily went to\"}'\n\n\n\nUnload the model\nseldon model unload custom-tiny-stories-text-gen\n\n\nAs a next step, why not try running a larger-scale model? You can find a definition for one in ./models/hf-text-gen-custom-gpt2.yaml. However, you may need to request and allocate more memory!\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/huggingface.html", "key": "examples/huggingface"}}, "explainers": {"sections": {"explainers": "\nExplainers\u00b6\nExplainers are Model resources with some extra settings. They allow a range of explainers from the Alibi-Explain library to be run on MLServer.\nAn example Anchors explainer definitions is shown below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/income-sklearn/anchor-explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nThe key additions are:\n\ntype : This must be one of the supported Alibi Explainer types supported by the Alibi Explain runtime in MLServer.\nmodelRef : The model name for black box explainers.\npipelineRef : The pipeline name for black box explainers.\n\nOnly one of modelRef and pipelineRef is allowed.\n\nPipeline Explanations\u00b6\nBlackbox explainers can explain a Pipeline as well as a model. An example from the Huggingface sentiment demo is show below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/speech-sentiment/explainer\"\n  explainer:\n    type: anchor_text\n    pipelineRef: sentiment-explain\n\n\n\n\nExamples\u00b6\n\nTabular income classification model with Anchor Tabular black box model explainer\nHuggingface Sentiment model with Anchor Text black box pipeline explainer\nAnchor Text movies sentiment explainer\n\n\n", "pipeline-explanations": "\nPipeline Explanations\u00b6\nBlackbox explainers can explain a Pipeline as well as a model. An example from the Huggingface sentiment demo is show below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/huggingface/speech-sentiment/explainer\"\n  explainer:\n    type: anchor_text\n    pipelineRef: sentiment-explain\n\n\n", "examples": "\nExamples\u00b6\n\nTabular income classification model with Anchor Tabular black box model explainer\nHuggingface Sentiment model with Anchor Text black box pipeline explainer\nAnchor Text movies sentiment explainer\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/explainers/index.html", "key": "explainers"}}, "examples/k8s-pvc": {"sections": {"kubernetes-server-with-pvc": "\nKubernetes Server with PVC\u00b6\n\nKubernetes PVC Example\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\nKind cluster setup\u00b6\nTo run this example in Kind we need to start Kind with access to a local folder where are models are location. In this example we will use a folder in /tmp and associate that with a path in the container.\n!cat kind-config.yaml\n\n\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n  extraMounts:\n    - hostPath: /tmp/models\n      containerPath: /models\n\n\nTo start a Kind cluster with these settings using our ansible script you can run from the project root folder\nansible-playbook ansible/playbooks/kind-cluster.yaml -e kind_config_file=${PWD}/samples/examples/local-pvc/kind-config.yaml\n\n\nNow you should finish the Seldon install following the docs.\nCreate the local folder we will use for our models and copy an example iris sklearn model to it.\n!mkdir -p /tmp/models\n!gsutil cp -r gs://seldon-models/mlserver/iris /tmp/models\n\n\n\n\nCreate Server with PVC\u00b6\nHere we create a storage class and associated persistent colume referencing the /models folder where our models are stored.\n!cat pvc.yaml\n\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-path-immediate\nprovisioner: rancher.io/local-path\nreclaimPolicy: Delete\nmountOptions:\n  - debug\nvolumeBindingMode: Immediate\n---\nkind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: ml-models-pv\n  namespace: seldon-mesh\n  labels:\n    type: local\nspec:\n  storageClassName: local-path-immediate\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/models\"\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: ml-models-pvc\n  namespace: seldon-mesh\nspec:\n  storageClassName: local-path-immediate\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  selector:\n    matchLabels:\n      type: local\n\n\nNow we create a new Server based on the provided MLServer configuration but extend it with our PVC by adding this to the rclone container which will allow rclone to move models from this PVC onto the server.\nWe also add a new capability pvc to allow us to schedule models to this server that has the PVC.\n!cat server.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\n\n\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model with the added pvc requirement so our MLServer with the PVC will be targeted during scheduling.\n!cat ./iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\n!kubectl create -f iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!kubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\n!seldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"dc032bcc-3f4e-4395-a2e4-7c1e3ef56e9e\",\n\t\"parameters\": {\n\t\t\"content_type\": null,\n\t\t\"headers\": null\n\t},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": null,\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\n!seldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kubectl delete -f ./iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\n\n\n\n", "kubernetes-pvc-example": "\nKubernetes PVC Example\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\nKind cluster setup\u00b6\nTo run this example in Kind we need to start Kind with access to a local folder where are models are location. In this example we will use a folder in /tmp and associate that with a path in the container.\n!cat kind-config.yaml\n\n\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n  extraMounts:\n    - hostPath: /tmp/models\n      containerPath: /models\n\n\nTo start a Kind cluster with these settings using our ansible script you can run from the project root folder\nansible-playbook ansible/playbooks/kind-cluster.yaml -e kind_config_file=${PWD}/samples/examples/local-pvc/kind-config.yaml\n\n\nNow you should finish the Seldon install following the docs.\nCreate the local folder we will use for our models and copy an example iris sklearn model to it.\n!mkdir -p /tmp/models\n!gsutil cp -r gs://seldon-models/mlserver/iris /tmp/models\n\n\n\n\nCreate Server with PVC\u00b6\nHere we create a storage class and associated persistent colume referencing the /models folder where our models are stored.\n!cat pvc.yaml\n\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-path-immediate\nprovisioner: rancher.io/local-path\nreclaimPolicy: Delete\nmountOptions:\n  - debug\nvolumeBindingMode: Immediate\n---\nkind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: ml-models-pv\n  namespace: seldon-mesh\n  labels:\n    type: local\nspec:\n  storageClassName: local-path-immediate\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/models\"\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: ml-models-pvc\n  namespace: seldon-mesh\nspec:\n  storageClassName: local-path-immediate\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  selector:\n    matchLabels:\n      type: local\n\n\nNow we create a new Server based on the provided MLServer configuration but extend it with our PVC by adding this to the rclone container which will allow rclone to move models from this PVC onto the server.\nWe also add a new capability pvc to allow us to schedule models to this server that has the PVC.\n!cat server.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\n\n\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model with the added pvc requirement so our MLServer with the PVC will be targeted during scheduling.\n!cat ./iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\n!kubectl create -f iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!kubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\n!seldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"dc032bcc-3f4e-4395-a2e4-7c1e3ef56e9e\",\n\t\"parameters\": {\n\t\t\"content_type\": null,\n\t\t\"headers\": null\n\t},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": null,\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\n!seldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kubectl delete -f ./iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\n\n\n", "kind-cluster-setup": "\nKind cluster setup\u00b6\nTo run this example in Kind we need to start Kind with access to a local folder where are models are location. In this example we will use a folder in /tmp and associate that with a path in the container.\n!cat kind-config.yaml\n\n\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n  extraMounts:\n    - hostPath: /tmp/models\n      containerPath: /models\n\n\nTo start a Kind cluster with these settings using our ansible script you can run from the project root folder\nansible-playbook ansible/playbooks/kind-cluster.yaml -e kind_config_file=${PWD}/samples/examples/local-pvc/kind-config.yaml\n\n\nNow you should finish the Seldon install following the docs.\nCreate the local folder we will use for our models and copy an example iris sklearn model to it.\n!mkdir -p /tmp/models\n!gsutil cp -r gs://seldon-models/mlserver/iris /tmp/models\n\n\n", "create-server-with-pvc": "\nCreate Server with PVC\u00b6\nHere we create a storage class and associated persistent colume referencing the /models folder where our models are stored.\n!cat pvc.yaml\n\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-path-immediate\nprovisioner: rancher.io/local-path\nreclaimPolicy: Delete\nmountOptions:\n  - debug\nvolumeBindingMode: Immediate\n---\nkind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: ml-models-pv\n  namespace: seldon-mesh\n  labels:\n    type: local\nspec:\n  storageClassName: local-path-immediate\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/models\"\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: ml-models-pvc\n  namespace: seldon-mesh\nspec:\n  storageClassName: local-path-immediate\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  selector:\n    matchLabels:\n      type: local\n\n\nNow we create a new Server based on the provided MLServer configuration but extend it with our PVC by adding this to the rclone container which will allow rclone to move models from this PVC onto the server.\nWe also add a new capability pvc to allow us to schedule models to this server that has the PVC.\n!cat server.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\n", "sklearn-model": "\nSKLearn Model\u00b6\nWe use a simple sklearn iris classification model with the added pvc requirement so our MLServer with the PVC will be targeted during scheduling.\n!cat ./iris.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\n!kubectl create -f iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!kubectl get model iris -n ${NAMESPACE} -o jsonpath='{.status}' | jq -M .\n\n\n{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"ModelReady\"\n    },\n    {\n      \"lastTransitionTime\": \"2022-12-24T11:04:37Z\",\n      \"status\": \"True\",\n      \"type\": \"Ready\"\n    }\n  ],\n  \"replicas\": 1\n}\n\n\n!seldon model infer iris --inference-host ${MESH_IP}:80 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"dc032bcc-3f4e-4395-a2e4-7c1e3ef56e9e\",\n\t\"parameters\": {\n\t\t\"content_type\": null,\n\t\t\"headers\": null\n\t},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": null,\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nDo a gRPC inference call\n!seldon model infer iris --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\",\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kubectl delete -f ./iris.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"iris\" deleted\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/k8s-pvc.html", "key": "examples/k8s-pvc"}}, "getting-started/kubernetes-installation/security/reference": {"sections": {"security-settings-reference": "\nSecurity Settings Reference\u00b6\n\nHelm Settings\u00b6\n# Security settings\nsecurity:\n  controlplane:\n    protocol: PLAINTEXT\n    ssl:\n      server:\n        secret: seldon-controlplane-server\n        clientValidationSecret: seldon-controlplane-client\n        keyPath: /tmp/certs/cps/tls.key\n        crtPath: /tmp/certs/cps/tls.crt\n        caPath: /tmp/certs/cps/ca.crt\n        clientCaPath: /tmp/certs/cpc/ca.crt\n      client:\n        secret: seldon-controlplane-client\n        serverValidationSecret: seldon-controlplane-server\n        keyPath: /tmp/certs/cpc/tls.key\n        crtPath: /tmp/certs/cpc/tls.crt\n        caPath: /tmp/certs/cpc/ca.crt\n        serverCaPath: /tmp/certs/cps/ca.crt\n  kafka:\n    protocol: PLAINTEXT\n    sasl:\n      mechanism: SCRAM-SHA-512\n      client:\n        username: seldon\n        secret:\n        passwordPath: password\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n        keyPath: /tmp/certs/kafka/client/tls.key\n        crtPath: /tmp/certs/kafka/client/tls.crt\n        caPath: /tmp/certs/kafka/client/ca.crt\n        brokerCaPath: /tmp/certs/kafka/broker/ca.crt\n        endpointIdentificationAlgorithm:\n  envoy:\n    protocol: PLAINTEXT\n    ssl:\n      upstream:\n        server:\n          secret: seldon-upstream-server\n          clientValidationSecret: seldon-upstream-client\n          keyPath: /tmp/certs/dus/tls.key\n          crtPath: /tmp/certs/dus/tls.crt\n          caPath: /tmp/certs/dus/ca.crt\n          clientCaPath: /tmp/certs/duc/ca.crt\n        client:\n          secret: seldon-upstream-client\n          serverValidationSecret: seldon-upstream-server\n          keyPath: /tmp/certs/duc/tls.key\n          crtPath: /tmp/certs/duc/tls.crt\n          caPath: /tmp/certs/duc/ca.crt\n          serverCaPath: /tmp/certs/dus/ca.crt\n      downstream:\n        server:\n          secret: seldon-downstream-server\n          clientValidationSecret:\n          keyPath: /tmp/certs/dds/tls.key\n          crtPath: /tmp/certs/dds/tls.crt\n          caPath: /tmp/certs/dds/ca.crt\n          clientCaPath: /tmp/certs/ddc/ca.crt\n        client:\n          mtls: false\n          secret:\n          serverValidationSecret: seldon-downstream-server\n          keyPath: /tmp/certs/ddc/tls.key\n          crtPath: /tmp/certs/ddc/tls.crt\n          caPath: /tmp/certs/ddc/ca.crt\n          serverCaPath: /tmp/certs/dds/ca.crt\n\n# A list of image pull secrets\nimagePullSecrets:\n\n\n\n\n\nEnvironment variables\u00b6\nKubernetes secrets and mounted files can be used to provide the certificates in PEM format. These are controlled by environment variables for server or client depending on the component:\n\nControl Plane\u00b6\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SECURITY_PROTOCOL\nSSL or PLAINTEXT\n\n\n\nFor a server (scheduler):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nCONTROL_PLANE_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nFor a client (agent, modelgateway, hodometer, CRD controller):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nCONTROL_PLANE_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n\n\nKafka\u00b6\n\n\nEnvVar\nValue\n\n\n\nKAFKA_SECURITY_PROTOCOL\nPLAINTXT, SSL, or SASL_SSL\n\n\n\n\n\nEnvVar\nValue\n\n\n\nKAFKA_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the Kafka client certificate\n\nKAFKA_CLIENT_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nKAFKA_CLIENT_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nKAFKA_CLIENT_SERVER_TLS_CA_LOCATION\nthe path to the CA chain for the client\n\nKAFKA_BROKER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots for the kafka broker\n\nKAFKA_BROKER_TLS_CA_LOCATION\nThe path to the broker validatiob CA chain\n\nKAFKA_CLIENT_SASL_USERNAME\nSASL username\n\nKAFKA_CLIENT_SASL_SECRET_NAME\nthe name of the namespaced secret which holds the SASL password\n\nKAFKA_CLIENT_SASL_PASSWORD_LOCATION\nthe path to the file containing the SASL password\n\n\n\n\n\nEnvoy\u00b6\nEnvoy xDS server will use the control plane server and client certificates defined above.\nDownstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nDownstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\nUpstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_UPSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nUpstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n\n\n", "helm-settings": "\nHelm Settings\u00b6\n# Security settings\nsecurity:\n  controlplane:\n    protocol: PLAINTEXT\n    ssl:\n      server:\n        secret: seldon-controlplane-server\n        clientValidationSecret: seldon-controlplane-client\n        keyPath: /tmp/certs/cps/tls.key\n        crtPath: /tmp/certs/cps/tls.crt\n        caPath: /tmp/certs/cps/ca.crt\n        clientCaPath: /tmp/certs/cpc/ca.crt\n      client:\n        secret: seldon-controlplane-client\n        serverValidationSecret: seldon-controlplane-server\n        keyPath: /tmp/certs/cpc/tls.key\n        crtPath: /tmp/certs/cpc/tls.crt\n        caPath: /tmp/certs/cpc/ca.crt\n        serverCaPath: /tmp/certs/cps/ca.crt\n  kafka:\n    protocol: PLAINTEXT\n    sasl:\n      mechanism: SCRAM-SHA-512\n      client:\n        username: seldon\n        secret:\n        passwordPath: password\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n        keyPath: /tmp/certs/kafka/client/tls.key\n        crtPath: /tmp/certs/kafka/client/tls.crt\n        caPath: /tmp/certs/kafka/client/ca.crt\n        brokerCaPath: /tmp/certs/kafka/broker/ca.crt\n        endpointIdentificationAlgorithm:\n  envoy:\n    protocol: PLAINTEXT\n    ssl:\n      upstream:\n        server:\n          secret: seldon-upstream-server\n          clientValidationSecret: seldon-upstream-client\n          keyPath: /tmp/certs/dus/tls.key\n          crtPath: /tmp/certs/dus/tls.crt\n          caPath: /tmp/certs/dus/ca.crt\n          clientCaPath: /tmp/certs/duc/ca.crt\n        client:\n          secret: seldon-upstream-client\n          serverValidationSecret: seldon-upstream-server\n          keyPath: /tmp/certs/duc/tls.key\n          crtPath: /tmp/certs/duc/tls.crt\n          caPath: /tmp/certs/duc/ca.crt\n          serverCaPath: /tmp/certs/dus/ca.crt\n      downstream:\n        server:\n          secret: seldon-downstream-server\n          clientValidationSecret:\n          keyPath: /tmp/certs/dds/tls.key\n          crtPath: /tmp/certs/dds/tls.crt\n          caPath: /tmp/certs/dds/ca.crt\n          clientCaPath: /tmp/certs/ddc/ca.crt\n        client:\n          mtls: false\n          secret:\n          serverValidationSecret: seldon-downstream-server\n          keyPath: /tmp/certs/ddc/tls.key\n          crtPath: /tmp/certs/ddc/tls.crt\n          caPath: /tmp/certs/ddc/ca.crt\n          serverCaPath: /tmp/certs/dds/ca.crt\n\n# A list of image pull secrets\nimagePullSecrets:\n\n\n\n", "environment-variables": "\nEnvironment variables\u00b6\nKubernetes secrets and mounted files can be used to provide the certificates in PEM format. These are controlled by environment variables for server or client depending on the component:\n\nControl Plane\u00b6\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SECURITY_PROTOCOL\nSSL or PLAINTEXT\n\n\n\nFor a server (scheduler):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nCONTROL_PLANE_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nFor a client (agent, modelgateway, hodometer, CRD controller):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nCONTROL_PLANE_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n\n\nKafka\u00b6\n\n\nEnvVar\nValue\n\n\n\nKAFKA_SECURITY_PROTOCOL\nPLAINTXT, SSL, or SASL_SSL\n\n\n\n\n\nEnvVar\nValue\n\n\n\nKAFKA_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the Kafka client certificate\n\nKAFKA_CLIENT_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nKAFKA_CLIENT_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nKAFKA_CLIENT_SERVER_TLS_CA_LOCATION\nthe path to the CA chain for the client\n\nKAFKA_BROKER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots for the kafka broker\n\nKAFKA_BROKER_TLS_CA_LOCATION\nThe path to the broker validatiob CA chain\n\nKAFKA_CLIENT_SASL_USERNAME\nSASL username\n\nKAFKA_CLIENT_SASL_SECRET_NAME\nthe name of the namespaced secret which holds the SASL password\n\nKAFKA_CLIENT_SASL_PASSWORD_LOCATION\nthe path to the file containing the SASL password\n\n\n\n\n\nEnvoy\u00b6\nEnvoy xDS server will use the control plane server and client certificates defined above.\nDownstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nDownstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\nUpstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_UPSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nUpstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n\n", "control-plane": "\nControl Plane\u00b6\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SECURITY_PROTOCOL\nSSL or PLAINTEXT\n\n\n\nFor a server (scheduler):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nCONTROL_PLANE_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nFor a client (agent, modelgateway, hodometer, CRD controller):\n\n\nEnvVar\nValue\n\n\n\nCONTROL_PLANE_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nCONTROL_PLANE_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nCONTROL_PLANE_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nCONTROL_PLANE_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nCONTROL_PLANE_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client\n\nCONTROL_PLANE_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n", "kafka": "\nKafka\u00b6\n\n\nEnvVar\nValue\n\n\n\nKAFKA_SECURITY_PROTOCOL\nPLAINTXT, SSL, or SASL_SSL\n\n\n\n\n\nEnvVar\nValue\n\n\n\nKAFKA_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the Kafka client certificate\n\nKAFKA_CLIENT_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nKAFKA_CLIENT_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nKAFKA_CLIENT_SERVER_TLS_CA_LOCATION\nthe path to the CA chain for the client\n\nKAFKA_BROKER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots for the kafka broker\n\nKAFKA_BROKER_TLS_CA_LOCATION\nThe path to the broker validatiob CA chain\n\nKAFKA_CLIENT_SASL_USERNAME\nSASL username\n\nKAFKA_CLIENT_SASL_SECRET_NAME\nthe name of the namespaced secret which holds the SASL password\n\nKAFKA_CLIENT_SASL_PASSWORD_LOCATION\nthe path to the file containing the SASL password\n\n\n\n", "envoy": "\nEnvoy\u00b6\nEnvoy xDS server will use the control plane server and client certificates defined above.\nDownstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nDownstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_DOWNSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_DOWNSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_DOWNSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_DOWNSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\nUpstream server\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the client certificate\n\nENVOY_UPSTREAM_SERVER_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_SERVER_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the client for mTLS verification\n\n\n\nUpstream client\n\n\nEnvVar\nValue\n\n\n\nENVOY_UPSTREAM_CLIENT_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the certificates\n\nENVOY_UPSTREAM_SERVER_TLS_SECRET_NAME\n(optional) the name of the namespaced secret which holds the validation ca roots to verify the server certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_KEY_LOCATION\nthe path to the TLS private key\n\nENVOY_UPSTREAM_CLIENT_TLS_CRT_LOCATION\nthe path to the TLS certificate\n\nENVOY_UPSTREAM_CLIENT_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server\n\nENVOY_UPSTREAM_SERVER_TLS_CA_LOCATION\nthe path to the TLS CA chain for the server for mTLS verification\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/reference.html", "key": "getting-started/kubernetes-installation/security/reference"}}, "metrics/operational": {"sections": {"operational-metrics": "\nOperational Metrics\u00b6\nWhile the system is running we collect metrics via Prometheus that allow users to observe different aspects of SCv2 with regards to throughout, latency, memory, CPU etc.\nThis is in addition to the standard Kubernetes metrics that are scraped by Prometheus.\nThere is a Grafana dashboard (referenced below) that provides an overview of the system.\n\nList of SCv2 metrics\u00b6\nThe list of SCv2 metrics that we are compiling is as follows.\nFor the agent that sits next to the inference servers:\n// Model metrics\nconst (\n\t// Histograms do no include pipeline label for efficiency\n\tmodelHistogramName = \"seldon_model_infer_api_seconds\"\n\t// We use base infer counters to store core metrics per pipeline\n\tmodelInferCounterName                 = \"seldon_model_infer_total\"\n\tmodelInferLatencyCounterName          = \"seldon_model_infer_seconds_total\"\n\tmodelAggregateInferCounterName        = \"seldon_model_aggregate_infer_total\"\n\tmodelAggregateInferLatencyCounterName = \"seldon_model_aggregate_infer_seconds_total\"\n)\n\n// Agent metrics\nconst (\n\tcacheEvictCounterName                              = \"seldon_cache_evict_count\"\n\tcacheMissCounterName                               = \"seldon_cache_miss_count\"\n\tloadModelCounterName                               = \"seldon_load_model_counter\"\n\tunloadModelCounterName                             = \"seldon_unload_model_counter\"\n\tloadedModelGaugeName                               = \"seldon_loaded_model_gauge\"\n\tloadedModelMemoryGaugeName                         = \"seldon_loaded_model_memory_bytes_gauge\"\n\tevictedModelMemoryGaugeName                        = \"seldon_evicted_model_memory_bytes_gauge\"\n\tserverReplicaMemoryCapacityGaugeName               = \"seldon_server_replica_memory_capacity_bytes_gauge\"\n\tserverReplicaMemoryCapacityWithOverCommitGaugeName = \"seldon_server_replica_memory_capacity_overcommit_bytes_gauge\"\n)\n\n\nFor the pipeline gateway that handles requests to pipelines:\n//\n// The aggregate metrics exist for efficiency, as the summation can be\n// very slow in Prometheus when many pipelines exist.\nconst (\n\t// Histograms do no include model label for efficiency\n\tpipelineHistogramName = \"seldon_pipeline_infer_api_seconds\"\n\t// We use base infer counters to store core metrics per pipeline\n\tpipelineInferCounterName                 = \"seldon_pipeline_infer_total\"\n\tpipelineInferLatencyCounterName          = \"seldon_pipeline_infer_seconds_total\"\n\tpipelineAggregateInferCounterName        = \"seldon_pipeline_aggregate_infer_total\"\n\tpipelineAggregateInferLatencyCounterName = \"seldon_pipeline_aggregate_infer_seconds_total\"\n)\n\n\nMany of these metrics are model and pipeline level counters and gauges.\nWe also aggregate some of these metrics to speed up the display of graphs. We don\u2019t presently store per-model histogram metrics for performance reasons. However, we do presently store per-pipeline histogram metrics.\nThis is experimental and these metrics are bound to change to reflect the trends we want to capture as we get more information about the usage of the system.\n\n\nGrafana dashboard\u00b6\nWe have a prebuilt Grafana dashboard that makes use of many of the metrics that we expose.\n\n\nLocal Use\u00b6\nGrafana and Prometheus are available when you run Seldon locally.\nYou will be able to connect to the Grafana dashboard at http://localhost:3000.\nPrometheus will be available at http://localhost:9090.\n\n\nKubernetes Installation\u00b6\nDownload the dashboard from SCv2 dashboard and import it in Grafana, making sure that the data source is pointing to the correct Prometheus store.\nFind more information on how to import the dashboard here.\n\n\nLocal Metrics Examples\u00b6\nAn example to show raw metrics that Prometheus will scrape.\n\n\n\n\n", "list-of-scv2-metrics": "\nList of SCv2 metrics\u00b6\nThe list of SCv2 metrics that we are compiling is as follows.\nFor the agent that sits next to the inference servers:\n// Model metrics\nconst (\n\t// Histograms do no include pipeline label for efficiency\n\tmodelHistogramName = \"seldon_model_infer_api_seconds\"\n\t// We use base infer counters to store core metrics per pipeline\n\tmodelInferCounterName                 = \"seldon_model_infer_total\"\n\tmodelInferLatencyCounterName          = \"seldon_model_infer_seconds_total\"\n\tmodelAggregateInferCounterName        = \"seldon_model_aggregate_infer_total\"\n\tmodelAggregateInferLatencyCounterName = \"seldon_model_aggregate_infer_seconds_total\"\n)\n\n// Agent metrics\nconst (\n\tcacheEvictCounterName                              = \"seldon_cache_evict_count\"\n\tcacheMissCounterName                               = \"seldon_cache_miss_count\"\n\tloadModelCounterName                               = \"seldon_load_model_counter\"\n\tunloadModelCounterName                             = \"seldon_unload_model_counter\"\n\tloadedModelGaugeName                               = \"seldon_loaded_model_gauge\"\n\tloadedModelMemoryGaugeName                         = \"seldon_loaded_model_memory_bytes_gauge\"\n\tevictedModelMemoryGaugeName                        = \"seldon_evicted_model_memory_bytes_gauge\"\n\tserverReplicaMemoryCapacityGaugeName               = \"seldon_server_replica_memory_capacity_bytes_gauge\"\n\tserverReplicaMemoryCapacityWithOverCommitGaugeName = \"seldon_server_replica_memory_capacity_overcommit_bytes_gauge\"\n)\n\n\nFor the pipeline gateway that handles requests to pipelines:\n//\n// The aggregate metrics exist for efficiency, as the summation can be\n// very slow in Prometheus when many pipelines exist.\nconst (\n\t// Histograms do no include model label for efficiency\n\tpipelineHistogramName = \"seldon_pipeline_infer_api_seconds\"\n\t// We use base infer counters to store core metrics per pipeline\n\tpipelineInferCounterName                 = \"seldon_pipeline_infer_total\"\n\tpipelineInferLatencyCounterName          = \"seldon_pipeline_infer_seconds_total\"\n\tpipelineAggregateInferCounterName        = \"seldon_pipeline_aggregate_infer_total\"\n\tpipelineAggregateInferLatencyCounterName = \"seldon_pipeline_aggregate_infer_seconds_total\"\n)\n\n\nMany of these metrics are model and pipeline level counters and gauges.\nWe also aggregate some of these metrics to speed up the display of graphs. We don\u2019t presently store per-model histogram metrics for performance reasons. However, we do presently store per-pipeline histogram metrics.\nThis is experimental and these metrics are bound to change to reflect the trends we want to capture as we get more information about the usage of the system.\n", "grafana-dashboard": "\nGrafana dashboard\u00b6\nWe have a prebuilt Grafana dashboard that makes use of many of the metrics that we expose.\n\n\nLocal Use\u00b6\nGrafana and Prometheus are available when you run Seldon locally.\nYou will be able to connect to the Grafana dashboard at http://localhost:3000.\nPrometheus will be available at http://localhost:9090.\n\n\nKubernetes Installation\u00b6\nDownload the dashboard from SCv2 dashboard and import it in Grafana, making sure that the data source is pointing to the correct Prometheus store.\nFind more information on how to import the dashboard here.\n\n\nLocal Metrics Examples\u00b6\nAn example to show raw metrics that Prometheus will scrape.\n\n\n\n", "local-use": "\nLocal Use\u00b6\nGrafana and Prometheus are available when you run Seldon locally.\nYou will be able to connect to the Grafana dashboard at http://localhost:3000.\nPrometheus will be available at http://localhost:9090.\n", "kubernetes-installation": "\nKubernetes Installation\u00b6\nDownload the dashboard from SCv2 dashboard and import it in Grafana, making sure that the data source is pointing to the correct Prometheus store.\nFind more information on how to import the dashboard here.\n", "local-metrics-examples": "\nLocal Metrics Examples\u00b6\nAn example to show raw metrics that Prometheus will scrape.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/metrics/operational.html", "key": "metrics/operational"}}, "examples/pipeline-ready-and-metadata": {"sections": {"pipeline-readiness": "\nPipeline Readiness\u00b6\nRun these examples from the samples folder.\n\nPipeline Readiness Check and Metdata Calls\u00b6\nLocal example settings.\n%env INFER_REST_ENDPOINT=http://0.0.0.0:9000\n%env INFER_GRPC_ENDPOINT=0.0.0.0:9000\n%env SELDON_SCHEDULE_HOST=0.0.0.0:9004\n\n\nenv: INFER_REST_ENDPOINT=http://0.0.0.0:9000\nenv: INFER_GRPC_ENDPOINT=0.0.0.0:9000\nenv: SELDON_SCHEDULE_HOST=0.0.0.0:9004\n\n\n\nRemote k8s cluster example settings - change as neeed for your needs.\n#%env INFER_REST_ENDPOINT=http://172.19.255.1:80\n#%env INFER_GRPC_ENDPOINT=172.19.255.1:80\n#%env SELDON_SCHEDULE_HOST=172.19.255.2:9004\n\n\n\nModel Chain - Ready Check\u00b6\nWe will check the readiness of the Pipeline after every change to model and pipeline.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepit2i8ufs73flaitg\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:16.365934922Z\"}}]}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline unload tfsimples\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nModels will still be ready even though Pipeline terminated\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepj5qi8ufs73flaiu0\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:51.626155116Z\", \"modelsReady\":true}}]}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\nseldon pipeline unload tfsimples\n\n\n\n\nKubernetes Resource Example\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=1s pipeline --all -n ${NAMESPACE}\n\n\nerror: timed out waiting for the condition on pipelines/tfsimples\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:31Z\",\"status\":\"False\",\"type\":\"ModelsReady\"}\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:49Z\",\"status\":\"True\",\"type\":\"ModelsReady\"}\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\n\n\n\n\n\n", "pipeline-readiness-check-and-metdata-calls": "\nPipeline Readiness Check and Metdata Calls\u00b6\nLocal example settings.\n%env INFER_REST_ENDPOINT=http://0.0.0.0:9000\n%env INFER_GRPC_ENDPOINT=0.0.0.0:9000\n%env SELDON_SCHEDULE_HOST=0.0.0.0:9004\n\n\nenv: INFER_REST_ENDPOINT=http://0.0.0.0:9000\nenv: INFER_GRPC_ENDPOINT=0.0.0.0:9000\nenv: SELDON_SCHEDULE_HOST=0.0.0.0:9004\n\n\n\nRemote k8s cluster example settings - change as neeed for your needs.\n#%env INFER_REST_ENDPOINT=http://172.19.255.1:80\n#%env INFER_GRPC_ENDPOINT=172.19.255.1:80\n#%env SELDON_SCHEDULE_HOST=172.19.255.2:9004\n\n\n\nModel Chain - Ready Check\u00b6\nWe will check the readiness of the Pipeline after every change to model and pipeline.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepit2i8ufs73flaitg\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:16.365934922Z\"}}]}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline unload tfsimples\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nModels will still be ready even though Pipeline terminated\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepj5qi8ufs73flaiu0\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:51.626155116Z\", \"modelsReady\":true}}]}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\nseldon pipeline unload tfsimples\n\n\n\n\nKubernetes Resource Example\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=1s pipeline --all -n ${NAMESPACE}\n\n\nerror: timed out waiting for the condition on pipelines/tfsimples\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:31Z\",\"status\":\"False\",\"type\":\"ModelsReady\"}\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:49Z\",\"status\":\"True\",\"type\":\"ModelsReady\"}\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\n\n\n\n\n", "model-chain-ready-check": "\nModel Chain - Ready Check\u00b6\nWe will check the readiness of the Pipeline after every change to model and pipeline.\ncat ./pipelines/tfsimples.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: tfsimples\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n      inputs:\n      - tfsimple1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple1.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple2\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepit2i8ufs73flaitg\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:16.365934922Z\"}}]}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple1.yaml\nseldon model status tfsimple1 -w ModelAvailable\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n\n}\n\n\nseldon model load -f ./models/tfsimple2.yaml\nseldon model status tfsimple2 -w ModelAvailable | jq -M .\n\n\n{}\n{}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline unload tfsimples\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\nERROR:\n  Code: Unimplemented\n  Message:\n\n\nModels will still be ready even though Pipeline terminated\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon pipeline load -f ./pipelines/tfsimples.yaml\nseldon pipeline status tfsimples -w PipelineReady\n\n\n{\"pipelineName\":\"tfsimples\", \"versions\":[{\"pipeline\":{\"name\":\"tfsimples\", \"uid\":\"ciepj5qi8ufs73flaiu0\", \"version\":1, \"steps\":[{\"name\":\"tfsimple1\"}, {\"name\":\"tfsimple2\", \"inputs\":[\"tfsimple1.outputs\"], \"tensorMap\":{\"tfsimple1.outputs.OUTPUT0\":\"INPUT0\", \"tfsimple1.outputs.OUTPUT1\":\"INPUT1\"}}], \"output\":{\"steps\":[\"tfsimple2.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:47:51.626155116Z\", \"modelsReady\":true}}]}\n\n\ncurl -Ik ${INFER_REST_ENDPOINT}/v2/pipelines/tfsimples/ready\n\n\n\n\n\ngrpcurl -d '{\"name\":\"tfsimples\"}' \\\n    -plaintext \\\n    -import-path ../apis \\\n    -proto ../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    -rpc-header seldon-model:tfsimples.pipeline \\\n    ${INFER_GRPC_ENDPOINT} inference.GRPCInferenceService/ModelReady\n\n\n{\n  \"ready\": true\n}\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\ntrue\n\n\n\nseldon model unload tfsimple1\nseldon model unload tfsimple2\n\n\nseldon pipeline status tfsimples | jq .versions[0].state.modelsReady\n\n\nnull\n\n\n\nseldon pipeline unload tfsimples\n\n\n", "kubernetes-resource-example": "\nKubernetes Resource Example\u00b6\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\nkubectl create -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples created\n\n\n\nkubectl wait --for condition=ready --timeout=1s pipeline --all -n ${NAMESPACE}\n\n\nerror: timed out waiting for the condition on pipelines/tfsimples\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:31Z\",\"status\":\"False\",\"type\":\"ModelsReady\"}\n\n\nkubectl create -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl create -f ./models/tfsimple2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/tfsimple1 created\nmodel.mlops.seldon.io/tfsimple2 created\n\n\n\nkubectl wait --for condition=ready --timeout=300s pipeline --all -n ${NAMESPACE}\n\n\npipeline.mlops.seldon.io/tfsimples condition met\n\n\n\nkubectl get pipeline tfsimples -o jsonpath='{.status.conditions[0]}' -n ${NAMESPACE}\n\n\n{\"lastTransitionTime\":\"2022-11-14T10:25:49Z\",\"status\":\"True\",\"type\":\"ModelsReady\"}\n\n\nkubectl delete -f ./models/tfsimple1.yaml -n ${NAMESPACE}\nkubectl delete -f ./models/tfsimple2.yaml -n ${NAMESPACE}\nkubectl delete -f ./pipelines/tfsimples.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"tfsimple1\" deleted\nmodel.mlops.seldon.io \"tfsimple2\" deleted\npipeline.mlops.seldon.io \"tfsimples\" deleted\n\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/pipeline-ready-and-metadata.html", "key": "examples/pipeline-ready-and-metadata"}}, "inference": {"sections": {"inference": "\nInference\u00b6\nThis section will discuss how to make inference calls against your Seldon models or pipelines.\nYou can make synchronous inference requests via REST or gRPC or asynchronous requests via Kafka topics.\nThe content of your request should be an inference v2 protocol payload:\n\nREST payloads will generally be in the JSON v2 protocol format.\ngRPC and Kafka payloads must be in the Protobuf v2 protocol format.\n\n\nSynchronous Requests\u00b6\nFor making synchronous requests, the process will generally be:\n\nFind the appropriate service endpoint (IP address and port) for accessing the installation of Seldon Core v2.\nDetermine the appropriate headers/metadata for the request.\nMake requests via REST or gRPC.\n\n\nFind the Seldon Service Endpoint\u00b6\n\nDocker ComposeKubernetesIn the default Docker Compose setup, container ports are accessible from the host machine.\nThis means you can use localhost or 0.0.0.0 as the hostname.\nThe default port for sending inference requests to the Seldon system is 9000.\nThis is controlled by the ENVOY_DATA_PORT environment variable for Compose.\nPutting this together, you can send inference requests to 0.0.0.0:9000.\nIn Kubernetes, Seldon creates a single Service called seldon-mesh in the namespace it is installed into.\nBy default, this namespace is also called seldon-mesh.\nIf this Service is exposed via a load balancer, the appropriate address and port can be found via:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nIf you are not using a LoadBalancer for the seldon-mesh Service, you can still send inference requests.\nFor development and testing purposes, you can port-forward the Service locally using the below.\nInference requests can then be sent to localhost:8080.\nkubectl port-forward svc/seldon-mesh -n seldon-mesh 8080:80\n\n\nIf you are using a service mesh like Istio or Ambassador, you will need to use the IP address of the service mesh ingress and determine the appropriate port.\n\n\n\nMake Inference Requests\u00b6\nLet us imagine making inference requests to a model called iris.\nThis iris model has the following schema, which can be set in a model-settings.json file for MLServer:\n{\n    \"name\": \"iris\",\n    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n    \"inputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"FP32\",\n            \"shape\": [-1, 4]\n        }\n    ],\n    \"outputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"INT64\",\n            \"shape\": [-1, 1]\n        }\n    ],\n    \"parameters\": {\n        \"version\": \"1\"\n    }\n}\n\n\nExamples are given below for some common tools for making requests.\n\nSeldon CLIcURLgrpcurlPython tritonclientAn example seldon request might look like this:\nseldon model infer iris \\\n        '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nThe default inference mode is REST, but you can also send gRPC requests like this:\nseldon model infer iris \\\n        --inference-mode grpc \\\n        '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nAn example curl request might look like this:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nAn example grpcurl request might look like this:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nThe above request was run from the project root folder allowing reference to the Protobuf manifests defined in the apis/ folder.\nYou can use the Python tritonclient package to send inference requests.\nA short, self-contained example is:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\"iris\", inputs)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\nTip\nFor pipelines, a synchronous request is possible if the pipeline has an outputs section defined in its spec.\n\n\n\nRequest Routing\u00b6\n\nSeldon Routes\u00b6\nSeldon needs to determine where to route requests to, as models and pipelines might have the same name.\nThere are two ways of doing this: header-based routing (preferred) and path-based routing.\n\nHeadersPathsSeldon can route requests to the correct endpoint via headers in HTTP calls, both for REST (HTTP/1.1) and gRPC (HTTP/2).\nUse the Seldon-Model header as follows:\n\nFor models, use the model name as the value.\nFor example, to send requests to a model named foo use the header Seldon-Model: foo.\nFor pipelines, use the pipeline name followed by .pipeline as the value.\nFor example, to send requests to a pipeline named foo use the header Seldon-Model: foo.pipeline.\n\nThe seldon CLI is aware of these rules and can be used to easily send requests to your deployed resources.\nSee the examples and the Seldon CLI docs for more information.\nThe inference v2 protocol is only aware of models, thus has no concept of pipelines.\nSeldon works around this limitation by introducing virtual endpoints for pipelines.\nVirtual means that Seldon understands them, but other v2 protocol-compatible components like inference servers do not.\nUse the following rules for paths to route to models and pipelines:\n\nFor models, use the path prefix /v2/models/{model name}.\nThis is normal usage of the inference v2 protocol.\nFor pipelines, you can use the path prefix /v2/pipelines/{pipeline name}.\nOtherwise calling pipelines looks just like the inference v2 protocol for models.\nDo not use any suffix for the pipeline name as you would for routing headers.\nFor pipelines, you can also use the path prefix /v2/models/{pipeline name}.pipeline.\nAgain, this form looks just like the inference v2 protocol for models.\n\n\nExtending our examples from above, the requests may look like the below when using header-based routing.\n\nSeldon CLIcURLgrpcurlPython tritonclientNo changes are required as the seldon CLI already understands how to set the appropriate gRPC and REST headers.\nNote the header in the last line:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \\\n        -H \"Seldon-Model: iris\"\n\n\nNote the rpc-header flag in the penultimate line:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t-rpc-header seldon-model:iris \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nNote the headers dictionary in the client.infer() call:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\n    \"iris\",\n    inputs,\n    headers={\"Seldon-Model\": \"iris\"},\n)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\n\nIngress Routes\u00b6\nIf you are using an ingress controller to make inference requests with Seldon, you will need to configure the routing rules correctly.\nThere are many ways to do this, but custom path prefixes will not work with gRPC.\nThis is because gRPC determines the path based on the Protobuf definition.\nSome gRPC implementations permit manipulating paths when sending requests, but this is by no means universal.\nIf you want to expose your inference endpoints via gRPC and REST in a consistent way, you should use virtual hosts, subdomains, or headers.\nThe downside of using only paths is that you cannot differentiate between different installations of Seldon Core v2 or between traffic to Seldon and any other inference endpoints you may have exposed via the same ingress.\nYou might want to use a mixture of these methods; the choice is yours.\n\nVirtual HostsSubdomainsHeadersPathsVirtual hosts are a way of differentiating between logical services accessed via the same physical machine(s).\nVirtual hosts are defined by the Host header for HTTP/1 and the :authority pseudo-header for HTTP/2.\nThese represent the same thing, and the HTTP/2 specification defines how to translate these when converting between protocol versions.\nMany tools and libraries treat these headers as special and have particular ways of handling them.\nSome common ones are given below:\n\nThe seldon CLI has an --authority flag which applies to both REST and gRPC inference calls.\ncurl accepts Host as a normal header.\ngrpcurl has an -authority flag.\nIn Go, the standard library\u2019s http.Request struct has a Host field and ignores attempts to set this value via headers.\nIn Python, the requests library accepts the host as a normal header.\n\nBe sure to check the documentation for how to set this with your preferred tools and languages.\nSubdomain names constitute a part of the overall host name.\nAs such, specifying a subdomain name for requests will involve setting the appropriate host in the URI.\nFor example, you may expose inference services in the namespaces seldon-1 and seldon-2 as in the following snippets:\ncurl https://seldon-1.example.com/v2/models/iris/infer ...\n\nseldon model infer --inference-host https://seldon-2.example.com/v2/models/iris/infer ...\n\n\nMany popular ingresses support subdomain-based routing, including Istio and Nginx.\nPlease refer to the documentation for your ingress of choice for further information.\nMany ingress controllers and service meshes support routing on headers.\nYou can use whatever headers you prefer, so long as they do not conflict with any Seldon relies upon.\nMany tools and libraries support adding custom headers to requests.\nSome common ones are given below:\n\nThe seldon CLI accepts headers using the --header flag, which can be specified multiple times.\ncurl accepts headers using the -H or --header flags.\ngrpcurl accepts headers using the -H flag, which can be specified multiple times.\n\nIt is possible to route on paths by using well-known path prefixes defined by the inference v2 protocol.\nFor gRPC, the full path (or \u201cmethod\u201d) for an inference call is:\n/inference.GRPCInferenceService/ModelInfer\n\n\nThis corresponds to the package (inference), service (GRPCInferenceService), and RPC name (ModelInfer) in the Protobuf definition of the inference v2 protocol.\nYou could use an exact match or a regex like .*inference.* to match this path, for example.\n\n\n\n\n\nAsynchronous Requests\u00b6\nThe Seldon architecture uses Kafka and therefore asynchronous requests can be sent by pushing inference v2 protocol payloads to the appropriate topic.\nTopics have the following form:\nseldon.<namespace>.<model|pipeline>.<name>.<inputs|outputs>\n\n\n\nNote\nIf writing to a pipeline topic, you will need to include a Kafka header with the key pipeline and the value being the name of the pipeline.\n\n\nModel Inference\u00b6\nFor a local install if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.default.model.iris.inputs.\nThe response will appear on seldon.default.model.iris.outputs.\nFor a Kubernetes install in seldon-mesh if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.model.iris.inputs.\nThe response will appear on seldon.seldon-mesh.model.iris.outputs.\n\n\nPipeline Inference\u00b6\nFor a local install if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.default.pipeline.mypipeline.inputs. The response will appear on seldon.default.pipeline.mypipeline.outputs.\nFor a Kubernetes install in seldon-mesh if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.pipeline.mypipeline.inputs. The response will appear on seldon.seldon-mesh.pipeline.mypipeline.outputs.\n\n\n\nPipeline Metadata\u00b6\nIt may be useful to send metadata alongside your inference.\nIf using Kafka directly as described above, you can attach Kafka metadata to your request, which will be passed around the graph.\nWhen making synchronous requests to your pipeline with REST or gRPC you can also do this.\n\nFor REST requests add HTTP headers prefixed with X-\nFor gRPC requests add metadata with keys starting with X-\n\nYou can also do this with the Seldon CLI by setting headers with the --header argument (and also showing response headers with the --show-headers argument)\nseldon pipeline infer --show-headers --header X-foo=bar tfsimples \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n\n\nRequest IDs\u00b6\nFor both model and pipeline requests the response will contain a x-request-id response header. For pipeline requests this can be used to inspect the pipeline steps via the CLI, e.g.:\nseldon pipeline inspect tfsimples --request-id carjjolvqj3j2pfbut10 --offset 10\n\n\nThe --offset parameter specifies how many messages (from the latest) you want to search to find your request. If not specified the last request will be shown.\nx-request-id will also appear in tracing spans.\nIf x-request-id is passed in by the caller then this will be used. It is the caller\u2019s responsibility to ensure it is unique.\nThe IDs generated are XIDs.\n\n", "synchronous-requests": "\nSynchronous Requests\u00b6\nFor making synchronous requests, the process will generally be:\n\nFind the appropriate service endpoint (IP address and port) for accessing the installation of Seldon Core v2.\nDetermine the appropriate headers/metadata for the request.\nMake requests via REST or gRPC.\n\n\nFind the Seldon Service Endpoint\u00b6\n\nDocker ComposeKubernetesIn the default Docker Compose setup, container ports are accessible from the host machine.\nThis means you can use localhost or 0.0.0.0 as the hostname.\nThe default port for sending inference requests to the Seldon system is 9000.\nThis is controlled by the ENVOY_DATA_PORT environment variable for Compose.\nPutting this together, you can send inference requests to 0.0.0.0:9000.\nIn Kubernetes, Seldon creates a single Service called seldon-mesh in the namespace it is installed into.\nBy default, this namespace is also called seldon-mesh.\nIf this Service is exposed via a load balancer, the appropriate address and port can be found via:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nIf you are not using a LoadBalancer for the seldon-mesh Service, you can still send inference requests.\nFor development and testing purposes, you can port-forward the Service locally using the below.\nInference requests can then be sent to localhost:8080.\nkubectl port-forward svc/seldon-mesh -n seldon-mesh 8080:80\n\n\nIf you are using a service mesh like Istio or Ambassador, you will need to use the IP address of the service mesh ingress and determine the appropriate port.\n\n\n\nMake Inference Requests\u00b6\nLet us imagine making inference requests to a model called iris.\nThis iris model has the following schema, which can be set in a model-settings.json file for MLServer:\n{\n    \"name\": \"iris\",\n    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n    \"inputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"FP32\",\n            \"shape\": [-1, 4]\n        }\n    ],\n    \"outputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"INT64\",\n            \"shape\": [-1, 1]\n        }\n    ],\n    \"parameters\": {\n        \"version\": \"1\"\n    }\n}\n\n\nExamples are given below for some common tools for making requests.\n\nSeldon CLIcURLgrpcurlPython tritonclientAn example seldon request might look like this:\nseldon model infer iris \\\n        '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nThe default inference mode is REST, but you can also send gRPC requests like this:\nseldon model infer iris \\\n        --inference-mode grpc \\\n        '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nAn example curl request might look like this:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nAn example grpcurl request might look like this:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nThe above request was run from the project root folder allowing reference to the Protobuf manifests defined in the apis/ folder.\nYou can use the Python tritonclient package to send inference requests.\nA short, self-contained example is:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\"iris\", inputs)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\nTip\nFor pipelines, a synchronous request is possible if the pipeline has an outputs section defined in its spec.\n\n\n\nRequest Routing\u00b6\n\nSeldon Routes\u00b6\nSeldon needs to determine where to route requests to, as models and pipelines might have the same name.\nThere are two ways of doing this: header-based routing (preferred) and path-based routing.\n\nHeadersPathsSeldon can route requests to the correct endpoint via headers in HTTP calls, both for REST (HTTP/1.1) and gRPC (HTTP/2).\nUse the Seldon-Model header as follows:\n\nFor models, use the model name as the value.\nFor example, to send requests to a model named foo use the header Seldon-Model: foo.\nFor pipelines, use the pipeline name followed by .pipeline as the value.\nFor example, to send requests to a pipeline named foo use the header Seldon-Model: foo.pipeline.\n\nThe seldon CLI is aware of these rules and can be used to easily send requests to your deployed resources.\nSee the examples and the Seldon CLI docs for more information.\nThe inference v2 protocol is only aware of models, thus has no concept of pipelines.\nSeldon works around this limitation by introducing virtual endpoints for pipelines.\nVirtual means that Seldon understands them, but other v2 protocol-compatible components like inference servers do not.\nUse the following rules for paths to route to models and pipelines:\n\nFor models, use the path prefix /v2/models/{model name}.\nThis is normal usage of the inference v2 protocol.\nFor pipelines, you can use the path prefix /v2/pipelines/{pipeline name}.\nOtherwise calling pipelines looks just like the inference v2 protocol for models.\nDo not use any suffix for the pipeline name as you would for routing headers.\nFor pipelines, you can also use the path prefix /v2/models/{pipeline name}.pipeline.\nAgain, this form looks just like the inference v2 protocol for models.\n\n\nExtending our examples from above, the requests may look like the below when using header-based routing.\n\nSeldon CLIcURLgrpcurlPython tritonclientNo changes are required as the seldon CLI already understands how to set the appropriate gRPC and REST headers.\nNote the header in the last line:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \\\n        -H \"Seldon-Model: iris\"\n\n\nNote the rpc-header flag in the penultimate line:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t-rpc-header seldon-model:iris \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nNote the headers dictionary in the client.infer() call:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\n    \"iris\",\n    inputs,\n    headers={\"Seldon-Model\": \"iris\"},\n)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\n\nIngress Routes\u00b6\nIf you are using an ingress controller to make inference requests with Seldon, you will need to configure the routing rules correctly.\nThere are many ways to do this, but custom path prefixes will not work with gRPC.\nThis is because gRPC determines the path based on the Protobuf definition.\nSome gRPC implementations permit manipulating paths when sending requests, but this is by no means universal.\nIf you want to expose your inference endpoints via gRPC and REST in a consistent way, you should use virtual hosts, subdomains, or headers.\nThe downside of using only paths is that you cannot differentiate between different installations of Seldon Core v2 or between traffic to Seldon and any other inference endpoints you may have exposed via the same ingress.\nYou might want to use a mixture of these methods; the choice is yours.\n\nVirtual HostsSubdomainsHeadersPathsVirtual hosts are a way of differentiating between logical services accessed via the same physical machine(s).\nVirtual hosts are defined by the Host header for HTTP/1 and the :authority pseudo-header for HTTP/2.\nThese represent the same thing, and the HTTP/2 specification defines how to translate these when converting between protocol versions.\nMany tools and libraries treat these headers as special and have particular ways of handling them.\nSome common ones are given below:\n\nThe seldon CLI has an --authority flag which applies to both REST and gRPC inference calls.\ncurl accepts Host as a normal header.\ngrpcurl has an -authority flag.\nIn Go, the standard library\u2019s http.Request struct has a Host field and ignores attempts to set this value via headers.\nIn Python, the requests library accepts the host as a normal header.\n\nBe sure to check the documentation for how to set this with your preferred tools and languages.\nSubdomain names constitute a part of the overall host name.\nAs such, specifying a subdomain name for requests will involve setting the appropriate host in the URI.\nFor example, you may expose inference services in the namespaces seldon-1 and seldon-2 as in the following snippets:\ncurl https://seldon-1.example.com/v2/models/iris/infer ...\n\nseldon model infer --inference-host https://seldon-2.example.com/v2/models/iris/infer ...\n\n\nMany popular ingresses support subdomain-based routing, including Istio and Nginx.\nPlease refer to the documentation for your ingress of choice for further information.\nMany ingress controllers and service meshes support routing on headers.\nYou can use whatever headers you prefer, so long as they do not conflict with any Seldon relies upon.\nMany tools and libraries support adding custom headers to requests.\nSome common ones are given below:\n\nThe seldon CLI accepts headers using the --header flag, which can be specified multiple times.\ncurl accepts headers using the -H or --header flags.\ngrpcurl accepts headers using the -H flag, which can be specified multiple times.\n\nIt is possible to route on paths by using well-known path prefixes defined by the inference v2 protocol.\nFor gRPC, the full path (or \u201cmethod\u201d) for an inference call is:\n/inference.GRPCInferenceService/ModelInfer\n\n\nThis corresponds to the package (inference), service (GRPCInferenceService), and RPC name (ModelInfer) in the Protobuf definition of the inference v2 protocol.\nYou could use an exact match or a regex like .*inference.* to match this path, for example.\n\n\n\n", "find-the-seldon-service-endpoint": "\nFind the Seldon Service Endpoint\u00b6\n\nDocker ComposeKubernetesIn the default Docker Compose setup, container ports are accessible from the host machine.\nThis means you can use localhost or 0.0.0.0 as the hostname.\nThe default port for sending inference requests to the Seldon system is 9000.\nThis is controlled by the ENVOY_DATA_PORT environment variable for Compose.\nPutting this together, you can send inference requests to 0.0.0.0:9000.\nIn Kubernetes, Seldon creates a single Service called seldon-mesh in the namespace it is installed into.\nBy default, this namespace is also called seldon-mesh.\nIf this Service is exposed via a load balancer, the appropriate address and port can be found via:\nkubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n\n\nIf you are not using a LoadBalancer for the seldon-mesh Service, you can still send inference requests.\nFor development and testing purposes, you can port-forward the Service locally using the below.\nInference requests can then be sent to localhost:8080.\nkubectl port-forward svc/seldon-mesh -n seldon-mesh 8080:80\n\n\nIf you are using a service mesh like Istio or Ambassador, you will need to use the IP address of the service mesh ingress and determine the appropriate port.\n\n", "make-inference-requests": "\nMake Inference Requests\u00b6\nLet us imagine making inference requests to a model called iris.\nThis iris model has the following schema, which can be set in a model-settings.json file for MLServer:\n{\n    \"name\": \"iris\",\n    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n    \"inputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"FP32\",\n            \"shape\": [-1, 4]\n        }\n    ],\n    \"outputs\": [\n        {\n            \"name\": \"predict\",\n            \"datatype\": \"INT64\",\n            \"shape\": [-1, 1]\n        }\n    ],\n    \"parameters\": {\n        \"version\": \"1\"\n    }\n}\n\n\nExamples are given below for some common tools for making requests.\n\nSeldon CLIcURLgrpcurlPython tritonclientAn example seldon request might look like this:\nseldon model infer iris \\\n        '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nThe default inference mode is REST, but you can also send gRPC requests like this:\nseldon model infer iris \\\n        --inference-mode grpc \\\n        '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nAn example curl request might look like this:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nAn example grpcurl request might look like this:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nThe above request was run from the project root folder allowing reference to the Protobuf manifests defined in the apis/ folder.\nYou can use the Python tritonclient package to send inference requests.\nA short, self-contained example is:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\"iris\", inputs)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\nTip\nFor pipelines, a synchronous request is possible if the pipeline has an outputs section defined in its spec.\n\n", "request-routing": "\nRequest Routing\u00b6\n\nSeldon Routes\u00b6\nSeldon needs to determine where to route requests to, as models and pipelines might have the same name.\nThere are two ways of doing this: header-based routing (preferred) and path-based routing.\n\nHeadersPathsSeldon can route requests to the correct endpoint via headers in HTTP calls, both for REST (HTTP/1.1) and gRPC (HTTP/2).\nUse the Seldon-Model header as follows:\n\nFor models, use the model name as the value.\nFor example, to send requests to a model named foo use the header Seldon-Model: foo.\nFor pipelines, use the pipeline name followed by .pipeline as the value.\nFor example, to send requests to a pipeline named foo use the header Seldon-Model: foo.pipeline.\n\nThe seldon CLI is aware of these rules and can be used to easily send requests to your deployed resources.\nSee the examples and the Seldon CLI docs for more information.\nThe inference v2 protocol is only aware of models, thus has no concept of pipelines.\nSeldon works around this limitation by introducing virtual endpoints for pipelines.\nVirtual means that Seldon understands them, but other v2 protocol-compatible components like inference servers do not.\nUse the following rules for paths to route to models and pipelines:\n\nFor models, use the path prefix /v2/models/{model name}.\nThis is normal usage of the inference v2 protocol.\nFor pipelines, you can use the path prefix /v2/pipelines/{pipeline name}.\nOtherwise calling pipelines looks just like the inference v2 protocol for models.\nDo not use any suffix for the pipeline name as you would for routing headers.\nFor pipelines, you can also use the path prefix /v2/models/{pipeline name}.pipeline.\nAgain, this form looks just like the inference v2 protocol for models.\n\n\nExtending our examples from above, the requests may look like the below when using header-based routing.\n\nSeldon CLIcURLgrpcurlPython tritonclientNo changes are required as the seldon CLI already understands how to set the appropriate gRPC and REST headers.\nNote the header in the last line:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \\\n        -H \"Seldon-Model: iris\"\n\n\nNote the rpc-header flag in the penultimate line:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t-rpc-header seldon-model:iris \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nNote the headers dictionary in the client.infer() call:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\n    \"iris\",\n    inputs,\n    headers={\"Seldon-Model\": \"iris\"},\n)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n\n\nIngress Routes\u00b6\nIf you are using an ingress controller to make inference requests with Seldon, you will need to configure the routing rules correctly.\nThere are many ways to do this, but custom path prefixes will not work with gRPC.\nThis is because gRPC determines the path based on the Protobuf definition.\nSome gRPC implementations permit manipulating paths when sending requests, but this is by no means universal.\nIf you want to expose your inference endpoints via gRPC and REST in a consistent way, you should use virtual hosts, subdomains, or headers.\nThe downside of using only paths is that you cannot differentiate between different installations of Seldon Core v2 or between traffic to Seldon and any other inference endpoints you may have exposed via the same ingress.\nYou might want to use a mixture of these methods; the choice is yours.\n\nVirtual HostsSubdomainsHeadersPathsVirtual hosts are a way of differentiating between logical services accessed via the same physical machine(s).\nVirtual hosts are defined by the Host header for HTTP/1 and the :authority pseudo-header for HTTP/2.\nThese represent the same thing, and the HTTP/2 specification defines how to translate these when converting between protocol versions.\nMany tools and libraries treat these headers as special and have particular ways of handling them.\nSome common ones are given below:\n\nThe seldon CLI has an --authority flag which applies to both REST and gRPC inference calls.\ncurl accepts Host as a normal header.\ngrpcurl has an -authority flag.\nIn Go, the standard library\u2019s http.Request struct has a Host field and ignores attempts to set this value via headers.\nIn Python, the requests library accepts the host as a normal header.\n\nBe sure to check the documentation for how to set this with your preferred tools and languages.\nSubdomain names constitute a part of the overall host name.\nAs such, specifying a subdomain name for requests will involve setting the appropriate host in the URI.\nFor example, you may expose inference services in the namespaces seldon-1 and seldon-2 as in the following snippets:\ncurl https://seldon-1.example.com/v2/models/iris/infer ...\n\nseldon model infer --inference-host https://seldon-2.example.com/v2/models/iris/infer ...\n\n\nMany popular ingresses support subdomain-based routing, including Istio and Nginx.\nPlease refer to the documentation for your ingress of choice for further information.\nMany ingress controllers and service meshes support routing on headers.\nYou can use whatever headers you prefer, so long as they do not conflict with any Seldon relies upon.\nMany tools and libraries support adding custom headers to requests.\nSome common ones are given below:\n\nThe seldon CLI accepts headers using the --header flag, which can be specified multiple times.\ncurl accepts headers using the -H or --header flags.\ngrpcurl accepts headers using the -H flag, which can be specified multiple times.\n\nIt is possible to route on paths by using well-known path prefixes defined by the inference v2 protocol.\nFor gRPC, the full path (or \u201cmethod\u201d) for an inference call is:\n/inference.GRPCInferenceService/ModelInfer\n\n\nThis corresponds to the package (inference), service (GRPCInferenceService), and RPC name (ModelInfer) in the Protobuf definition of the inference v2 protocol.\nYou could use an exact match or a regex like .*inference.* to match this path, for example.\n\n\n", "seldon-routes": "\nSeldon Routes\u00b6\nSeldon needs to determine where to route requests to, as models and pipelines might have the same name.\nThere are two ways of doing this: header-based routing (preferred) and path-based routing.\n\nHeadersPathsSeldon can route requests to the correct endpoint via headers in HTTP calls, both for REST (HTTP/1.1) and gRPC (HTTP/2).\nUse the Seldon-Model header as follows:\n\nFor models, use the model name as the value.\nFor example, to send requests to a model named foo use the header Seldon-Model: foo.\nFor pipelines, use the pipeline name followed by .pipeline as the value.\nFor example, to send requests to a pipeline named foo use the header Seldon-Model: foo.pipeline.\n\nThe seldon CLI is aware of these rules and can be used to easily send requests to your deployed resources.\nSee the examples and the Seldon CLI docs for more information.\nThe inference v2 protocol is only aware of models, thus has no concept of pipelines.\nSeldon works around this limitation by introducing virtual endpoints for pipelines.\nVirtual means that Seldon understands them, but other v2 protocol-compatible components like inference servers do not.\nUse the following rules for paths to route to models and pipelines:\n\nFor models, use the path prefix /v2/models/{model name}.\nThis is normal usage of the inference v2 protocol.\nFor pipelines, you can use the path prefix /v2/pipelines/{pipeline name}.\nOtherwise calling pipelines looks just like the inference v2 protocol for models.\nDo not use any suffix for the pipeline name as you would for routing headers.\nFor pipelines, you can also use the path prefix /v2/models/{pipeline name}.pipeline.\nAgain, this form looks just like the inference v2 protocol for models.\n\n\nExtending our examples from above, the requests may look like the below when using header-based routing.\n\nSeldon CLIcURLgrpcurlPython tritonclientNo changes are required as the seldon CLI already understands how to set the appropriate gRPC and REST headers.\nNote the header in the last line:\ncurl -v http://0.0.0.0:9000/v2/models/iris/infer \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}' \\\n        -H \"Seldon-Model: iris\"\n\n\nNote the rpc-header flag in the penultimate line:\ngrpcurl \\\n\t-d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n\t-plaintext \\\n\t-import-path apis \\\n\t-proto apis/mlops/v2_dataplane/v2_dataplane.proto \\\n\t-rpc-header seldon-model:iris \\\n\t0.0.0.0:9000 inference.GRPCInferenceService/ModelInfer\n\n\nNote the headers dictionary in the client.infer() call:\nimport tritonclient.http as httpclient\nimport numpy as np\n\nclient = httpclient.InferenceServerClient(\n    url=\"localhost:8080\",\n    verbose=False,\n)\n\ninputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\ninputs[0].set_data_from_numpy(\n    np.array([[1, 2, 3, 4]]).astype(\"float64\"),\n    binary_data=False,\n)\n\nresult = client.infer(\n    \"iris\",\n    inputs,\n    headers={\"Seldon-Model\": \"iris\"},\n)\nprint(\"result is:\", result.as_numpy(\"predict\"))\n\n\n\n", "ingress-routes": "\nIngress Routes\u00b6\nIf you are using an ingress controller to make inference requests with Seldon, you will need to configure the routing rules correctly.\nThere are many ways to do this, but custom path prefixes will not work with gRPC.\nThis is because gRPC determines the path based on the Protobuf definition.\nSome gRPC implementations permit manipulating paths when sending requests, but this is by no means universal.\nIf you want to expose your inference endpoints via gRPC and REST in a consistent way, you should use virtual hosts, subdomains, or headers.\nThe downside of using only paths is that you cannot differentiate between different installations of Seldon Core v2 or between traffic to Seldon and any other inference endpoints you may have exposed via the same ingress.\nYou might want to use a mixture of these methods; the choice is yours.\n\nVirtual HostsSubdomainsHeadersPathsVirtual hosts are a way of differentiating between logical services accessed via the same physical machine(s).\nVirtual hosts are defined by the Host header for HTTP/1 and the :authority pseudo-header for HTTP/2.\nThese represent the same thing, and the HTTP/2 specification defines how to translate these when converting between protocol versions.\nMany tools and libraries treat these headers as special and have particular ways of handling them.\nSome common ones are given below:\n\nThe seldon CLI has an --authority flag which applies to both REST and gRPC inference calls.\ncurl accepts Host as a normal header.\ngrpcurl has an -authority flag.\nIn Go, the standard library\u2019s http.Request struct has a Host field and ignores attempts to set this value via headers.\nIn Python, the requests library accepts the host as a normal header.\n\nBe sure to check the documentation for how to set this with your preferred tools and languages.\nSubdomain names constitute a part of the overall host name.\nAs such, specifying a subdomain name for requests will involve setting the appropriate host in the URI.\nFor example, you may expose inference services in the namespaces seldon-1 and seldon-2 as in the following snippets:\ncurl https://seldon-1.example.com/v2/models/iris/infer ...\n\nseldon model infer --inference-host https://seldon-2.example.com/v2/models/iris/infer ...\n\n\nMany popular ingresses support subdomain-based routing, including Istio and Nginx.\nPlease refer to the documentation for your ingress of choice for further information.\nMany ingress controllers and service meshes support routing on headers.\nYou can use whatever headers you prefer, so long as they do not conflict with any Seldon relies upon.\nMany tools and libraries support adding custom headers to requests.\nSome common ones are given below:\n\nThe seldon CLI accepts headers using the --header flag, which can be specified multiple times.\ncurl accepts headers using the -H or --header flags.\ngrpcurl accepts headers using the -H flag, which can be specified multiple times.\n\nIt is possible to route on paths by using well-known path prefixes defined by the inference v2 protocol.\nFor gRPC, the full path (or \u201cmethod\u201d) for an inference call is:\n/inference.GRPCInferenceService/ModelInfer\n\n\nThis corresponds to the package (inference), service (GRPCInferenceService), and RPC name (ModelInfer) in the Protobuf definition of the inference v2 protocol.\nYou could use an exact match or a regex like .*inference.* to match this path, for example.\n\n", "asynchronous-requests": "\nAsynchronous Requests\u00b6\nThe Seldon architecture uses Kafka and therefore asynchronous requests can be sent by pushing inference v2 protocol payloads to the appropriate topic.\nTopics have the following form:\nseldon.<namespace>.<model|pipeline>.<name>.<inputs|outputs>\n\n\n\nNote\nIf writing to a pipeline topic, you will need to include a Kafka header with the key pipeline and the value being the name of the pipeline.\n\n\nModel Inference\u00b6\nFor a local install if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.default.model.iris.inputs.\nThe response will appear on seldon.default.model.iris.outputs.\nFor a Kubernetes install in seldon-mesh if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.model.iris.inputs.\nThe response will appear on seldon.seldon-mesh.model.iris.outputs.\n\n\nPipeline Inference\u00b6\nFor a local install if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.default.pipeline.mypipeline.inputs. The response will appear on seldon.default.pipeline.mypipeline.outputs.\nFor a Kubernetes install in seldon-mesh if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.pipeline.mypipeline.inputs. The response will appear on seldon.seldon-mesh.pipeline.mypipeline.outputs.\n\n", "model-inference": "\nModel Inference\u00b6\nFor a local install if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.default.model.iris.inputs.\nThe response will appear on seldon.default.model.iris.outputs.\nFor a Kubernetes install in seldon-mesh if you have a model iris, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.model.iris.inputs.\nThe response will appear on seldon.seldon-mesh.model.iris.outputs.\n", "pipeline-inference": "\nPipeline Inference\u00b6\nFor a local install if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.default.pipeline.mypipeline.inputs. The response will appear on seldon.default.pipeline.mypipeline.outputs.\nFor a Kubernetes install in seldon-mesh if you have a pipeline mypipeline, you would be able to send a prediction request by pushing to the topic: seldon.seldon-mesh.pipeline.mypipeline.inputs. The response will appear on seldon.seldon-mesh.pipeline.mypipeline.outputs.\n", "pipeline-metadata": "\nPipeline Metadata\u00b6\nIt may be useful to send metadata alongside your inference.\nIf using Kafka directly as described above, you can attach Kafka metadata to your request, which will be passed around the graph.\nWhen making synchronous requests to your pipeline with REST or gRPC you can also do this.\n\nFor REST requests add HTTP headers prefixed with X-\nFor gRPC requests add metadata with keys starting with X-\n\nYou can also do this with the Seldon CLI by setting headers with the --header argument (and also showing response headers with the --show-headers argument)\nseldon pipeline infer --show-headers --header X-foo=bar tfsimples \\\n    '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'\n\n\n", "request-ids": "\nRequest IDs\u00b6\nFor both model and pipeline requests the response will contain a x-request-id response header. For pipeline requests this can be used to inspect the pipeline steps via the CLI, e.g.:\nseldon pipeline inspect tfsimples --request-id carjjolvqj3j2pfbut10 --offset 10\n\n\nThe --offset parameter specifies how many messages (from the latest) you want to search to find your request. If not specified the last request will be shown.\nx-request-id will also appear in tracing spans.\nIf x-request-id is passed in by the caller then this will be used. It is the caller\u2019s responsibility to ensure it is unique.\nThe IDs generated are XIDs.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/inference/index.html", "key": "inference"}}, "getting-started/kubernetes-installation/security": {"sections": {"security": "\nSecurity\u00b6\nSeldon can be run with secure control plane and data plane operations. There are three areas of concern:\n\nControl Plane\nKafka\nData Plane\n\nThe various communication points between services are shown in the diagram below:\n\n\nControl Plane\u00b6\nTLS control plane activation is switched on and off via the environment variable: CONTROL_PLANE_SECURITY_PROTOCOL whose values can be PLAINTEXT or SSL.\nCertificates will be loaded and used for the control plane gRPC services. The secrets or folders will be watched for updates (on certificate renewal) and automatically loaded again.\n\nHelm Control Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set the secret names for your certificates. If using cert-manager example discussed below this would be as follows:\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n      --set security.controlplane.protocol=SSL\n\n\n\n\n\nKafka\u00b6\nKafka secure activation is switched on and off via the environment variable: KAFKA_SECURITY_PROTOCOL whose values can be PLAINTEXT, SSL or SASL_SSL.\nExamples are shown below:\n\nmTLS Strimzi example\nmTLS AWS MSK example\nSASL PLAIN with Confluent Cloud example\nSASL PLAIN with Azure Event Hub example\nSASL SCRAM with Strimzi example\nSASL SCRAM with AWS MSK example\nSASL OAUTH with Confluent Cloud example\n\n\n\nData Plane\u00b6\nTLS Data plane activation is switched on and off via the environment variable: ENVOY_SECURITY_PROTOCOL whose values can be PLAINTEXT or SSL.\nWhen activated this ensures TLS is used to communicate to Envoy via the xDS server as well as using the SDS service to send cretificates to envoy to use for  upstream and downstream networking. Downstream is the external access to Seldon and upstream is the path from Envoy to the model servers or pipeline gateway.\n\nHelm Data Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set data plane operations to TLS as below. This assumes the secrets installed by the helm chart at the end of this section.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh --set security.envoy.protocol=SSL\n\n\nThe above uses default secret names defined for the certificates installed. You can change the names of the required certificate secrets as shown in a longer configuration below (again using the default names for illustration).\nFor this we use the following updated Helm values (k8s/samples/values-tls-dataplane-example.yaml):\nsecurity:\n  controlplane:\n    protocol: PLAINTEXT\n    ssl:\n      server:\n        secret: seldon-controlplane-server\n        clientValidationSecret: seldon-controlplane-client\n      client:\n        secret: seldon-controlplane-client\n        serverValidationSecret: seldon-controlplane-server\n  envoy:\n    protocol: SSL\n    ssl:\n      upstream:\n        server:\n          secret: seldon-upstream-server\n          clientValidationSecret: seldon-upstream-client\n        client:\n          secret: seldon-upstream-client\n          serverValidationSecret: seldon-upstream-server\n      downstream:\n        client:\n          serverValidationSecret: seldon-downstream-server\n        server: \n          secret: seldon-downstream-server\n\n\nWe use Envoy internally to direct traffic and in Envoy\u2019s terminology upstream is for internal model servers called from Envoy while the downstream server is the entrypoint server running in Envoy to receive grpc and REST calls. The above settings ensure mTLS for internal \u201cupstream\u201d traffic while provides a standard SSL non-mTLS entrpoint.\nTo use the above with the seldon CLI you would need a custom config file as follow:\n{\n    \"dataplane\": {\n\t\"tls\": true,\n\t\"skipSSLVerify\": true\n    }\n}\n\n\nWe skip SSL Verify as these are internal self-signed certificates. For production use you would change this to the correct DNS name you are exposing the Seldon entrypoint.\n\n\n\nCetificate Providers\u00b6\nThe installer/cluster controller for Seldon needs to provide the certificates. As part of Seldon we provide an example set of certificate issuers and certificates using cert-manager.\n\nHelm\u00b6\nYou can install Certificates into the desired namespace, here we use seldon-mesh as an example.\nhelm install seldon-v2-certs k8s/helm-charts/seldon-core-v2-certs/ -n seldon-mesh\n\n\n\n\n\n\n", "control-plane": "\nControl Plane\u00b6\nTLS control plane activation is switched on and off via the environment variable: CONTROL_PLANE_SECURITY_PROTOCOL whose values can be PLAINTEXT or SSL.\nCertificates will be loaded and used for the control plane gRPC services. The secrets or folders will be watched for updates (on certificate renewal) and automatically loaded again.\n\nHelm Control Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set the secret names for your certificates. If using cert-manager example discussed below this would be as follows:\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n      --set security.controlplane.protocol=SSL\n\n\n\n", "helm-control-plane-install": "\nHelm Control Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set the secret names for your certificates. If using cert-manager example discussed below this would be as follows:\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n      --set security.controlplane.protocol=SSL\n\n\n", "kafka": "\nKafka\u00b6\nKafka secure activation is switched on and off via the environment variable: KAFKA_SECURITY_PROTOCOL whose values can be PLAINTEXT, SSL or SASL_SSL.\nExamples are shown below:\n\nmTLS Strimzi example\nmTLS AWS MSK example\nSASL PLAIN with Confluent Cloud example\nSASL PLAIN with Azure Event Hub example\nSASL SCRAM with Strimzi example\nSASL SCRAM with AWS MSK example\nSASL OAUTH with Confluent Cloud example\n\n", "data-plane": "\nData Plane\u00b6\nTLS Data plane activation is switched on and off via the environment variable: ENVOY_SECURITY_PROTOCOL whose values can be PLAINTEXT or SSL.\nWhen activated this ensures TLS is used to communicate to Envoy via the xDS server as well as using the SDS service to send cretificates to envoy to use for  upstream and downstream networking. Downstream is the external access to Seldon and upstream is the path from Envoy to the model servers or pipeline gateway.\n\nHelm Data Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set data plane operations to TLS as below. This assumes the secrets installed by the helm chart at the end of this section.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh --set security.envoy.protocol=SSL\n\n\nThe above uses default secret names defined for the certificates installed. You can change the names of the required certificate secrets as shown in a longer configuration below (again using the default names for illustration).\nFor this we use the following updated Helm values (k8s/samples/values-tls-dataplane-example.yaml):\nsecurity:\n  controlplane:\n    protocol: PLAINTEXT\n    ssl:\n      server:\n        secret: seldon-controlplane-server\n        clientValidationSecret: seldon-controlplane-client\n      client:\n        secret: seldon-controlplane-client\n        serverValidationSecret: seldon-controlplane-server\n  envoy:\n    protocol: SSL\n    ssl:\n      upstream:\n        server:\n          secret: seldon-upstream-server\n          clientValidationSecret: seldon-upstream-client\n        client:\n          secret: seldon-upstream-client\n          serverValidationSecret: seldon-upstream-server\n      downstream:\n        client:\n          serverValidationSecret: seldon-downstream-server\n        server: \n          secret: seldon-downstream-server\n\n\nWe use Envoy internally to direct traffic and in Envoy\u2019s terminology upstream is for internal model servers called from Envoy while the downstream server is the entrypoint server running in Envoy to receive grpc and REST calls. The above settings ensure mTLS for internal \u201cupstream\u201d traffic while provides a standard SSL non-mTLS entrpoint.\nTo use the above with the seldon CLI you would need a custom config file as follow:\n{\n    \"dataplane\": {\n\t\"tls\": true,\n\t\"skipSSLVerify\": true\n    }\n}\n\n\nWe skip SSL Verify as these are internal self-signed certificates. For production use you would change this to the correct DNS name you are exposing the Seldon entrypoint.\n\n", "helm-data-plane-install": "\nHelm Data Plane Install\u00b6\nWhen installing seldon-core-v2-setup you can set data plane operations to TLS as below. This assumes the secrets installed by the helm chart at the end of this section.\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh --set security.envoy.protocol=SSL\n\n\nThe above uses default secret names defined for the certificates installed. You can change the names of the required certificate secrets as shown in a longer configuration below (again using the default names for illustration).\nFor this we use the following updated Helm values (k8s/samples/values-tls-dataplane-example.yaml):\nsecurity:\n  controlplane:\n    protocol: PLAINTEXT\n    ssl:\n      server:\n        secret: seldon-controlplane-server\n        clientValidationSecret: seldon-controlplane-client\n      client:\n        secret: seldon-controlplane-client\n        serverValidationSecret: seldon-controlplane-server\n  envoy:\n    protocol: SSL\n    ssl:\n      upstream:\n        server:\n          secret: seldon-upstream-server\n          clientValidationSecret: seldon-upstream-client\n        client:\n          secret: seldon-upstream-client\n          serverValidationSecret: seldon-upstream-server\n      downstream:\n        client:\n          serverValidationSecret: seldon-downstream-server\n        server: \n          secret: seldon-downstream-server\n\n\nWe use Envoy internally to direct traffic and in Envoy\u2019s terminology upstream is for internal model servers called from Envoy while the downstream server is the entrypoint server running in Envoy to receive grpc and REST calls. The above settings ensure mTLS for internal \u201cupstream\u201d traffic while provides a standard SSL non-mTLS entrpoint.\nTo use the above with the seldon CLI you would need a custom config file as follow:\n{\n    \"dataplane\": {\n\t\"tls\": true,\n\t\"skipSSLVerify\": true\n    }\n}\n\n\nWe skip SSL Verify as these are internal self-signed certificates. For production use you would change this to the correct DNS name you are exposing the Seldon entrypoint.\n", "cetificate-providers": "\nCetificate Providers\u00b6\nThe installer/cluster controller for Seldon needs to provide the certificates. As part of Seldon we provide an example set of certificate issuers and certificates using cert-manager.\n\nHelm\u00b6\nYou can install Certificates into the desired namespace, here we use seldon-mesh as an example.\nhelm install seldon-v2-certs k8s/helm-charts/seldon-core-v2-certs/ -n seldon-mesh\n\n\n\n\n\n", "helm": "\nHelm\u00b6\nYou can install Certificates into the desired namespace, here we use seldon-mesh as an example.\nhelm install seldon-v2-certs k8s/helm-charts/seldon-core-v2-certs/ -n seldon-mesh\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/index.html", "key": "getting-started/kubernetes-installation/security"}}, "examples/model-zoo": {"sections": {"model-zoo": "\nModel Zoo\u00b6\nRun these examples from the samples folder.\n\nSeldon Model Zoo\u00b6\nExamples of various model artifact types from various frameworks running under Seldon Core V2.\n\nSKlearn\nTensorflow\nXGBoost\nONNX\nLightgbm\nMLFlow\nPyTorch\n\nPython requirements in model-zoo-requirements.txt\n\nSKLearn Iris Classification Model\u00b6\nThe training code for this model can be found at ./scripts/models/iris\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"09263298-ca66-49c5-acb9-0ca75b06f825\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload iris\n\n\n{}\n\n\n\n\nTensorflow CIFAR10 Image Classification Model\u00b6\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-03-09 19:43:43.637892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-09 19:43:43.637906: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\ndef infer(resourceName: str, idx: int):\n    rows = X_train[idx:idx+1]\n    show(rows[0])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(classes[probs.argmax(axis=0)])\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\ncat ./models/cifar10-no-config.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/tensorflow/cifar10\"\n  requirements:\n  - tensorflow\n\n\nseldon model load -f ./models/cifar10-no-config.yaml\n\n\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer(\"cifar10\",4)\n\n\n![png](model-zoo_files/model-zoo_14_0.png)\n\n\n\ncar\n\n\n\nseldon model unload cifar10\n\n\n{}\n\n\n\n\nXGBoost Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-xgb\ncat ./models/income-xgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-xgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-xgb\"\n  requirements:\n  - xgboost\n\n\nseldon model load -f ./models/income-xgb.yaml\n\n\n{}\n\n\nseldon model status income-xgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-xgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-xgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"e30c3b44-fa14-4e5f-88f5-d6f4d287da20\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP32\",\n\t\t\t\"data\": [\n\t\t\t\t-1.8380107879638672\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-xgb\n\n\n{}\n\n\n\n\n\nONNX MNIST Model\u00b6\nThis model is a pretrained model as defined in ./scripts/models/Makefile target mnist-onnx\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\ntraining_data = MNIST(\n    root=\".\",\n    download=True,\n    train=False,\n    transform = transforms.Compose([\n              transforms.ToTensor()\n          ])\n)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"Input3\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndl = DataLoader(training_data, batch_size=1, shuffle=False)\ndlIter = iter(dl)\n\ndef infer_mnist():\n    x, y = next(dlIter)\n    data = x.cpu().numpy()\n    reqJson[\"inputs\"][0][\"data\"] = data.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 1, 28, 28]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":\"mnist-onnx\"}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    show_mnist(x)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(probs.argmax(axis=0))\n\n\ndef show_mnist(X):\n    plt.imshow(X.reshape(28, 28))\n    plt.axis(\"off\")\n    plt.show()\n\n\ncat ./models/mnist-onnx.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mnist-onnx\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mnist-onnx\"\n  requirements:\n  - onnx\n\n\nseldon model load -f ./models/mnist-onnx.yaml\n\n\n{}\n\n\nseldon model status mnist-onnx -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer_mnist()\n\n\n![png](model-zoo_files/model-zoo_28_0.png)\n\n\n\n7\n\n\n\nseldon model unload mnist-onnx\n\n\n{}\n\n\n\nLightGBM Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-lgb\ncat ./models/income-lgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-lgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-lgb\"\n  requirements:\n  - lightgbm\n\n\nseldon model load -f ./models/income-lgb.yaml\n\n\n{}\n\n\nseldon model status income-lgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-lgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-lgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"4437a71e-9af1-4e3b-aa4b-cb95d2cd86b9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP64\",\n\t\t\t\"data\": [\n\t\t\t\t0.06279460120044741\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-lgb\n\n\n{}\n\n\n\n\nMLFlow Wine Model\u00b6\nThe training code for this model can be found at ./scripts/models/wine-mlflow\ncat ./models/wine-mlflow.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: wine\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/wine-mlflow\"\n  requirements:\n  - mlflow\n\n\nseldon model load -f ./models/wine-mlflow.yaml\n\n\n{}\n\n\nseldon model status wine -w ModelAvailable | jq -M .\n\n\n{}\n\n\nimport requests\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ninference_request = {\n    \"inputs\": [\n        {\n          \"name\": \"fixed acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [7.4],\n        },\n        {\n          \"name\": \"volatile acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.7000],\n        },\n        {\n          \"name\": \"citric acid\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0],\n        },\n        {\n          \"name\": \"residual sugar\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [1.9],\n        },\n        {\n          \"name\": \"chlorides\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.076],\n        },\n        {\n          \"name\": \"free sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [11],\n        },\n        {\n          \"name\": \"total sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [34],\n        },\n        {\n          \"name\": \"density\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.9978],\n        },\n        {\n          \"name\": \"pH\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [3.51],\n        },\n        {\n          \"name\": \"sulphates\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.56],\n        },\n        {\n          \"name\": \"alcohol\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [9.4],\n        },\n    ]\n}\nheaders = {\"Content-Type\": \"application/json\", \"seldon-model\":\"wine\"}\nresponse_raw = requests.post(url, json=inference_request, headers=headers)\nprint(response_raw.json())\n\n\n{'model_name': 'wine_1', 'model_version': '1', 'id': '0d7e44f8-b46c-4438-b8af-a749e6aa6039', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [1, 1], 'datatype': 'FP64', 'data': [5.576883936610762]}]}\n\n\n\nseldon model unload wine\n\n\n{}\n\n\n\n\n\nPytorch MNIST Model\u00b6\nThis example model is downloaded and trained in ./scripts/models/Makefile target mnist-pytorch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\ntraining_data = MNIST(\n    root=\".\",\n    download=True,\n    train=False,\n    transform = transforms.Compose([\n              transforms.ToTensor()\n          ])\n)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"x__0\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndl = DataLoader(training_data, batch_size=1, shuffle=False)\ndlIter = iter(dl)\n\ndef infer_mnist():\n    x, y = next(dlIter)\n    data = x.cpu().numpy()\n    reqJson[\"inputs\"][0][\"data\"] = data.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 1, 28, 28]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":\"mnist-pytorch\"}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    show_mnist(x)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(probs.argmax(axis=0))\n\n\ndef show_mnist(X):\n    plt.imshow(X.reshape(28, 28))\n    plt.axis(\"off\")\n    plt.show()\n\n\ncat ./models/mnist-pytorch.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mnist-pytorch\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mnist-pytorch\"\n  requirements:\n  - pytorch\n\n\nseldon model load -f ./models/mnist-pytorch.yaml\n\n\n{}\n\n\nseldon model status mnist-pytorch -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer_mnist()\n\n\n![png](model-zoo_files/model-zoo_48_0.png)\n\n\n\n7\n\n\n\nseldon model unload mnist-pytorch\n\n\n{}\n\n\n\n\n\n\n", "seldon-model-zoo": "\nSeldon Model Zoo\u00b6\nExamples of various model artifact types from various frameworks running under Seldon Core V2.\n\nSKlearn\nTensorflow\nXGBoost\nONNX\nLightgbm\nMLFlow\nPyTorch\n\nPython requirements in model-zoo-requirements.txt\n\nSKLearn Iris Classification Model\u00b6\nThe training code for this model can be found at ./scripts/models/iris\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"09263298-ca66-49c5-acb9-0ca75b06f825\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload iris\n\n\n{}\n\n\n\n\nTensorflow CIFAR10 Image Classification Model\u00b6\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-03-09 19:43:43.637892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-09 19:43:43.637906: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\ndef infer(resourceName: str, idx: int):\n    rows = X_train[idx:idx+1]\n    show(rows[0])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(classes[probs.argmax(axis=0)])\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\ncat ./models/cifar10-no-config.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/tensorflow/cifar10\"\n  requirements:\n  - tensorflow\n\n\nseldon model load -f ./models/cifar10-no-config.yaml\n\n\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer(\"cifar10\",4)\n\n\n![png](model-zoo_files/model-zoo_14_0.png)\n\n\n\ncar\n\n\n\nseldon model unload cifar10\n\n\n{}\n\n\n\n\nXGBoost Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-xgb\ncat ./models/income-xgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-xgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-xgb\"\n  requirements:\n  - xgboost\n\n\nseldon model load -f ./models/income-xgb.yaml\n\n\n{}\n\n\nseldon model status income-xgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-xgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-xgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"e30c3b44-fa14-4e5f-88f5-d6f4d287da20\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP32\",\n\t\t\t\"data\": [\n\t\t\t\t-1.8380107879638672\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-xgb\n\n\n{}\n\n\n\n", "sklearn-iris-classification-model": "\nSKLearn Iris Classification Model\u00b6\nThe training code for this model can be found at ./scripts/models/iris\ncat ./models/sklearn-iris-gs.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nseldon model load -f ./models/sklearn-iris-gs.yaml\n\n\n{}\n\n\nseldon model status iris -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer iris \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"09263298-ca66-49c5-acb9-0ca75b06f825\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload iris\n\n\n{}\n\n\n", "tensorflow-cifar10-image-classification-model": "\nTensorflow CIFAR10 Image Classification Model\u00b6\nimport requests\nimport json\nfrom typing import Dict, List\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.datasets import fetch_cifar10c\nimport matplotlib.pyplot as plt\ntf.keras.backend.clear_session()\n\n\n2023-03-09 19:43:43.637892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-09 19:43:43.637906: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n\n\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\ndef infer(resourceName: str, idx: int):\n    rows = X_train[idx:idx+1]\n    show(rows[0])\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 32, 32, 3]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(classes[probs.argmax(axis=0)])\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n\n\n\ncat ./models/cifar10-no-config.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/tensorflow/cifar10\"\n  requirements:\n  - tensorflow\n\n\nseldon model load -f ./models/cifar10-no-config.yaml\n\n\n{}\n\n\nseldon model status cifar10 -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer(\"cifar10\",4)\n\n\n![png](model-zoo_files/model-zoo_14_0.png)\n\n\n\ncar\n\n\n\nseldon model unload cifar10\n\n\n{}\n\n\n", "xgboost-model": "\nXGBoost Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-xgb\ncat ./models/income-xgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-xgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-xgb\"\n  requirements:\n  - xgboost\n\n\nseldon model load -f ./models/income-xgb.yaml\n\n\n{}\n\n\nseldon model status income-xgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-xgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-xgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"e30c3b44-fa14-4e5f-88f5-d6f4d287da20\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP32\",\n\t\t\t\"data\": [\n\t\t\t\t-1.8380107879638672\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-xgb\n\n\n{}\n\n\n", "onnx-mnist-model": "\nONNX MNIST Model\u00b6\nThis model is a pretrained model as defined in ./scripts/models/Makefile target mnist-onnx\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\ntraining_data = MNIST(\n    root=\".\",\n    download=True,\n    train=False,\n    transform = transforms.Compose([\n              transforms.ToTensor()\n          ])\n)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"Input3\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndl = DataLoader(training_data, batch_size=1, shuffle=False)\ndlIter = iter(dl)\n\ndef infer_mnist():\n    x, y = next(dlIter)\n    data = x.cpu().numpy()\n    reqJson[\"inputs\"][0][\"data\"] = data.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 1, 28, 28]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":\"mnist-onnx\"}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    show_mnist(x)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(probs.argmax(axis=0))\n\n\ndef show_mnist(X):\n    plt.imshow(X.reshape(28, 28))\n    plt.axis(\"off\")\n    plt.show()\n\n\ncat ./models/mnist-onnx.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mnist-onnx\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mnist-onnx\"\n  requirements:\n  - onnx\n\n\nseldon model load -f ./models/mnist-onnx.yaml\n\n\n{}\n\n\nseldon model status mnist-onnx -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer_mnist()\n\n\n![png](model-zoo_files/model-zoo_28_0.png)\n\n\n\n7\n\n\n\nseldon model unload mnist-onnx\n\n\n{}\n\n\n\nLightGBM Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-lgb\ncat ./models/income-lgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-lgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-lgb\"\n  requirements:\n  - lightgbm\n\n\nseldon model load -f ./models/income-lgb.yaml\n\n\n{}\n\n\nseldon model status income-lgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-lgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-lgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"4437a71e-9af1-4e3b-aa4b-cb95d2cd86b9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP64\",\n\t\t\t\"data\": [\n\t\t\t\t0.06279460120044741\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-lgb\n\n\n{}\n\n\n\n\nMLFlow Wine Model\u00b6\nThe training code for this model can be found at ./scripts/models/wine-mlflow\ncat ./models/wine-mlflow.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: wine\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/wine-mlflow\"\n  requirements:\n  - mlflow\n\n\nseldon model load -f ./models/wine-mlflow.yaml\n\n\n{}\n\n\nseldon model status wine -w ModelAvailable | jq -M .\n\n\n{}\n\n\nimport requests\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ninference_request = {\n    \"inputs\": [\n        {\n          \"name\": \"fixed acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [7.4],\n        },\n        {\n          \"name\": \"volatile acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.7000],\n        },\n        {\n          \"name\": \"citric acid\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0],\n        },\n        {\n          \"name\": \"residual sugar\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [1.9],\n        },\n        {\n          \"name\": \"chlorides\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.076],\n        },\n        {\n          \"name\": \"free sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [11],\n        },\n        {\n          \"name\": \"total sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [34],\n        },\n        {\n          \"name\": \"density\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.9978],\n        },\n        {\n          \"name\": \"pH\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [3.51],\n        },\n        {\n          \"name\": \"sulphates\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.56],\n        },\n        {\n          \"name\": \"alcohol\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [9.4],\n        },\n    ]\n}\nheaders = {\"Content-Type\": \"application/json\", \"seldon-model\":\"wine\"}\nresponse_raw = requests.post(url, json=inference_request, headers=headers)\nprint(response_raw.json())\n\n\n{'model_name': 'wine_1', 'model_version': '1', 'id': '0d7e44f8-b46c-4438-b8af-a749e6aa6039', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [1, 1], 'datatype': 'FP64', 'data': [5.576883936610762]}]}\n\n\n\nseldon model unload wine\n\n\n{}\n\n\n\n", "lightgbm-model": "\nLightGBM Model\u00b6\nThe training code for this model can be found at ./scripts/models/income-lgb\ncat ./models/income-lgb.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-lgb\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/income-lgb\"\n  requirements:\n  - lightgbm\n\n\nseldon model load -f ./models/income-lgb.yaml\n\n\n{}\n\n\nseldon model status income-lgb -w ModelAvailable | jq -M .\n\n\n{}\n\n\nseldon model infer income-lgb \\\n  '{ \"parameters\": {\"content_type\": \"pd\"}, \"inputs\": [{\"name\": \"Age\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [47]},{\"name\": \"Workclass\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Education\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Marital Status\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Occupation\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Relationship\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [3]},{\"name\": \"Race\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [4]},{\"name\": \"Sex\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [1]},{\"name\": \"Capital Gain\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Capital Loss\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [0]},{\"name\": \"Hours per week\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [40]},{\"name\": \"Country\", \"shape\": [1, 1], \"datatype\": \"INT64\", \"data\": [9]}]}'\n\n\n{\n\t\"model_name\": \"income-lgb_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"4437a71e-9af1-4e3b-aa4b-cb95d2cd86b9\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"FP64\",\n\t\t\t\"data\": [\n\t\t\t\t0.06279460120044741\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-lgb\n\n\n{}\n\n\n", "mlflow-wine-model": "\nMLFlow Wine Model\u00b6\nThe training code for this model can be found at ./scripts/models/wine-mlflow\ncat ./models/wine-mlflow.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: wine\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/wine-mlflow\"\n  requirements:\n  - mlflow\n\n\nseldon model load -f ./models/wine-mlflow.yaml\n\n\n{}\n\n\nseldon model status wine -w ModelAvailable | jq -M .\n\n\n{}\n\n\nimport requests\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ninference_request = {\n    \"inputs\": [\n        {\n          \"name\": \"fixed acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [7.4],\n        },\n        {\n          \"name\": \"volatile acidity\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.7000],\n        },\n        {\n          \"name\": \"citric acid\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0],\n        },\n        {\n          \"name\": \"residual sugar\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [1.9],\n        },\n        {\n          \"name\": \"chlorides\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.076],\n        },\n        {\n          \"name\": \"free sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [11],\n        },\n        {\n          \"name\": \"total sulfur dioxide\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [34],\n        },\n        {\n          \"name\": \"density\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.9978],\n        },\n        {\n          \"name\": \"pH\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [3.51],\n        },\n        {\n          \"name\": \"sulphates\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [0.56],\n        },\n        {\n          \"name\": \"alcohol\",\n          \"shape\": [1],\n          \"datatype\": \"FP32\",\n          \"data\": [9.4],\n        },\n    ]\n}\nheaders = {\"Content-Type\": \"application/json\", \"seldon-model\":\"wine\"}\nresponse_raw = requests.post(url, json=inference_request, headers=headers)\nprint(response_raw.json())\n\n\n{'model_name': 'wine_1', 'model_version': '1', 'id': '0d7e44f8-b46c-4438-b8af-a749e6aa6039', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [1, 1], 'datatype': 'FP64', 'data': [5.576883936610762]}]}\n\n\n\nseldon model unload wine\n\n\n{}\n\n\n", "pytorch-mnist-model": "\nPytorch MNIST Model\u00b6\nThis example model is downloaded and trained in ./scripts/models/Makefile target mnist-pytorch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\ntraining_data = MNIST(\n    root=\".\",\n    download=True,\n    train=False,\n    transform = transforms.Compose([\n              transforms.ToTensor()\n          ])\n)\n\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"x__0\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\ndl = DataLoader(training_data, batch_size=1, shuffle=False)\ndlIter = iter(dl)\n\ndef infer_mnist():\n    x, y = next(dlIter)\n    data = x.cpu().numpy()\n    reqJson[\"inputs\"][0][\"data\"] = data.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [1, 1, 28, 28]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":\"mnist-pytorch\"}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    show_mnist(x)\n    probs = np.array(response_raw.json()[\"outputs\"][0][\"data\"])\n    print(probs.argmax(axis=0))\n\n\ndef show_mnist(X):\n    plt.imshow(X.reshape(28, 28))\n    plt.axis(\"off\")\n    plt.show()\n\n\ncat ./models/mnist-pytorch.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mnist-pytorch\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mnist-pytorch\"\n  requirements:\n  - pytorch\n\n\nseldon model load -f ./models/mnist-pytorch.yaml\n\n\n{}\n\n\nseldon model status mnist-pytorch -w ModelAvailable | jq -M .\n\n\n{}\n\n\ninfer_mnist()\n\n\n![png](model-zoo_files/model-zoo_48_0.png)\n\n\n\n7\n\n\n\nseldon model unload mnist-pytorch\n\n\n{}\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/model-zoo.html", "key": "examples/model-zoo"}}, "kubernetes/autoscaling": {"sections": {"autoscaling": "\nAutoscaling\u00b6\nAutoscaling in Seldon applies to various concerns:\n\nInference servers autoscaling\nModel autoscaling\nModel memory overcommit\n\n\nInference servers autoscaling\u00b6\nAutoscaling of servers can be done via HorizontalPodAutoscaler (HPA).\nHPA can be applied to any deployed Server resource.\nIn this case HPA will manage the number of server replicas in the corresponding statefulset according to utilisation metrics  (e.g. CPU or memory).\nFor example assuming that a triton server is deployed, then the user can attach an HPA based on cpu utilisation as follows:\nkubectl autoscale server triton --cpu-percent=50 --min=1 --max=5\n\n\nIn this case, according to load, the system will add / remove server replicas to / from the triton statefulset.\nIt is worth considering the following points:\n\nIf HPA adds a new server replica, this new replica will be included in any future scheduling decisions.\nIn other words, when deploying a new model or rescheduling failed models this new replica will be considered.\nIf HPA deletes an existing server replica, the scheduler will first attempt to drain any loaded model on this server replica before the server replica gets actually deleted. This is achieved by leveraging a PreStop hook on the server replica pod that triggers a process before receiving the termination signal. This draining process is capped by terminationGracePeriodSeconds, which the user can set (default is 2 minutes).\n\nTherefore there should generally be minimal disruption to the inference workload during scaling.\nFor more details on HPA check this Kubernetes walk-through.\n\nNote\nAutoscaling of inference servers via seldon-scheduler is under consideration for the roadmap. This allow for more fine grained interactions with model autoscaling.\n\n\n\nModel autoscaling\u00b6\nAs each model server can serve multiple models, models can scale across the available replicas of the server according to load.\nAutoscaling of models is enabled if at least MinReplicas or MaxReplicas is set in the model custom resource. Then according to load the system will scale the number of Replicas within this range.\nFor example the following model will be deployed at first with 1 replica and it can scale up according to load.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n  minReplicas: 1\n  replicas: 1\n\n\nNote that model autoscaling will not attempt to add extra servers if the desired number of replicas cannot be currently fulfilled by the current provisioned number of servers. This is a process left to be done by server autoscaling.\nAdditionally when the system autoscales, the initial model spec is not changed (e.g. the number of Replicas) and therefore the user cannot reset the number of replicas back to the initial specified value without an explicit change.\nIf only Replicas is specified by the user, autoscaling of models is disabled and the system will have exactly the number of replicas of this model deployed regardless of inference load.\n\nArchitecture\u00b6\nThe model autoscaling architecture is designed such as each agent decides on which models to scale up / down according to some defined internal metrics and then sends a triggering message to the scheduler. The current metrics are collected from the data plane (inference path), representing a proxy on how loaded is a given model with fulfilling inference requests.\n\n\n\nAgent autoscaling stats collection\u00b6\n\nScale up logic:\u00b6\nThe main idea is that we keep the \u201clag\u201d for each model. We define the \u201clag\u201d as the difference between incoming and outgoing requests in a given time period. If the lag crosses a threshold, then we trigger a model scale up event. This threshold can be defined via SELDON_MODEL_INFERENCE_LAG_THRESHOLD inference server environment variable.\n\n\nScale down logic:\u00b6\nFor now we keep things simple and we trigger model scale down events if a model has not been used for a number of seconds. This is defined in SELDON_MODEL_INACTIVE_SECONDS_THRESHOLD inference server environment variable.\nEach agent checks the above stats periodically and if any model hits the corresponding threshold, then the agent sends an event to the scheduler to request model scaling.\nHow often this process executes can be defined via SELDON_SCALING_STATS_PERIOD_SECONDS inference server environment variable.\n\n\n\nScheduler autoscaling\u00b6\n\nThe scheduler will perform model autoscale if:\n\nThe model is stable (no state change in the last 5 minutes) and available.\nThe desired number of replicas is within range. Note we always have a least 1 replica of any deployed model and we rely on over commit to reduce the resources used further.\nFor scaling up, there is enough capacity for the new model replica.\n\n\n\n\nModel memory overcommit\u00b6\nServers can hold more models than available memory if overcommit is swictched on (default yes). This allows under utilized models to be moved from inference server memory to allow for other models to take their place. Note that these evicted models are still registered and in the case future inference requests arrive, the system will reload the models back to memory before serving the requests. If traffic patterns for inference of models vary then this can allow more models than available server memory to be run on the system.\n\n", "inference-servers-autoscaling": "\nInference servers autoscaling\u00b6\nAutoscaling of servers can be done via HorizontalPodAutoscaler (HPA).\nHPA can be applied to any deployed Server resource.\nIn this case HPA will manage the number of server replicas in the corresponding statefulset according to utilisation metrics  (e.g. CPU or memory).\nFor example assuming that a triton server is deployed, then the user can attach an HPA based on cpu utilisation as follows:\nkubectl autoscale server triton --cpu-percent=50 --min=1 --max=5\n\n\nIn this case, according to load, the system will add / remove server replicas to / from the triton statefulset.\nIt is worth considering the following points:\n\nIf HPA adds a new server replica, this new replica will be included in any future scheduling decisions.\nIn other words, when deploying a new model or rescheduling failed models this new replica will be considered.\nIf HPA deletes an existing server replica, the scheduler will first attempt to drain any loaded model on this server replica before the server replica gets actually deleted. This is achieved by leveraging a PreStop hook on the server replica pod that triggers a process before receiving the termination signal. This draining process is capped by terminationGracePeriodSeconds, which the user can set (default is 2 minutes).\n\nTherefore there should generally be minimal disruption to the inference workload during scaling.\nFor more details on HPA check this Kubernetes walk-through.\n\nNote\nAutoscaling of inference servers via seldon-scheduler is under consideration for the roadmap. This allow for more fine grained interactions with model autoscaling.\n\n", "model-autoscaling": "\nModel autoscaling\u00b6\nAs each model server can serve multiple models, models can scale across the available replicas of the server according to load.\nAutoscaling of models is enabled if at least MinReplicas or MaxReplicas is set in the model custom resource. Then according to load the system will scale the number of Replicas within this range.\nFor example the following model will be deployed at first with 1 replica and it can scale up according to load.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: tfsimple\nspec:\n  storageUri: \"gs://seldon-models/triton/simple\"\n  requirements:\n  - tensorflow\n  memory: 100Ki\n  minReplicas: 1\n  replicas: 1\n\n\nNote that model autoscaling will not attempt to add extra servers if the desired number of replicas cannot be currently fulfilled by the current provisioned number of servers. This is a process left to be done by server autoscaling.\nAdditionally when the system autoscales, the initial model spec is not changed (e.g. the number of Replicas) and therefore the user cannot reset the number of replicas back to the initial specified value without an explicit change.\nIf only Replicas is specified by the user, autoscaling of models is disabled and the system will have exactly the number of replicas of this model deployed regardless of inference load.\n\nArchitecture\u00b6\nThe model autoscaling architecture is designed such as each agent decides on which models to scale up / down according to some defined internal metrics and then sends a triggering message to the scheduler. The current metrics are collected from the data plane (inference path), representing a proxy on how loaded is a given model with fulfilling inference requests.\n\n\n\nAgent autoscaling stats collection\u00b6\n\nScale up logic:\u00b6\nThe main idea is that we keep the \u201clag\u201d for each model. We define the \u201clag\u201d as the difference between incoming and outgoing requests in a given time period. If the lag crosses a threshold, then we trigger a model scale up event. This threshold can be defined via SELDON_MODEL_INFERENCE_LAG_THRESHOLD inference server environment variable.\n\n\nScale down logic:\u00b6\nFor now we keep things simple and we trigger model scale down events if a model has not been used for a number of seconds. This is defined in SELDON_MODEL_INACTIVE_SECONDS_THRESHOLD inference server environment variable.\nEach agent checks the above stats periodically and if any model hits the corresponding threshold, then the agent sends an event to the scheduler to request model scaling.\nHow often this process executes can be defined via SELDON_SCALING_STATS_PERIOD_SECONDS inference server environment variable.\n\n\n\nScheduler autoscaling\u00b6\n\nThe scheduler will perform model autoscale if:\n\nThe model is stable (no state change in the last 5 minutes) and available.\nThe desired number of replicas is within range. Note we always have a least 1 replica of any deployed model and we rely on over commit to reduce the resources used further.\nFor scaling up, there is enough capacity for the new model replica.\n\n\n", "architecture": "\nArchitecture\u00b6\nThe model autoscaling architecture is designed such as each agent decides on which models to scale up / down according to some defined internal metrics and then sends a triggering message to the scheduler. The current metrics are collected from the data plane (inference path), representing a proxy on how loaded is a given model with fulfilling inference requests.\n\n", "agent-autoscaling-stats-collection": "\nAgent autoscaling stats collection\u00b6\n\nScale up logic:\u00b6\nThe main idea is that we keep the \u201clag\u201d for each model. We define the \u201clag\u201d as the difference between incoming and outgoing requests in a given time period. If the lag crosses a threshold, then we trigger a model scale up event. This threshold can be defined via SELDON_MODEL_INFERENCE_LAG_THRESHOLD inference server environment variable.\n\n\nScale down logic:\u00b6\nFor now we keep things simple and we trigger model scale down events if a model has not been used for a number of seconds. This is defined in SELDON_MODEL_INACTIVE_SECONDS_THRESHOLD inference server environment variable.\nEach agent checks the above stats periodically and if any model hits the corresponding threshold, then the agent sends an event to the scheduler to request model scaling.\nHow often this process executes can be defined via SELDON_SCALING_STATS_PERIOD_SECONDS inference server environment variable.\n\n", "scale-up-logic": "\nScale up logic:\u00b6\nThe main idea is that we keep the \u201clag\u201d for each model. We define the \u201clag\u201d as the difference between incoming and outgoing requests in a given time period. If the lag crosses a threshold, then we trigger a model scale up event. This threshold can be defined via SELDON_MODEL_INFERENCE_LAG_THRESHOLD inference server environment variable.\n", "scale-down-logic": "\nScale down logic:\u00b6\nFor now we keep things simple and we trigger model scale down events if a model has not been used for a number of seconds. This is defined in SELDON_MODEL_INACTIVE_SECONDS_THRESHOLD inference server environment variable.\nEach agent checks the above stats periodically and if any model hits the corresponding threshold, then the agent sends an event to the scheduler to request model scaling.\nHow often this process executes can be defined via SELDON_SCALING_STATS_PERIOD_SECONDS inference server environment variable.\n", "scheduler-autoscaling": "\nScheduler autoscaling\u00b6\n\nThe scheduler will perform model autoscale if:\n\nThe model is stable (no state change in the last 5 minutes) and available.\nThe desired number of replicas is within range. Note we always have a least 1 replica of any deployed model and we rely on over commit to reduce the resources used further.\nFor scaling up, there is enough capacity for the new model replica.\n\n", "model-memory-overcommit": "\nModel memory overcommit\u00b6\nServers can hold more models than available memory if overcommit is swictched on (default yes). This allows under utilized models to be moved from inference server memory to allow for other models to take their place. Note that these evicted models are still registered and in the case future inference requests arrive, the system will reload the models back to memory before serving the requests. If traffic patterns for inference of models vary then this can allow more models than available server memory to be run on the system.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/autoscaling/index.html", "key": "kubernetes/autoscaling"}}, "models/rclone": {"sections": {"rclone": "\nRclone\u00b6\nWe utilize Rclone to copy model artifacts from a storage location to the model servers. This allows users to take advantage of Rclones support for over 40 cloud storage backends including Amazon S3, Google Storage and many others.\nFor local storage while developing see here.\nFor authorization needed for cloud storage when running on Kubernetes see here.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/models/rclone/index.html", "key": "models/rclone"}}, "getting-started/kubernetes-installation/security/confluent-oauth": {"sections": {"confluent-cloud-oauth-2-0-example": "\nConfluent Cloud Oauth 2.0 Example\u00b6\n\nNew in Seldon Core 2.7.0\n\nSeldon Core v2 can integrate with Confluent Cloud managed Kafka.\nIn this example we use Oauth 2.0 security mechanism.\n\nConfigure Identity Provider in Confluent Cloud Console\u00b6\nIn your Confluent Cloud Console go to Account & Access / Identity providers and register your Identity Provider.\nSee Confluent Cloud documentation for further details.\n\n\nConfigure Identity Pool\u00b6\nIn your Confluent Cloud Console go to Account & Access / Identity providers and add new identity pool to your newly registered Identity Provider.\nSee Confluent Cloud documentation for further details.\n\n\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects oauth credentials to be in form of K8s secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: confluent-kafka-oauth\n  namespace: seldon-mesh\ntype: Opaque\nstringData:\n  method: OIDC\n  client_id: <client id>\n  client_secret: <client secret>\n  token_endpoint_url: <token endpoint url>\n  extensions: logicalCluster=<cluster id>,identityPoolId=<identity pool id>\n  scope: \"\"\n\n\nYou will need following information from Confluent Cloud:\n\nCluster ID: Cluster Overview \u2192 Cluster Settings \u2192 General \u2192 Identification\nIdentity Pool ID: Accounts & access \u2192 Identity providers \u2192 <specific provider details>\n\nClient ID, client secret and token endpoint url should come from identity provider, e.g. Keycloak or Azure AD.\n\n\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: < Confluent Cloud Broker Endpoints >\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n  consumer:\n    messageMaxBytes: 8388608\n  producer:\n    messageMaxBytes: 8388608\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: OAUTHBEARER\n      client:\n          secret: confluent-kafka-oauth\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n\n\nTroubleshooting\u00b6\n\nFirst check Confluent Cloud documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n\n", "configure-identity-provider-in-confluent-cloud-console": "\nConfigure Identity Provider in Confluent Cloud Console\u00b6\nIn your Confluent Cloud Console go to Account & Access / Identity providers and register your Identity Provider.\nSee Confluent Cloud documentation for further details.\n", "configure-identity-pool": "\nConfigure Identity Pool\u00b6\nIn your Confluent Cloud Console go to Account & Access / Identity providers and add new identity pool to your newly registered Identity Provider.\nSee Confluent Cloud documentation for further details.\n", "create-kubernetes-secret": "\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects oauth credentials to be in form of K8s secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: confluent-kafka-oauth\n  namespace: seldon-mesh\ntype: Opaque\nstringData:\n  method: OIDC\n  client_id: <client id>\n  client_secret: <client secret>\n  token_endpoint_url: <token endpoint url>\n  extensions: logicalCluster=<cluster id>,identityPoolId=<identity pool id>\n  scope: \"\"\n\n\nYou will need following information from Confluent Cloud:\n\nCluster ID: Cluster Overview \u2192 Cluster Settings \u2192 General \u2192 Identification\nIdentity Pool ID: Accounts & access \u2192 Identity providers \u2192 <specific provider details>\n\nClient ID, client secret and token endpoint url should come from identity provider, e.g. Keycloak or Azure AD.\n", "configure-seldon-core-v2": "\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: < Confluent Cloud Broker Endpoints >\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n  consumer:\n    messageMaxBytes: 8388608\n  producer:\n    messageMaxBytes: 8388608\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: OAUTHBEARER\n      client:\n          secret: confluent-kafka-oauth\n    ssl:\n      client:\n        secret:\n        brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n", "troubleshooting": "\nTroubleshooting\u00b6\n\nFirst check Confluent Cloud documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/confluent-oauth.html", "key": "getting-started/kubernetes-installation/security/confluent-oauth"}}, "metrics": {"sections": {"metrics": "\nMetrics\u00b6\nThere are two kinds of metrics present in Seldon Core v2:\n\noperational metrics\nusage metrics\n\nOperational metrics describe the performance of components in the system.\nExamples of common operational considerations are memory consumption and CPU usage, request latency and throughput, and cache utilisation rates.\nGenerally speaking, these are the metrics system administrators, operations teams, and engineers will be interested in.\nUsage metrics describe the system at a higher and less dynamic level.\nExamples include the number of deployed servers and models, and component versions.\nThese are not typically metrics that engineers need insight into, but may be relevant to platform providers and operations teams.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/metrics/index.html", "key": "metrics"}}, "architecture/dataflow": {"sections": {"data-flow-design-paradigm-in-seldon-core": "\nData flow design paradigm in Seldon Core\u00b6\nSeldon Core v2 is designed around data flow paradigm. Here we will explain what that means and some of the rationals behind this choice.\n\nSeldon Core v1\u00b6\nInitial release of Seldon Core introduced a concept of an inference graph, which can be thought of as a sequence of operations that happen to the inference request. Here is how it may look like:\n\nIn reality though this was not how Seldon Core v1 is implemented. Instead, Seldon deployment consists of a range of independent services that host models, transformations, detectors and explainers, and a central orchestrator that knows the inference graph topology and makes service calls in the correct order, passing data between requests and responses as necessary. Here is how the picture looks under the hood:\n\nWhile this is a convenient way of implementing evaluation graph with microservices, it has a few problems. Orchestrator becomes a bottleneck and a single point of failure. It also hides all the data transformations that need to happen to translate one service\u2019s response to another service\u2019s request. Data tracing and lineage becomes difficult. All in all, while Seldon platform is all about processing data, under-the-hood implementation was still focused on order of operations and not on data itself.\n\n\nData flow\u00b6\nThe realisation of this disparity led to a new approach towards inference graph evaluation in v2, based on the data flow paradigm. Data flow is a well known concept in software engineering, known from 1960s. In contrast to services, that model programs as a control flow, focusing on the order of operations, data flow proposes to model software systems as a series of connections that modify incoming data, focusing on data flowing through the system. A particular flavor of data flow paradigm used by v2 is known as flow-based programming, FBP. FBP defines software applications as a set of processes which exchange data via connections that are external to those processes. Connections are made via named ports, which promotes data coupling between components of the system.\nData flow design makes data in software the top priority. That is one of the key messages of the so called \u201cdata-centric AI\u201d idea, which is becoming increasingly popular within the ML community. Data is a key component of a successful ML project. Data needs to be discovered, described, cleaned, understood, monitored and verified. Consequently, there is a growing demand for data-centric platforms and solutions. Making Seldon Core data-centric was one of the key goals of the v2 design.\n\n\nSeldon Core v2\u00b6\nIn the context of Seldon Core application of FBP design approach means that the evaluation implementation is done the same way inferece graph. So instead of routing everything through a centralized orchestrator the evaluation happens in the same graph-like manner:\n\nAs far as implementation goes, Seldon Core v2 runs on Kafka. Inference request is put onto a pipeline input topic, which triggers an evaluation. Each part of the inference graph is a service running in its own container fronted by a model gateway. Model gateway listens to a corresponding input Kafka topic, reads data from it, calls the service and puts the received response to an output Kafka topic. There is also a pipeline gateway that allows to interact with Seldon Core in synchronous manner.\nThis approach gives SCv2 several important features. Firstly, Seldon Core natively supports both synchronous and asynchronous modes of operation. Asynchronicity is achieved via streaming: input data can be sent to an input topic in Kafka, and after the evaluation the output topic will contain the inference result. For those looking to use it in the v1 style, a service API is provided.\nSecondly, there is no single point of failure. Even if one or more nodes in the graph go down, the data will still be sitting on the streams waiting to be processed, and the evaluation resumes whenever the failed node comes back up.\nThirdly, data flow means intermediate data can be accessed at any arbitrary step of the graph, inspected and collected as necessary. Data lineage is possible throughout, which opens up opportunities for advanced monitoring and explainability use cases. This is a key feature for effective error surfacing in production environments as it allows:\n\nAdding context from different parts of the graph to better understand a particular output\nReducing false positive rates of alerts as different slices of the data can be investigated\nEnabling reproducing of results as fined-grained lineage of computation and associated data transformation are tracked by design\n\nFinally, inference graph can now be extended with adding new nodes at arbitrary places, all without affecting the pipeline execution. This kind of flexibility was not possible with v1. This also allows multiple pipelines to share common nodes and therefore optimising resources usage.\n\n\nReferences\u00b6\nMore details and information on data-centric AI and data flow paradigm can be found in these resources:\n\nData-centric AI Resource Hub: https://datacentricai.org/\nStanford MLSys seminar \u201cWhat can Data-Centric AI Learn from Data and ML Engineering?\u201d: https://www.youtube.com/watch?v=cqDgxP8DcJA\nA paper that explores data flow in ML deployment context: https://arxiv.org/abs/2204.12781\nIntroduction to flow based programming from its creator J.P. Morrison: https://jpaulm.github.io/fbp/index.html\nPathways: Asynchronous Distributed Dataflow for ML (research work from Google on the design and implementation of data flow based orchestration layer for accelerators): https://arxiv.org/abs/2203.12533\nBetter understanding of data requires tracking its history and context: https://queue.acm.org/detail.cfm?id=2602651\n\n\n", "seldon-core-v1": "\nSeldon Core v1\u00b6\nInitial release of Seldon Core introduced a concept of an inference graph, which can be thought of as a sequence of operations that happen to the inference request. Here is how it may look like:\n\nIn reality though this was not how Seldon Core v1 is implemented. Instead, Seldon deployment consists of a range of independent services that host models, transformations, detectors and explainers, and a central orchestrator that knows the inference graph topology and makes service calls in the correct order, passing data between requests and responses as necessary. Here is how the picture looks under the hood:\n\nWhile this is a convenient way of implementing evaluation graph with microservices, it has a few problems. Orchestrator becomes a bottleneck and a single point of failure. It also hides all the data transformations that need to happen to translate one service\u2019s response to another service\u2019s request. Data tracing and lineage becomes difficult. All in all, while Seldon platform is all about processing data, under-the-hood implementation was still focused on order of operations and not on data itself.\n", "data-flow": "\nData flow\u00b6\nThe realisation of this disparity led to a new approach towards inference graph evaluation in v2, based on the data flow paradigm. Data flow is a well known concept in software engineering, known from 1960s. In contrast to services, that model programs as a control flow, focusing on the order of operations, data flow proposes to model software systems as a series of connections that modify incoming data, focusing on data flowing through the system. A particular flavor of data flow paradigm used by v2 is known as flow-based programming, FBP. FBP defines software applications as a set of processes which exchange data via connections that are external to those processes. Connections are made via named ports, which promotes data coupling between components of the system.\nData flow design makes data in software the top priority. That is one of the key messages of the so called \u201cdata-centric AI\u201d idea, which is becoming increasingly popular within the ML community. Data is a key component of a successful ML project. Data needs to be discovered, described, cleaned, understood, monitored and verified. Consequently, there is a growing demand for data-centric platforms and solutions. Making Seldon Core data-centric was one of the key goals of the v2 design.\n", "seldon-core-v2": "\nSeldon Core v2\u00b6\nIn the context of Seldon Core application of FBP design approach means that the evaluation implementation is done the same way inferece graph. So instead of routing everything through a centralized orchestrator the evaluation happens in the same graph-like manner:\n\nAs far as implementation goes, Seldon Core v2 runs on Kafka. Inference request is put onto a pipeline input topic, which triggers an evaluation. Each part of the inference graph is a service running in its own container fronted by a model gateway. Model gateway listens to a corresponding input Kafka topic, reads data from it, calls the service and puts the received response to an output Kafka topic. There is also a pipeline gateway that allows to interact with Seldon Core in synchronous manner.\nThis approach gives SCv2 several important features. Firstly, Seldon Core natively supports both synchronous and asynchronous modes of operation. Asynchronicity is achieved via streaming: input data can be sent to an input topic in Kafka, and after the evaluation the output topic will contain the inference result. For those looking to use it in the v1 style, a service API is provided.\nSecondly, there is no single point of failure. Even if one or more nodes in the graph go down, the data will still be sitting on the streams waiting to be processed, and the evaluation resumes whenever the failed node comes back up.\nThirdly, data flow means intermediate data can be accessed at any arbitrary step of the graph, inspected and collected as necessary. Data lineage is possible throughout, which opens up opportunities for advanced monitoring and explainability use cases. This is a key feature for effective error surfacing in production environments as it allows:\n\nAdding context from different parts of the graph to better understand a particular output\nReducing false positive rates of alerts as different slices of the data can be investigated\nEnabling reproducing of results as fined-grained lineage of computation and associated data transformation are tracked by design\n\nFinally, inference graph can now be extended with adding new nodes at arbitrary places, all without affecting the pipeline execution. This kind of flexibility was not possible with v1. This also allows multiple pipelines to share common nodes and therefore optimising resources usage.\n", "references": "\nReferences\u00b6\nMore details and information on data-centric AI and data flow paradigm can be found in these resources:\n\nData-centric AI Resource Hub: https://datacentricai.org/\nStanford MLSys seminar \u201cWhat can Data-Centric AI Learn from Data and ML Engineering?\u201d: https://www.youtube.com/watch?v=cqDgxP8DcJA\nA paper that explores data flow in ML deployment context: https://arxiv.org/abs/2204.12781\nIntroduction to flow based programming from its creator J.P. Morrison: https://jpaulm.github.io/fbp/index.html\nPathways: Asynchronous Distributed Dataflow for ML (research work from Google on the design and implementation of data flow based orchestration layer for accelerators): https://arxiv.org/abs/2203.12533\nBetter understanding of data requires tracking its history and context: https://queue.acm.org/detail.cfm?id=2602651\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/architecture/dataflow.html", "key": "architecture/dataflow"}}, "examples/multi-version": {"sections": {"artifact-versions": "\nArtifact Versions\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Kubernetes Multi Version Artifact Examples\u00b6\nWe have a Triton model that has two version folders\nModel 1 adds 10 to input, Model 2 multiples by 10 the input. The structure of the artifact repo is shown below:\nconfig.pbtxt\n1/model.py <add 10>\n2/model.py <mul 10>\n\n\n\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\n\nModel\u00b6\ncat ./models/multi-version-1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 1\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./models/multi-version-2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 2\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math configured\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_2\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"math\" deleted\n\n\n\n\n\n\n\n\n", "seldon-v2-kubernetes-multi-version-artifact-examples": "\nSeldon V2 Kubernetes Multi Version Artifact Examples\u00b6\nWe have a Triton model that has two version folders\nModel 1 adds 10 to input, Model 2 multiples by 10 the input. The structure of the artifact repo is shown below:\nconfig.pbtxt\n1/model.py <add 10>\n2/model.py <mul 10>\n\n\n\nimport os\nos.environ[\"NAMESPACE\"] = \"seldon-mesh\"\n\n\nMESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nMESH_IP=MESH_IP[0]\nimport os\nos.environ['MESH_IP'] = MESH_IP\nMESH_IP\n\n\n'172.19.255.1'\n\n\n\n\nModel\u00b6\ncat ./models/multi-version-1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 1\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./models/multi-version-2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 2\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math configured\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_2\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"math\" deleted\n\n\n\n\n\n\n\n", "model": "\nModel\u00b6\ncat ./models/multi-version-1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 1\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math created\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./models/multi-version-2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: math\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/multi-version\"\n  artifactVersion: 2\n  requirements:\n  - triton\n  - python\n\n\nkubectl apply -f ./models/multi-version-2.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math configured\n\n\n\nkubectl wait --for condition=ready --timeout=300s model --all -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io/math condition met\n\n\n\nseldon model infer math --inference-mode grpc --inference-host ${MESH_IP}:80 \\\n  '{\"model_name\":\"math\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"modelName\": \"math_2\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\nkubectl delete -f ./models/multi-version-1.yaml -n ${NAMESPACE}\n\n\nmodel.mlops.seldon.io \"math\" deleted\n\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/multi-version.html", "key": "examples/multi-version"}}, "performance-tests": {"sections": {"performance-tests": "\nPerformance tests\u00b6\nThis section describes how a user can run performance tests to understand the limits of a particular SCv2 deployment.\nThe base directly is tests/k6\n\nDriver\u00b6\nk6 is used to drive requests for load, unload and infer workloads. It is recommended that the load test is run within the same cluster that has SCv2 installed as it requires internal access to some of the services that are not automatically exposed to the outside world. Furthermore having the driver withthin the same cluster minimises link latency to SCv2 entrypoint; therefore infer latencies are more representatives of actual overheads of the system.\n\n\nTests\u00b6\n\nEnvoy\nTests synchronous inference requests via envoy\n\nTo run: make deploy-envoy-test\n\nAgent\nTests inference requests direct to a specific agent, defaults to triton-0 or mlserver-0\n\nTo run: make deploy-rproxy-test pr make deploy-rproxy-mlserver-test\n\nServer\nTests inference requests direct to a specific server (bypassing agent), defaults to triton-0 or mlserver-0\n\nto run: make deploy-server-test or deploy-server-mlserver-test\n\nPipeline gateway (HTTP-Kafka gateway)\nTests inference requests to one-node pipeline HTTP and GPRC requests\n\nTo run: make deploy-kpipeline-test\n\nModel gateway (Kafka-HTTP gateway)\nTests inference requests to a model via kafka\n\nTo run: deploy-kmodel-test\n\n\nResults\u00b6\nOne way to look at results is to look at the log of the pod that executed the kubernetes job.\nResults can also be persisted to a gs bucket, a service account k6-sa-key in the same namespace is required,\nUsers can also look at the metrics that are exposed in prometheus while the test is underway\n\n\nBuilding k6 image\u00b6\nIn the case a user is modifying the actual scenario of the test:\n\nexport DOCKERHUB_USERNAME=mydockerhubaccount\nbuild the k6 image via make build-push\nin the same shell environment, deploying jobs will use this custome built docker image\n\n\n\nModifying tests\u00b6\nUsers can modify settings of the tests in tests/k6/configs/k8s/base/k6.yaml. This will apply to all subsequent tests that are deployed using the above process.\n\n\nSettings\u00b6\nSome settings that can be changed\n\nk6 args\n          \"--no-teardown\",\n          \"--summary-export\",\n          \"results/base.json\",\n          \"--out\",\n          \"csv=results/base.gz\",\n          \"-u\",\n          \"5\",\n          \"-i\",\n          \"100000\",\n          \"-d\",\n          \"120m\",\n          \"scenarios/infer_constant_vu.js\",\n          ]\n        # infer_constant_vu\n        # args: [\n        #   \"--no-teardown\",\n        #   \"--summary-export\",\n        #   \"results/base.json\",\n        #   \"--out\",\n        #   \"csv=results/base.gz\",\n        #   \"scenarios/infer_constant_rate.js\",\n        #   ]\n\n\n\n\nfor a full list, check k6 args\n\nEnvironment variables\n        - name: INFER_HTTP_ITERATIONS\n          value: \"1\"\n        - name: INFER_GRPC_ITERATIONS\n          value: \"1\"\n        - name: MODELNAME_PREFIX\n          value: \"tfsimplea,pytorch_cifar10a,tfmnista,mlflow_winea,irisa\"\n        - name: MODEL_TYPE\n          value: \"tfsimple,pytorch_cifar10,tfmnist,mlflow_wine,iris\"\n        - name: MODEL_MEMORY_BYTES\n          value: \"400000,8000000,43000000,200000,3000000\"\n        - name: MAX_NUM_MODELS\n          value: \"800,100,25,100,100\"\n          # value: \"0,0,25,100,100\"\n        - name: INFER_BATCH_SIZE\n          value: \"1,1,1,1,1\"\n        - name: WARMUP\n          value: \"false\"\n\n\n\nfor MODEL_TYPE, choose from:\n\nconst tfsimple_string = \"tfsimple_string\"\nconst tfsimple = \"tfsimple\"\nconst iris = \"iris\"  // mlserver\nconst pytorch_cifar10 = \"pytorch_cifar10\"\nconst tfmnist = \"tfmnist\"\nconst tfresnet152 = \"tfresnet152\"\nconst onnx_gpt2 = \"onnx_gpt2\"\nconst mlflow_wine = \"mlflow_wine\" // mlserver\nconst add10 = \"add10\" // https://github.com/SeldonIO/triton-python-examples/tree/master/add10\nconst sentiment = \"sentiment\" // mlserver\n\n\n\n\n\n", "driver": "\nDriver\u00b6\nk6 is used to drive requests for load, unload and infer workloads. It is recommended that the load test is run within the same cluster that has SCv2 installed as it requires internal access to some of the services that are not automatically exposed to the outside world. Furthermore having the driver withthin the same cluster minimises link latency to SCv2 entrypoint; therefore infer latencies are more representatives of actual overheads of the system.\n", "tests": "\nTests\u00b6\n\nEnvoy\nTests synchronous inference requests via envoy\n\nTo run: make deploy-envoy-test\n\nAgent\nTests inference requests direct to a specific agent, defaults to triton-0 or mlserver-0\n\nTo run: make deploy-rproxy-test pr make deploy-rproxy-mlserver-test\n\nServer\nTests inference requests direct to a specific server (bypassing agent), defaults to triton-0 or mlserver-0\n\nto run: make deploy-server-test or deploy-server-mlserver-test\n\nPipeline gateway (HTTP-Kafka gateway)\nTests inference requests to one-node pipeline HTTP and GPRC requests\n\nTo run: make deploy-kpipeline-test\n\nModel gateway (Kafka-HTTP gateway)\nTests inference requests to a model via kafka\n\nTo run: deploy-kmodel-test\n", "results": "\nResults\u00b6\nOne way to look at results is to look at the log of the pod that executed the kubernetes job.\nResults can also be persisted to a gs bucket, a service account k6-sa-key in the same namespace is required,\nUsers can also look at the metrics that are exposed in prometheus while the test is underway\n", "building-k6-image": "\nBuilding k6 image\u00b6\nIn the case a user is modifying the actual scenario of the test:\n\nexport DOCKERHUB_USERNAME=mydockerhubaccount\nbuild the k6 image via make build-push\nin the same shell environment, deploying jobs will use this custome built docker image\n\n", "modifying-tests": "\nModifying tests\u00b6\nUsers can modify settings of the tests in tests/k6/configs/k8s/base/k6.yaml. This will apply to all subsequent tests that are deployed using the above process.\n", "settings": "\nSettings\u00b6\nSome settings that can be changed\n\nk6 args\n          \"--no-teardown\",\n          \"--summary-export\",\n          \"results/base.json\",\n          \"--out\",\n          \"csv=results/base.gz\",\n          \"-u\",\n          \"5\",\n          \"-i\",\n          \"100000\",\n          \"-d\",\n          \"120m\",\n          \"scenarios/infer_constant_vu.js\",\n          ]\n        # infer_constant_vu\n        # args: [\n        #   \"--no-teardown\",\n        #   \"--summary-export\",\n        #   \"results/base.json\",\n        #   \"--out\",\n        #   \"csv=results/base.gz\",\n        #   \"scenarios/infer_constant_rate.js\",\n        #   ]\n\n\n\n\nfor a full list, check k6 args\n\nEnvironment variables\n        - name: INFER_HTTP_ITERATIONS\n          value: \"1\"\n        - name: INFER_GRPC_ITERATIONS\n          value: \"1\"\n        - name: MODELNAME_PREFIX\n          value: \"tfsimplea,pytorch_cifar10a,tfmnista,mlflow_winea,irisa\"\n        - name: MODEL_TYPE\n          value: \"tfsimple,pytorch_cifar10,tfmnist,mlflow_wine,iris\"\n        - name: MODEL_MEMORY_BYTES\n          value: \"400000,8000000,43000000,200000,3000000\"\n        - name: MAX_NUM_MODELS\n          value: \"800,100,25,100,100\"\n          # value: \"0,0,25,100,100\"\n        - name: INFER_BATCH_SIZE\n          value: \"1,1,1,1,1\"\n        - name: WARMUP\n          value: \"false\"\n\n\n\nfor MODEL_TYPE, choose from:\n\nconst tfsimple_string = \"tfsimple_string\"\nconst tfsimple = \"tfsimple\"\nconst iris = \"iris\"  // mlserver\nconst pytorch_cifar10 = \"pytorch_cifar10\"\nconst tfmnist = \"tfmnist\"\nconst tfresnet152 = \"tfresnet152\"\nconst onnx_gpt2 = \"onnx_gpt2\"\nconst mlflow_wine = \"mlflow_wine\" // mlserver\nconst add10 = \"add10\" // https://github.com/SeldonIO/triton-python-examples/tree/master/add10\nconst sentiment = \"sentiment\" // mlserver\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/performance-tests/index.html", "key": "performance-tests"}}, "drift": {"sections": {"drift-detection": "\nDrift Detection\u00b6\nDrift detection models are treated as any other Model. You can run any saved Alibi-Detect drift detection model by adding the requirement alibi-detect.\nAn example drift detection model from the CIFAR10 image classification example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nUsually you would run these models in an asynchronous part of a Pipeline, i.e. they are not connected to the output of the Pipeline which defines the synchronous path. For example, the CIFAR-10 image detection example uses a pipeline as shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: cifar10-production\nspec:\n  steps:\n    - name: cifar10\n    - name: cifar10-outlier\n    - name: cifar10-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - cifar10\n    - cifar10-outlier.outputs.is_outlier\n\n\nNote how the cifar10-drift model is not part of the path to the outputs. Drift alerts can be read from the Kafka topic of the model.\n\nExamples\u00b6\n\nCIFAR10 image classification with drift detector\nTabular income classification model with drift detector\n\n\n", "examples": "\nExamples\u00b6\n\nCIFAR10 image classification with drift detector\nTabular income classification model with drift detector\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/drift/index.html", "key": "drift"}}, "kubernetes/service-meshes/istio": {"sections": {"istio": "\nIstio\u00b6\nIstio provides a service mesh and ingress solution.\nWe will run through some examples as shown in the notebook service-meshes/istio/istio.ipynb\n\nSingle Model\u00b6\n\nA Seldon Iris Model\nAn istio Gateway\nAn instio VirtualService to expose REST and gRPC\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n\n\n\n\nTraffic Split\u00b6\n\nTwo Iris Models\nAn istio Gateway\nAn istio VirtualService with traffic split\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2/models/iris\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n\n\n\n\nIstio Notebook Examples\u00b6\nAssumes\n\nYou have installed istio as per their docs\nYou have exposed the ingressgateway as an external loadbalancer\n\ntested with:\nistioctl version\n1.13.2\n\nistioctl install --set profile=demo -y\n\n\nINGRESS_IP=!kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nIstio Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris unchanged\ngateway.networking.istio.io/seldon-gateway unchanged\nvirtualservice.networking.istio.io/iris-route configured\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:34:11 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 802\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"83520c4a-c7f1-4363-9bfd-60c5d8ee2dc5\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n\n\nTraffic Split Two Models\u00b6\n!kustomize build config/traffic-split\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2/models/iris\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\ngateway.networking.istio.io/seldon-gateway created\nvirtualservice.networking.istio.io/iris-route created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:35:01 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 801\n< seldon-route: iris1_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris1_1\",\"model_version\":\"1\",\"id\":\"b54e6d8c-d253-4bb9-bb64-02c2ee49e89f\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n\n\n\n\n\n", "single-model": "\nSingle Model\u00b6\n\nA Seldon Iris Model\nAn istio Gateway\nAn instio VirtualService to expose REST and gRPC\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n\n\n", "traffic-split": "\nTraffic Split\u00b6\n\nTwo Iris Models\nAn istio Gateway\nAn istio VirtualService with traffic split\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2/models/iris\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n\n\n", "istio-notebook-examples": "\nIstio Notebook Examples\u00b6\nAssumes\n\nYou have installed istio as per their docs\nYou have exposed the ingressgateway as an external loadbalancer\n\ntested with:\nistioctl version\n1.13.2\n\nistioctl install --set profile=demo -y\n\n\nINGRESS_IP=!kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\nINGRESS_IP=INGRESS_IP[0]\nimport os\nos.environ['INGRESS_IP'] = INGRESS_IP\nINGRESS_IP\n\n\n'172.21.255.1'\n\n\n\nIstio Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris unchanged\ngateway.networking.istio.io/seldon-gateway unchanged\nvirtualservice.networking.istio.io/iris-route configured\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:34:11 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 802\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"83520c4a-c7f1-4363-9bfd-60c5d8ee2dc5\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n\n\nTraffic Split Two Models\u00b6\n!kustomize build config/traffic-split\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2/models/iris\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\ngateway.networking.istio.io/seldon-gateway created\nvirtualservice.networking.istio.io/iris-route created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:35:01 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 801\n< seldon-route: iris1_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris1_1\",\"model_version\":\"1\",\"id\":\"b54e6d8c-d253-4bb9-bb64-02c2ee49e89f\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n\n\n\n\n", "istio-single-model-example": "\nIstio Single Model Example\u00b6\n!kustomize build config/single-model\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - istio-system/seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris\n\n\n!kustomize build config/single-model | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris unchanged\ngateway.networking.istio.io/seldon-gateway unchanged\nvirtualservice.networking.istio.io/iris-route configured\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 196\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:34:11 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 802\n< seldon-route: iris_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris_1\",\"model_version\":\"1\",\"id\":\"83520c4a-c7f1-4363-9bfd-60c5d8ee2dc5\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/single-model | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n", "traffic-split-two-models": "\nTraffic Split Two Models\u00b6\n!kustomize build config/traffic-split\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris1\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\n  namespace: seldon-mesh\nspec:\n  requirements:\n  - sklearn\n  storageUri: gs://seldon-models/mlserver/iris\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: seldon-gateway\n  namespace: seldon-mesh\nspec:\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: http\n      number: 80\n      protocol: HTTP\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: iris-route\n  namespace: seldon-mesh\nspec:\n  gateways:\n  - seldon-gateway\n  hosts:\n  - '*'\n  http:\n  - match:\n    - uri:\n        prefix: /v2/models/iris\n    name: iris-http\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n  - match:\n    - uri:\n        prefix: /inference.GRPCInferenceService\n    name: iris-grpc\n    route:\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris1\n      weight: 50\n    - destination:\n        host: seldon-mesh.seldon-mesh.svc.cluster.local\n      headers:\n        request:\n          set:\n            seldon-model: iris2\n      weight: 50\n\n\n!kustomize build config/traffic-split | kubectl apply -f -\n\n\nmodel.mlops.seldon.io/iris1 created\nmodel.mlops.seldon.io/iris2 created\ngateway.networking.istio.io/seldon-gateway created\nvirtualservice.networking.istio.io/iris-route created\n\n\n!kubectl wait --for condition=ready --timeout=300s model --all -n seldon-mesh\n\n\nmodel.mlops.seldon.io/iris1 condition met\nmodel.mlops.seldon.io/iris2 condition met\n\n\n!curl -v http://${INGRESS_IP}/v2/models/iris/infer -H \"Content-Type: application/json\" \\\n        -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n*   Trying 172.21.255.1...\n* Connected to 172.21.255.1 (172.21.255.1) port 80 (#0)\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: 172.21.255.1\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 94\n> \n* upload completely sent off: 94 out of 94 bytes\n< HTTP/1.1 200 OK\n< content-length: 197\n< content-type: application/json\n< date: Sat, 16 Apr 2022 15:35:01 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 801\n< seldon-route: iris1_1\n< \n* Connection #0 to host 172.21.255.1 left intact\n{\"model_name\":\"iris1_1\",\"model_version\":\"1\",\"id\":\"b54e6d8c-d253-4bb9-bb64-02c2ee49e89f\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[1],\"datatype\":\"INT64\",\"parameters\":null,\"data\":[2]}]}\n\n\n!grpcurl -d '{\"model_name\":\"iris1\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}' \\\n    -plaintext \\\n    -import-path ../../apis \\\n    -proto ../../apis/mlops/v2_dataplane/v2_dataplane.proto \\\n    ${INGRESS_IP}:80 inference.GRPCInferenceService/ModelInfer\n\n\n{\n  \"modelName\": \"iris1_1\",\n  \"modelVersion\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"predict\",\n      \"datatype\": \"INT64\",\n      \"shape\": [\n        \"1\"\n      ],\n      \"contents\": {\n        \"int64Contents\": [\n          \"2\"\n        ]\n      }\n    }\n  ]\n}\n\n\n!kustomize build config/traffic-split | kubectl delete -f -\n\n\nmodel.mlops.seldon.io \"iris1\" deleted\nmodel.mlops.seldon.io \"iris2\" deleted\ngateway.networking.istio.io \"seldon-gateway\" deleted\nvirtualservice.networking.istio.io \"iris-route\" deleted\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/service-meshes/istio/index.html", "key": "kubernetes/service-meshes/istio"}}, "apis/internal": {"sections": {"internal-apis": "\nInternal APIs\u00b6\nSeldon provides some internal gRPC services it uses to manage components.\n\nAgent API\nChainer API\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/internal/index.html", "key": "apis/internal"}}, "faqs": {"sections": {"faqs": "\nFAQs\u00b6\n\nCan Seldon Core V2 be used with Seldon Core V1\u00b6\nThe two projects are able to be run side by side. Existing users of Seldon Core can update to deploy their models using Seldon Core V2 as needed to take advantage of the new functionality. Both will be supported.\n\n\nShould I choose V1 APIs or V2 APIs\u00b6\nThis depends on your use case. V2 APIs are not yet at GA so might contain breaking changes in future releases.\nUse V1 for:\n\nTight integration to Seldon V1 protocol\nTensorflow Server requirements\nNeed managed istio integration\n\nUse V2 for:\n\nMulti-model serving\nMore expressive DAG inference pipelines\nData-centric (Kafka)\nService mesh agnostic\nSimpler single model usage\nV2 Protocol\n\n\n\nCan I do payload logging in Seldon Core v2?\u00b6\nBy default, the input and output of every step in a pipeline (as well as the pipeline itself) is logged in Kafka.\nFrom there it\u2019s up to you what to do with the data.\nYou could use something like Kafka Connect to stream the logs to a datastore of your choice.\nNote that there is no automatic request logging for models being accessed directly over REST or gRPC.\nRequests need to be sent via pipelines to be recorded in Kafka.\n\n", "can-seldon-core-v2-be-used-with-seldon-core-v1": "\nCan Seldon Core V2 be used with Seldon Core V1\u00b6\nThe two projects are able to be run side by side. Existing users of Seldon Core can update to deploy their models using Seldon Core V2 as needed to take advantage of the new functionality. Both will be supported.\n", "should-i-choose-v1-apis-or-v2-apis": "\nShould I choose V1 APIs or V2 APIs\u00b6\nThis depends on your use case. V2 APIs are not yet at GA so might contain breaking changes in future releases.\nUse V1 for:\n\nTight integration to Seldon V1 protocol\nTensorflow Server requirements\nNeed managed istio integration\n\nUse V2 for:\n\nMulti-model serving\nMore expressive DAG inference pipelines\nData-centric (Kafka)\nService mesh agnostic\nSimpler single model usage\nV2 Protocol\n\n", "can-i-do-payload-logging-in-seldon-core-v2": "\nCan I do payload logging in Seldon Core v2?\u00b6\nBy default, the input and output of every step in a pipeline (as well as the pipeline itself) is logged in Kafka.\nFrom there it\u2019s up to you what to do with the data.\nYou could use something like Kafka Connect to stream the logs to a datastore of your choice.\nNote that there is no automatic request logging for models being accessed directly over REST or gRPC.\nRequests need to be sent via pipelines to be recorded in Kafka.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/faqs/index.html", "key": "faqs"}}, "getting-started/kubernetes-installation/security/aws-msk-sasl": {"sections": {"aws-msk-sasl": "\nAWS MSK SASL\u00b6\n\nNew in Seldon Core 2.5.0\n\nSeldon Core v2 can integrate with Amazon managed Apache Kafka (MSK). You can control access to your Amazon MSK clusters using sign-in credentials that are stored and secured using AWS Secrets Manager. Storing user credentials in Secrets Manager reduces the overhead of cluster authentication such as auditing, updating, and rotating credentials. Secrets Manager also lets you share user credentials across clusters.\n\nNote\nConfiguration of the AWS MSK instance itself is out of scope for this example.\nPlease follow the official AWS documentation on how to enable SASL and public access to the Kafka cluster (if required).\n\n\nSetting up SASL/SCRAM authentication for an Amazon MSK cluster\u00b6\nTo setup SASL/SCRAM in an Amazon MSK cluster, please follow the guide from Amazon\u2019s Official documentation.\nDo not forget to also copy the bootstrap.servers which we will use it in our configuration later below for Seldon.\n\n\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic aws-msk-kafka-secret -n seldon-mesh --from-literal password=\"<MSK SASL Password>\"\n\n\n\n\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: <msk-bootstrap-server-endpoints>\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n\nsecurity:\n    kafka:\n      protocol: SASL_SSL\n      sasl:\n        mechanism: SCRAM-SHA-512\n        client:\n          username: < username >\n          secret: aws-msk-kafka-secret\n          passwordPath: password\n      ssl:\n        client:\n          secret:\n          brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n\n\nTroubleshooting\u00b6\n\nPlease check Amazon MSK Troubleshooting documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n\n", "setting-up-sasl-scram-authentication-for-an-amazon-msk-cluster": "\nSetting up SASL/SCRAM authentication for an Amazon MSK cluster\u00b6\nTo setup SASL/SCRAM in an Amazon MSK cluster, please follow the guide from Amazon\u2019s Official documentation.\nDo not forget to also copy the bootstrap.servers which we will use it in our configuration later below for Seldon.\n", "create-kubernetes-secret": "\nCreate Kubernetes Secret\u00b6\nSeldon Core v2 expects password to be in form of K8s secret\nkubectl create secret generic aws-msk-kafka-secret -n seldon-mesh --from-literal password=\"<MSK SASL Password>\"\n\n\n", "configure-seldon-core-v2": "\nConfigure Seldon Core v2\u00b6\nConfigure Seldon Core v2 by setting following Helm values:\nkafka:\n  bootstrap: <msk-bootstrap-server-endpoints>\n  topics:\n    replicationFactor: 3\n    numPartitions: 4\n\nsecurity:\n    kafka:\n      protocol: SASL_SSL\n      sasl:\n        mechanism: SCRAM-SHA-512\n        client:\n          username: < username >\n          secret: aws-msk-kafka-secret\n          passwordPath: password\n      ssl:\n        client:\n          secret:\n          brokerValidationSecret:\n\n\nNote you may need to tweak replicationFactor and numPartitions to your cluster configuration.\n", "troubleshooting": "\nTroubleshooting\u00b6\n\nPlease check Amazon MSK Troubleshooting documentation.\nSet the kafka config map debug setting to all. For Helm install you can set kafka.debug=all.\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/aws-msk-sasl.html", "key": "getting-started/kubernetes-installation/security/aws-msk-sasl"}}, "apis/scheduler": {"sections": {"scheduler-api": "\nScheduler API\u00b6\nThe Seldon scheduler API provides a gRPC service to allow Models, Servers, Experiments, and Pipelines to be managed. In Kubernetes the manager deployed by Seldon translates Kubernetes custom resource definitions into calls to the Seldon Scheduler.\nIn non-Kubernetes environments users of Seldon could create a client to directly control Seldon resources using this API.\n\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.scheduler;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/scheduler\";\n\nimport \"google/protobuf/timestamp.proto\";\n\n// [START Messages]\n\nmessage LoadModelRequest {\n  Model model = 1;\n}\n\nmessage Model {\n  MetaData meta = 1;\n  ModelSpec modelSpec = 2;\n  DeploymentSpec deploymentSpec = 3;\n  StreamSpec streamSpec = 4;\n}\n\nmessage MetaData {\n  string name = 1;\n  optional string kind = 2;\n  optional string version = 3;\n  optional KubernetesMeta kubernetesMeta = 4; // Kubernetes specific config\n}\n\nmessage DeploymentSpec {\n  uint32 replicas = 1;\n  uint32 minReplicas = 2;\n  uint32 maxReplicas = 3;\n  bool logPayloads = 4;\n}\n\n/* ModelDetails\n*/\nmessage ModelSpec {\n  string uri = 1; // storage uri from where to download the artifacts\n  optional uint32 artifactVersion = 2; // Optional v2 version folder to select\n  optional StorageConfig storageConfig = 3; // Storage auth configuration\n  repeated string requirements = 4; // list of capabilities the server must satisfy to run this model\n  optional uint64 memoryBytes = 5; // Requested memory\n  optional string server = 6; // the particular model server to load the model. If unspecified will be chosen.\n  optional ExplainerSpec explainer = 7; // optional black box explainer details\n  repeated ParameterSpec parameters = 8; // parameters to load with model\n}\n\nmessage ParameterSpec {\n  string name = 1;\n  string value = 2;\n}\n\nmessage ExplainerSpec {\n  string type = 1;\n  // 1 of semantic either model or pipeline reference\n  optional string modelRef = 2;\n  optional string pipelineRef = 3;\n}\n\nmessage KubernetesMeta {\n  string namespace = 1;\n  int64 generation = 2;\n}\n\nmessage StreamSpec {\n  string inputTopic = 2;\n  string outputTopic = 3;\n}\n\nmessage StorageConfig {\n  oneof config {\n    string storageSecretName = 1;\n    string storageRcloneConfig = 2;\n  }\n}\n\nmessage LoadModelResponse {\n\n}\n\n/* ModelReference represents a unique model\n*/\nmessage ModelReference {\n  string name = 1;\n  optional uint32 version = 2;\n}\n\nmessage UnloadModelRequest {\n  ModelReference model = 1;\n  optional KubernetesMeta kubernetesMeta = 2;\n}\n\nmessage UnloadModelResponse {\n}\n\n/* ModelStatusResponse provides the current assignment of the model onto a server\n*/\nmessage ModelStatusResponse {\n  string modelName = 1;\n  repeated ModelVersionStatus versions = 2;\n  bool deleted = 3;\n}\n\nmessage ModelVersionStatus {\n  uint32 version = 2;\n  string serverName = 3;\n  optional KubernetesMeta kubernetesMeta = 4;\n  map<int32,ModelReplicaStatus> modelReplicaState = 5;\n  ModelStatus state = 6;\n  optional Model modelDefn = 7;\n}\n\nmessage ModelStatus {\n  enum ModelState {\n      ModelStateUnknown = 0;\n      ModelProgressing = 1;\n      ModelAvailable = 2;\n      ModelFailed = 3;\n      ModelTerminating = 4;\n      ModelTerminated = 5;\n      ModelTerminateFailed = 6;\n      ScheduleFailed = 7;\n  }\n  ModelState state = 1;\n  string reason = 2;\n  uint32 availableReplicas = 3;\n  uint32 unavailableReplicas = 4;\n  google.protobuf.Timestamp lastChangeTimestamp = 5;\n}\n\nmessage ModelReplicaStatus {\n  enum ModelReplicaState {\n      ModelReplicaStateUnknown = 0;\n      LoadRequested = 1;\n      Loading = 2;\n      Loaded = 3;\n      LoadFailed = 4;\n      UnloadRequested = 5;\n      Unloading = 6;\n      Unloaded = 7;\n      UnloadFailed = 8;\n      Available = 9;\n      LoadedUnavailable = 10;\n      UnloadEnvoyRequested = 11;\n      Draining = 12;\n  }\n  ModelReplicaState state = 1;\n  string reason = 2;\n  google.protobuf.Timestamp lastChangeTimestamp = 3;\n}\n\nmessage ServerStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all servers\n}\n\n/* ServerStatusResponse provides details of current server status\n*/\nmessage ServerStatusResponse {\n  string serverName = 1;\n  repeated ServerReplicaResources resources = 2;\n  int32 expectedReplicas = 3;\n  int32 availableReplicas = 4;\n  int32 numLoadedModelReplicas = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n}\n\nmessage ServerReplicaResources {\n  uint32 replicaIdx = 1;\n  uint64 totalMemoryBytes = 2;\n  uint64 availableMemoryBytes = 3;\n  int32 numLoadedModels = 4;\n  uint32 overCommitPercentage = 5;\n}\n\nmessage ModelSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage ModelStatusRequest {\n  string subscriberName = 1;\n  optional ModelReference model = 2;\n  bool allVersions = 3;\n}\n\nmessage ServerNotifyRequest {\n  string name = 1;\n  int32 expectedReplicas = 2;\n  bool shared = 3;\n  optional KubernetesMeta kubernetesMeta = 4;\n}\n\nmessage ServerNotifyResponse {\n\n}\n\nmessage ServerSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\n// Experiments\n\nmessage StartExperimentRequest {\n  Experiment experiment = 1;\n}\n\nenum ResourceType {\n  MODEL = 0;\n  PIPELINE = 1;\n}\n\nmessage Experiment {\n  string name = 1;\n  optional string default = 2;\n  repeated ExperimentCandidate candidates = 3;\n  optional ExperimentMirror mirror = 4;\n  optional ExperimentConfig config = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n  ResourceType resourceType = 7;\n}\n\nmessage ExperimentConfig {\n  bool stickySessions = 1;\n}\n\nmessage ExperimentCandidate {\n  string name = 1;\n  uint32 weight = 2;\n}\n\nmessage ExperimentMirror {\n  string name = 1;\n  uint32 percent = 2;\n}\n\nmessage StartExperimentResponse {\n}\n\nmessage StopExperimentRequest {\n  string name = 1;\n}\n\nmessage StopExperimentResponse {\n}\n\nmessage ExperimentSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage ExperimentStatusResponse {\n  string experimentName = 1;\n  bool active = 2;\n  bool candidatesReady = 3;\n  bool mirrorReady = 4;\n  string statusDescription = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n}\n\nmessage LoadPipelineRequest {\n  Pipeline pipeline = 1;\n}\n\nmessage ExperimentStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all experiments\n}\n\nmessage Pipeline {\n  string name = 1;\n  string uid = 2;\n  uint32 version = 3;\n  repeated PipelineStep steps = 4;\n  optional PipelineOutput output = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n  optional PipelineInput input = 7;\n}\n\nmessage PipelineStep {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  string name = 1;\n  repeated string inputs = 2;\n  optional uint32 joinWindowMs = 3; // Join window millisecs, some nonzero default (TBD)\n  map<string,string> tensorMap = 4; // optional map of tensor name mappings\n  JoinOp inputsJoin = 5;\n  repeated string triggers = 6;\n  JoinOp triggersJoin = 7;\n  Batch batch = 8;\n}\n\nmessage Batch {\n  optional uint32 size = 1;\n  optional uint32 windowMs = 2;\n}\n\nmessage PipelineInput {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  repeated string externalInputs = 1;\n  repeated string externalTriggers = 2;\n  optional uint32 joinWindowMs = 3; // Join window millisecs for output, default 0\n  JoinOp joinType = 4;\n  JoinOp triggersJoin = 5;\n  map<string,string> tensorMap = 6; // optional map of tensor name mappings\n}\n\nmessage PipelineOutput {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  repeated string steps = 1;\n  uint32 joinWindowMs = 2; // Join window millisecs for output, default 0\n  JoinOp stepsJoin = 3;\n  map<string,string> tensorMap = 4; // optional map of tensor name mappings\n}\n\nmessage LoadPipelineResponse {\n\n}\n\nmessage UnloadPipelineRequest {\n  string name = 1;\n}\n\nmessage UnloadPipelineResponse {\n\n}\n\nmessage PipelineStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all pipelines\n  bool allVersions = 3;\n}\n\nmessage PipelineSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage PipelineStatusResponse {\n  string pipelineName = 1;\n  repeated PipelineWithState versions = 2;\n}\n\nmessage PipelineWithState {\n  Pipeline pipeline = 1;\n  PipelineVersionState state = 2;\n}\n\nmessage PipelineVersionState {\n  enum PipelineStatus {\n    PipelineStatusUnknown = 0;\n    PipelineCreate = 1;\n    PipelineCreating = 2;\n    PipelineReady = 3;\n    PipelineFailed = 4;\n    PipelineTerminate = 5;\n    PipelineTerminating = 6;\n    PipelineTerminated = 7;\n  }\n  uint32 pipelineVersion = 1;\n  PipelineStatus status = 2;\n  string reason = 3;\n  google.protobuf.Timestamp lastChangeTimestamp = 4;\n  bool modelsReady = 5;\n}\n\nmessage SchedulerStatusRequest {\n  string subscriberName = 1;\n}\n\nmessage SchedulerStatusResponse {\n  string applicationVersion = 1;\n}\n\n// [END Messages]\n\n\n// [START Services]\n\nservice Scheduler {\n  rpc ServerNotify(ServerNotifyRequest) returns (ServerNotifyResponse) {};\n\n  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse) {};\n  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse) {};\n\n  rpc LoadPipeline(LoadPipelineRequest) returns (LoadPipelineResponse) {};\n  rpc UnloadPipeline(UnloadPipelineRequest) returns (UnloadPipelineResponse) {};\n\n  rpc StartExperiment(StartExperimentRequest) returns (StartExperimentResponse) {};\n  rpc StopExperiment(StopExperimentRequest) returns (StopExperimentResponse) {};\n\n  rpc ServerStatus(ServerStatusRequest) returns (stream ServerStatusResponse) {}\n  rpc ModelStatus(ModelStatusRequest) returns (stream ModelStatusResponse) {}\n  rpc PipelineStatus(PipelineStatusRequest) returns (stream PipelineStatusResponse) {};\n  rpc ExperimentStatus(ExperimentStatusRequest) returns (stream ExperimentStatusResponse) {};\n  rpc SchedulerStatus(SchedulerStatusRequest) returns (SchedulerStatusResponse) {};\n\n  rpc SubscribeServerStatus(ServerSubscriptionRequest) returns (stream ServerStatusResponse) {};\n  rpc SubscribeModelStatus(ModelSubscriptionRequest) returns (stream ModelStatusResponse) {};\n  rpc SubscribeExperimentStatus(ExperimentSubscriptionRequest) returns (stream ExperimentStatusResponse) {};\n  rpc SubscribePipelineStatus(PipelineSubscriptionRequest) returns (stream PipelineStatusResponse) {};\n}\n\n// [END Services]\n\n\n\n", "proto-definition": "\nProto Definition\u00b6\nsyntax = \"proto3\";\n\npackage seldon.mlops.scheduler;\n\noption go_package = \"github.com/seldonio/seldon-core/apis/go/v2/mlops/scheduler\";\n\nimport \"google/protobuf/timestamp.proto\";\n\n// [START Messages]\n\nmessage LoadModelRequest {\n  Model model = 1;\n}\n\nmessage Model {\n  MetaData meta = 1;\n  ModelSpec modelSpec = 2;\n  DeploymentSpec deploymentSpec = 3;\n  StreamSpec streamSpec = 4;\n}\n\nmessage MetaData {\n  string name = 1;\n  optional string kind = 2;\n  optional string version = 3;\n  optional KubernetesMeta kubernetesMeta = 4; // Kubernetes specific config\n}\n\nmessage DeploymentSpec {\n  uint32 replicas = 1;\n  uint32 minReplicas = 2;\n  uint32 maxReplicas = 3;\n  bool logPayloads = 4;\n}\n\n/* ModelDetails\n*/\nmessage ModelSpec {\n  string uri = 1; // storage uri from where to download the artifacts\n  optional uint32 artifactVersion = 2; // Optional v2 version folder to select\n  optional StorageConfig storageConfig = 3; // Storage auth configuration\n  repeated string requirements = 4; // list of capabilities the server must satisfy to run this model\n  optional uint64 memoryBytes = 5; // Requested memory\n  optional string server = 6; // the particular model server to load the model. If unspecified will be chosen.\n  optional ExplainerSpec explainer = 7; // optional black box explainer details\n  repeated ParameterSpec parameters = 8; // parameters to load with model\n}\n\nmessage ParameterSpec {\n  string name = 1;\n  string value = 2;\n}\n\nmessage ExplainerSpec {\n  string type = 1;\n  // 1 of semantic either model or pipeline reference\n  optional string modelRef = 2;\n  optional string pipelineRef = 3;\n}\n\nmessage KubernetesMeta {\n  string namespace = 1;\n  int64 generation = 2;\n}\n\nmessage StreamSpec {\n  string inputTopic = 2;\n  string outputTopic = 3;\n}\n\nmessage StorageConfig {\n  oneof config {\n    string storageSecretName = 1;\n    string storageRcloneConfig = 2;\n  }\n}\n\nmessage LoadModelResponse {\n\n}\n\n/* ModelReference represents a unique model\n*/\nmessage ModelReference {\n  string name = 1;\n  optional uint32 version = 2;\n}\n\nmessage UnloadModelRequest {\n  ModelReference model = 1;\n  optional KubernetesMeta kubernetesMeta = 2;\n}\n\nmessage UnloadModelResponse {\n}\n\n/* ModelStatusResponse provides the current assignment of the model onto a server\n*/\nmessage ModelStatusResponse {\n  string modelName = 1;\n  repeated ModelVersionStatus versions = 2;\n  bool deleted = 3;\n}\n\nmessage ModelVersionStatus {\n  uint32 version = 2;\n  string serverName = 3;\n  optional KubernetesMeta kubernetesMeta = 4;\n  map<int32,ModelReplicaStatus> modelReplicaState = 5;\n  ModelStatus state = 6;\n  optional Model modelDefn = 7;\n}\n\nmessage ModelStatus {\n  enum ModelState {\n      ModelStateUnknown = 0;\n      ModelProgressing = 1;\n      ModelAvailable = 2;\n      ModelFailed = 3;\n      ModelTerminating = 4;\n      ModelTerminated = 5;\n      ModelTerminateFailed = 6;\n      ScheduleFailed = 7;\n  }\n  ModelState state = 1;\n  string reason = 2;\n  uint32 availableReplicas = 3;\n  uint32 unavailableReplicas = 4;\n  google.protobuf.Timestamp lastChangeTimestamp = 5;\n}\n\nmessage ModelReplicaStatus {\n  enum ModelReplicaState {\n      ModelReplicaStateUnknown = 0;\n      LoadRequested = 1;\n      Loading = 2;\n      Loaded = 3;\n      LoadFailed = 4;\n      UnloadRequested = 5;\n      Unloading = 6;\n      Unloaded = 7;\n      UnloadFailed = 8;\n      Available = 9;\n      LoadedUnavailable = 10;\n      UnloadEnvoyRequested = 11;\n      Draining = 12;\n  }\n  ModelReplicaState state = 1;\n  string reason = 2;\n  google.protobuf.Timestamp lastChangeTimestamp = 3;\n}\n\nmessage ServerStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all servers\n}\n\n/* ServerStatusResponse provides details of current server status\n*/\nmessage ServerStatusResponse {\n  string serverName = 1;\n  repeated ServerReplicaResources resources = 2;\n  int32 expectedReplicas = 3;\n  int32 availableReplicas = 4;\n  int32 numLoadedModelReplicas = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n}\n\nmessage ServerReplicaResources {\n  uint32 replicaIdx = 1;\n  uint64 totalMemoryBytes = 2;\n  uint64 availableMemoryBytes = 3;\n  int32 numLoadedModels = 4;\n  uint32 overCommitPercentage = 5;\n}\n\nmessage ModelSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage ModelStatusRequest {\n  string subscriberName = 1;\n  optional ModelReference model = 2;\n  bool allVersions = 3;\n}\n\nmessage ServerNotifyRequest {\n  string name = 1;\n  int32 expectedReplicas = 2;\n  bool shared = 3;\n  optional KubernetesMeta kubernetesMeta = 4;\n}\n\nmessage ServerNotifyResponse {\n\n}\n\nmessage ServerSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\n// Experiments\n\nmessage StartExperimentRequest {\n  Experiment experiment = 1;\n}\n\nenum ResourceType {\n  MODEL = 0;\n  PIPELINE = 1;\n}\n\nmessage Experiment {\n  string name = 1;\n  optional string default = 2;\n  repeated ExperimentCandidate candidates = 3;\n  optional ExperimentMirror mirror = 4;\n  optional ExperimentConfig config = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n  ResourceType resourceType = 7;\n}\n\nmessage ExperimentConfig {\n  bool stickySessions = 1;\n}\n\nmessage ExperimentCandidate {\n  string name = 1;\n  uint32 weight = 2;\n}\n\nmessage ExperimentMirror {\n  string name = 1;\n  uint32 percent = 2;\n}\n\nmessage StartExperimentResponse {\n}\n\nmessage StopExperimentRequest {\n  string name = 1;\n}\n\nmessage StopExperimentResponse {\n}\n\nmessage ExperimentSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage ExperimentStatusResponse {\n  string experimentName = 1;\n  bool active = 2;\n  bool candidatesReady = 3;\n  bool mirrorReady = 4;\n  string statusDescription = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n}\n\nmessage LoadPipelineRequest {\n  Pipeline pipeline = 1;\n}\n\nmessage ExperimentStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all experiments\n}\n\nmessage Pipeline {\n  string name = 1;\n  string uid = 2;\n  uint32 version = 3;\n  repeated PipelineStep steps = 4;\n  optional PipelineOutput output = 5;\n  optional KubernetesMeta kubernetesMeta = 6;\n  optional PipelineInput input = 7;\n}\n\nmessage PipelineStep {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  string name = 1;\n  repeated string inputs = 2;\n  optional uint32 joinWindowMs = 3; // Join window millisecs, some nonzero default (TBD)\n  map<string,string> tensorMap = 4; // optional map of tensor name mappings\n  JoinOp inputsJoin = 5;\n  repeated string triggers = 6;\n  JoinOp triggersJoin = 7;\n  Batch batch = 8;\n}\n\nmessage Batch {\n  optional uint32 size = 1;\n  optional uint32 windowMs = 2;\n}\n\nmessage PipelineInput {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  repeated string externalInputs = 1;\n  repeated string externalTriggers = 2;\n  optional uint32 joinWindowMs = 3; // Join window millisecs for output, default 0\n  JoinOp joinType = 4;\n  JoinOp triggersJoin = 5;\n  map<string,string> tensorMap = 6; // optional map of tensor name mappings\n}\n\nmessage PipelineOutput {\n  enum JoinOp {\n    INNER = 0;\n    OUTER = 1;\n    ANY = 2;\n  }\n  repeated string steps = 1;\n  uint32 joinWindowMs = 2; // Join window millisecs for output, default 0\n  JoinOp stepsJoin = 3;\n  map<string,string> tensorMap = 4; // optional map of tensor name mappings\n}\n\nmessage LoadPipelineResponse {\n\n}\n\nmessage UnloadPipelineRequest {\n  string name = 1;\n}\n\nmessage UnloadPipelineResponse {\n\n}\n\nmessage PipelineStatusRequest {\n  string subscriberName = 1;\n  optional string name = 2; // Leave empty for all pipelines\n  bool allVersions = 3;\n}\n\nmessage PipelineSubscriptionRequest {\n  string subscriberName = 1; //Name of the subscription caller\n}\n\nmessage PipelineStatusResponse {\n  string pipelineName = 1;\n  repeated PipelineWithState versions = 2;\n}\n\nmessage PipelineWithState {\n  Pipeline pipeline = 1;\n  PipelineVersionState state = 2;\n}\n\nmessage PipelineVersionState {\n  enum PipelineStatus {\n    PipelineStatusUnknown = 0;\n    PipelineCreate = 1;\n    PipelineCreating = 2;\n    PipelineReady = 3;\n    PipelineFailed = 4;\n    PipelineTerminate = 5;\n    PipelineTerminating = 6;\n    PipelineTerminated = 7;\n  }\n  uint32 pipelineVersion = 1;\n  PipelineStatus status = 2;\n  string reason = 3;\n  google.protobuf.Timestamp lastChangeTimestamp = 4;\n  bool modelsReady = 5;\n}\n\nmessage SchedulerStatusRequest {\n  string subscriberName = 1;\n}\n\nmessage SchedulerStatusResponse {\n  string applicationVersion = 1;\n}\n\n// [END Messages]\n\n\n// [START Services]\n\nservice Scheduler {\n  rpc ServerNotify(ServerNotifyRequest) returns (ServerNotifyResponse) {};\n\n  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse) {};\n  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse) {};\n\n  rpc LoadPipeline(LoadPipelineRequest) returns (LoadPipelineResponse) {};\n  rpc UnloadPipeline(UnloadPipelineRequest) returns (UnloadPipelineResponse) {};\n\n  rpc StartExperiment(StartExperimentRequest) returns (StartExperimentResponse) {};\n  rpc StopExperiment(StopExperimentRequest) returns (StopExperimentResponse) {};\n\n  rpc ServerStatus(ServerStatusRequest) returns (stream ServerStatusResponse) {}\n  rpc ModelStatus(ModelStatusRequest) returns (stream ModelStatusResponse) {}\n  rpc PipelineStatus(PipelineStatusRequest) returns (stream PipelineStatusResponse) {};\n  rpc ExperimentStatus(ExperimentStatusRequest) returns (stream ExperimentStatusResponse) {};\n  rpc SchedulerStatus(SchedulerStatusRequest) returns (SchedulerStatusResponse) {};\n\n  rpc SubscribeServerStatus(ServerSubscriptionRequest) returns (stream ServerStatusResponse) {};\n  rpc SubscribeModelStatus(ModelSubscriptionRequest) returns (stream ModelStatusResponse) {};\n  rpc SubscribeExperimentStatus(ExperimentSubscriptionRequest) returns (stream ExperimentStatusResponse) {};\n  rpc SubscribePipelineStatus(PipelineSubscriptionRequest) returns (stream PipelineStatusResponse) {};\n}\n\n// [END Services]\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/scheduler/index.html", "key": "apis/scheduler"}}, "development/release#id1": {"sections": {"release-process": "\nRelease process\u00b6\n", "id1": "\nRelease Process\u00b6\nThis document summarizes the release process for Seldon Core v2.\nIt is aimed mainly at the maintainers.\n\n:warning: NOTE: This is a work in progress.\nThis is an early version of the release process, which is subject to change.\nPlease, always check this document before conducting a release, and verify if everything goes as expected.\n\n\nProcess Summary\u00b6\n\nCut branch for release, e.g. release-0.1\nRun \u201cDraft New Release\u201d workflow (e.g. choose release-0.1 branch and v0.1.0-rc1 version)\nRun \u201cBuild docker images\u201d workflow (e.g. choose release-0.1 branch and 0.1.0-rc1 tag)\nVerify correctness of created artifacts and images (not yet automated!)\nPublish release\nPublish tags for the Go modules\n\n\n\nProcess discussion\u00b6\nThe development process follows a standard GitHub workflow.\n\nThe main development is happening in the v2 branch.\nThis is where new features land through Pull Requests.\nWhen all features for a new release have been merged, for example v0.1.0, we cut a branch for that release, e.g. release-0.1.\nThe release-0.1 branch will be the base for the v0.1.0 release as well as the release candidates, i.e. v0.1.0-rcX, and successive patch releases, i.e. v0.1.X.\nWe use GitHub Actions to prepare the release, build images and run all necessary testing.\nIf the release draft needs to be updated before being published, the new commits should be merged into the release-0.1 branch and relevant workflows re-triggered as required.\n\n\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n\n\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n\n\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n\n\n", "process-summary": "\nProcess Summary\u00b6\n\nCut branch for release, e.g. release-0.1\nRun \u201cDraft New Release\u201d workflow (e.g. choose release-0.1 branch and v0.1.0-rc1 version)\nRun \u201cBuild docker images\u201d workflow (e.g. choose release-0.1 branch and 0.1.0-rc1 tag)\nVerify correctness of created artifacts and images (not yet automated!)\nPublish release\nPublish tags for the Go modules\n\n", "process-discussion": "\nProcess discussion\u00b6\nThe development process follows a standard GitHub workflow.\n\nThe main development is happening in the v2 branch.\nThis is where new features land through Pull Requests.\nWhen all features for a new release have been merged, for example v0.1.0, we cut a branch for that release, e.g. release-0.1.\nThe release-0.1 branch will be the base for the v0.1.0 release as well as the release candidates, i.e. v0.1.0-rcX, and successive patch releases, i.e. v0.1.X.\nWe use GitHub Actions to prepare the release, build images and run all necessary testing.\nIf the release draft needs to be updated before being published, the new commits should be merged into the release-0.1 branch and relevant workflows re-triggered as required.\n\n\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n\n\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n\n\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n\n", "draft-new-release-action": "\nDraft New Release Action\u00b6\nThe Draft New Release workflow is the first one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version (here v0.1.0-rc1)\n\n\nThis workflow cannot run on the v2 branch.\nIt will validate the provided version against a SemVer regex.\nIt will create a few commits with:\n\nUpdated Helm charts\nUpdated Kubernetes YAML manifests\nAn updated changelog\n\n\nOnce the workflow finishes, you will find a new release draft waiting to be published.\n\n\n:warning: NOTE: Before publishing the release, run the images build workflow and necessary tests (not yet automated)!\n\n", "build-docker-images-action": "\nBuild docker images Action\u00b6\nThe Build docker images workflow is the second one to run.\nIt must be triggered manually using the Actions interface in the GitHub UI.\nWhen triggering the workflow, you must:\n\nSelect the release branch (here release-0.1)\nSpecify the release version, e.g. 0.1.0-rc1\n\nNote the lack of the v prefix here\n\n\n\n\nThis workflow will then run unit tests and build a series of Docker images that will be automatically pushed to DockerHub.\n", "add-go-module-tags": "\nAdd Go module tags\u00b6\nGo module versions are mapped to VCS versions via semantic version tags.\nThis process is described in the Go documentation.\nAs we have multiple Go modules in subdirectories of the repository, we need to use corresponding prefixes for our git tags.\nFrom the above link on mapping versions to commits:\n\nIf a module is defined in a subdirectory within the repository, that is, the module subdirectory portion of the module path is not empty, then each tag name must be prefixed with the module subdirectory, followed by a slash. For example, the module golang.org/x/tools/gopls is defined in the gopls subdirectory of the repository with root path golang.org/x/tools. The version v0.4.0 of that module must have the tag named gopls/v0.4.0 in that repository.\n\nThus, for any given release, we should have one tag for the release as a whole plus one corresponding tag for every Go module.\nAt the time of writing, this comprises:\n\napis/go\ncomponents/tls\nhodometer\noperator\nscheduler\n\n\n:warning: Adding these tags is currently a manual process.\n\nTo add the appropriate tags:\n\nCheck out the relevant tag, e.g.\ngit checkout v2.4.0\n\n\n\nFind all relevant Go modules and identify their subdirectory paths, e.g. with\nfind . -name go.mod -exec sed -n '1 { s|^module.*seldon-core/||; s|/v2$||; p }' {} \\;\n\n\n\nAdd corresponding tags for each module, e.g.\ngit tag apis/go/v2.4.0 v2.4.0\n\n\n\nConfirm that all tags point to the same place, e.g. with\ngit tag --contains v2.4.0\n\n\n\nPush the tags to the upstream repository, e.g.\ngit push <upstream name> apis/go/v2.4.0 components/tls/v2.4.0 ...\n\n\n\n\nIf you are feeling confident in the process, you can chain these together into a longer pipeline.\nIn any case, it is best to confirm that the tags appear as expected both via the git CLI and also in the GitHub UI.\nA short list of commands to cover all above in single go is:\nVERSION=v2.4.0\ngit tag apis/go/${VERSION} ${VERSION}\ngit tag components/tls/${VERSION} ${VERSION}\ngit tag hodometer/${VERSION} ${VERSION}\ngit tag operator/${VERSION} ${VERSION}\ngit tag scheduler/${VERSION} ${VERSION}\n\ngit push origin apis/go/${VERSION}\ngit push origin components/tls/${VERSION}\ngit push origin hodometer/${VERSION}\ngit push origin operator/${VERSION}\ngit push origin scheduler/${VERSION}\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/development/release/index.html#id1", "key": "development/release#id1"}}, "examples/local-experiments": {"sections": {"local-experiments": "\nLocal Experiments\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Non Kubernetes Local Experiment Examples\u00b6\n\nModel Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate experiments.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::27 :iris_1::23]\n\n\n\nShow sticky session header x-seldon-route that is returned\nseldon model infer iris --show-headers \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: localhost:9000\n> Content-Type:[application/json]\n> Seldon-Model:[iris]\n\n< X-Seldon-Route:[:iris_1:]\n< Ce-Id:[463e96ad-645f-4442-8890-4c340b58820b]\n< Traceparent:[00-fe9e87fcbe4be98ed82fb76166e15ceb-d35e7ac96bd8b718-01]\n< X-Envoy-Upstream-Service-Time:[3]\n< Ce-Specversion:[0.3]\n< Date:[Thu, 29 Jun 2023 14:03:03 GMT]\n< Ce-Source:[io.seldon.serving.deployment.mlserver]\n< Content-Type:[application/json]\n< Server:[envoy]\n< X-Request-Id:[cieou5ofh5ss73fbjdu0]\n< Ce-Endpoint:[iris_1]\n< Ce-Modelid:[iris_1]\n< Ce-Type:[io.seldon.serving.inference.response]\n< Content-Length:[213]\n< Ce-Inferenceservicename:[mlserver]\n< Ce-Requestid:[463e96ad-645f-4442-8890-4c340b58820b]\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"463e96ad-645f-4442-8890-4c340b58820b\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\nPipeline Experiment\u00b6\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n\n\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n\n\ncat ./pipelines/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-mul10\nspec:\n  steps:\n    - name: mul10\n  output:\n    steps:\n    - mul10\n\n\ncat ./pipelines/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-add10\nspec:\n  steps:\n    - name: add10\n  output:\n    steps:\n    - add10\n\n\nseldon pipeline load -f ./pipelines/add10.yaml\nseldon pipeline load -f ./pipelines/mul10.yaml\n\n\nseldon pipeline status pipeline-add10 -w PipelineReady\nseldon pipeline status pipeline-mul10 -w PipelineReady\n\n\n{\"pipelineName\":\"pipeline-add10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-add10\", \"uid\":\"cieov47l80lc739juklg\", \"version\":1, \"steps\":[{\"name\":\"add10\"}], \"output\":{\"steps\":[\"add10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.460868091Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"pipeline-mul10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-mul10\", \"uid\":\"cieov47l80lc739jukm0\", \"version\":1, \"steps\":[{\"name\":\"mul10\"}], \"output\":{\"steps\":[\"mul10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.631980330Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer pipeline-add10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-mul10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./experiments/addmul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 50\n  - name: pipeline-mul10\n    weight: 50\n\n\nseldon experiment start -f ./experiments/addmul10.yaml\n\n\nseldon experiment status addmul10 -w | jq -M .\n\n\n{\n  \"experimentName\": \"addmul10\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon pipeline infer pipeline-add10 -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::28 :mul10_1::22 :pipeline-add10.pipeline::28 :pipeline-mul10.pipeline::22]\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieov8ofh5ss739277i0]\n< date:[Thu, 29 Jun 2023 14:05:23 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-envoy-upstream-service-time:[6]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< x-forwarded-proto:[http]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :pipeline-add10.pipeline:]\n< x-request-id:[cieov90fh5ss739277ig]\n< x-envoy-upstream-service-time:[7]\n< date:[Thu, 29 Jun 2023 14:05:24 GMT]\n< server:[envoy]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::50 :pipeline-add10.pipeline::150]\n\n\n\ncat ./models/add20.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add20\nspec:\n  storageUri: \"gs://seldon-models/triton/add20\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add20.yaml\n\n\n{}\n\n\nseldon model status add20 -w ModelAvailable\n\n\n{}\n\n\ncat ./experiments/add1020.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: add1020\nspec:\n  default: add10\n  candidates:\n  - name: add10\n    weight: 50\n  - name: add20\n    weight: 50\n\n\nseldon experiment start -f ./experiments/add1020.yaml\n\n\nseldon experiment status add1020 -w | jq -M .\n\n\n{\n  \"experimentName\": \"add1020\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon model infer add10 -i 50  --inference-mode grpc \\\n  '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::22 :add20_1::28]\n\n\n\nseldon pipeline infer pipeline-add10 -i 100 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::24 :add20_1::32 :mul10_1::44 :pipeline-add10.pipeline::56 :pipeline-mul10.pipeline::44]\n\n\n\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-request-id:[cieovf0fh5ss739279u0]\n< x-envoy-upstream-service-time:[5]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< date:[Thu, 29 Jun 2023 14:05:48 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieovf8fh5ss739279ug]\n< x-envoy-upstream-service-time:[6]\n< date:[Thu, 29 Jun 2023 14:05:49 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :add20_1: :pipeline-add10.pipeline:]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[21, 22, 23, 24]}}]}\n\n\n\nseldon experiment stop addmul10\nseldon experiment stop add1020\nseldon pipeline unload pipeline-add10\nseldon pipeline unload pipeline-mul10\nseldon model unload add10\nseldon model unload add20\nseldon model unload mul10\n\n\n\n\nModel Mirror Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate a model with a mirror.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nCreate an experiment that modifies in which we mirror traffic to iris also to iris2.\ncat ./experiments/sklearn-mirror.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: sklearn-mirror\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 100\n  mirror:\n    name: iris2\n    percent: 100\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/sklearn-mirror.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status sklearn-mirror -w | jq -M .\n\n\n{\n  \"experimentName\": \"sklearn-mirror\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nWe get responses from iris but all requests would also have been mirrored to iris2\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nWe can check the local prometheus port from the agent to validate requests went to iris2\ncurl -s 0.0.0:9006/metrics | grep seldon_model_infer_total | grep iris2_1\n\n\nseldon_model_infer_total{code=\"200\",method_type=\"rest\",model=\"iris\",model_internal=\"iris2_1\",server=\"mlserver\",server_replica=\"0\"} 50\n\n\n\nStop the experiment\nseldon experiment stop sklearn-mirror\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\n\nPipeline Mirror Experiment\u00b6\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n\n\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n\n\ncat ./pipelines/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-mul10\nspec:\n  steps:\n    - name: mul10\n  output:\n    steps:\n    - mul10\n\n\ncat ./pipelines/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-add10\nspec:\n  steps:\n    - name: add10\n  output:\n    steps:\n    - add10\n\n\nseldon pipeline load -f ./pipelines/add10.yaml\nseldon pipeline load -f ./pipelines/mul10.yaml\n\n\nseldon pipeline status pipeline-add10 -w PipelineReady\nseldon pipeline status pipeline-mul10 -w PipelineReady\n\n\n{\"pipelineName\":\"pipeline-add10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-add10\", \"uid\":\"ciep072i8ufs73flaipg\", \"version\":1, \"steps\":[{\"name\":\"add10\"}], \"output\":{\"steps\":[\"add10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:07:24.903503109Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"pipeline-mul10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-mul10\", \"uid\":\"ciep072i8ufs73flaiq0\", \"version\":1, \"steps\":[{\"name\":\"mul10\"}], \"output\":{\"steps\":[\"mul10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:07:25.082642153Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer pipeline-add10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\nseldon pipeline infer pipeline-mul10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\n\n\ncat ./experiments/addmul10-mirror.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10-mirror\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 100\n  mirror:\n    name: pipeline-mul10\n    percent: 100\n\n\nseldon experiment start -f ./experiments/addmul10-mirror.yaml\n\n\nseldon experiment status addmul10-mirror -w | jq -M .\n\n\n{\n  \"experimentName\": \"addmul10-mirror\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon pipeline infer pipeline-add10 -i 1 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\nLet\u2019s check that the mul10 model was called.\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep mul10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"mul10\",model_internal=\"mul10_1\",server=\"triton\",server_replica=\"0\"} 2\n\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep add10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"add10\",model_internal=\"add10_1\",server=\"triton\",server_replica=\"0\"} 2\n\n\n\nLet\u2019s do an http call and check agaib the two models\nseldon pipeline infer pipeline-add10 -i 1 \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"data\":[1,2,3,4],\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t11,\n\t\t\t\t12,\n\t\t\t\t13,\n\t\t\t\t14\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT\",\n\t\t\t\"shape\": [\n\t\t\t\t4\n\t\t\t],\n\t\t\t\"datatype\": \"FP32\"\n\t\t}\n\t]\n}\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep mul10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"mul10\",model_internal=\"mul10_1\",server=\"triton\",server_replica=\"0\"} 3\n\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep add10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"add10\",model_internal=\"add10_1\",server=\"triton\",server_replica=\"0\"} 3\n\n\n\nseldon pipeline inspect pipeline-mul10\n\n\nseldon.default.model.mul10.inputs\tciep0bofh5ss73dpdiq0\t{\"inputs\":[{\"name\":\"INPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[1, 2, 3, 4]}}]}\nseldon.default.model.mul10.outputs\tciep0bofh5ss73dpdiq0\t{\"modelName\":\"mul10_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\nseldon.default.pipeline.pipeline-mul10.inputs\tciep0bofh5ss73dpdiq0\t{\"inputs\":[{\"name\":\"INPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[1, 2, 3, 4]}}]}\nseldon.default.pipeline.pipeline-mul10.outputs\tciep0bofh5ss73dpdiq0\t{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\n\n\n\nseldon experiment stop addmul10-mirror\nseldon pipeline unload pipeline-add10\nseldon pipeline unload pipeline-mul10\nseldon model unload add10\nseldon model unload mul10\n\n\n\n\n\n\n", "seldon-v2-non-kubernetes-local-experiment-examples": "\nSeldon V2 Non Kubernetes Local Experiment Examples\u00b6\n\nModel Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate experiments.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::27 :iris_1::23]\n\n\n\nShow sticky session header x-seldon-route that is returned\nseldon model infer iris --show-headers \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: localhost:9000\n> Content-Type:[application/json]\n> Seldon-Model:[iris]\n\n< X-Seldon-Route:[:iris_1:]\n< Ce-Id:[463e96ad-645f-4442-8890-4c340b58820b]\n< Traceparent:[00-fe9e87fcbe4be98ed82fb76166e15ceb-d35e7ac96bd8b718-01]\n< X-Envoy-Upstream-Service-Time:[3]\n< Ce-Specversion:[0.3]\n< Date:[Thu, 29 Jun 2023 14:03:03 GMT]\n< Ce-Source:[io.seldon.serving.deployment.mlserver]\n< Content-Type:[application/json]\n< Server:[envoy]\n< X-Request-Id:[cieou5ofh5ss73fbjdu0]\n< Ce-Endpoint:[iris_1]\n< Ce-Modelid:[iris_1]\n< Ce-Type:[io.seldon.serving.inference.response]\n< Content-Length:[213]\n< Ce-Inferenceservicename:[mlserver]\n< Ce-Requestid:[463e96ad-645f-4442-8890-4c340b58820b]\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"463e96ad-645f-4442-8890-4c340b58820b\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n\nPipeline Experiment\u00b6\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n\n\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n\n\ncat ./pipelines/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-mul10\nspec:\n  steps:\n    - name: mul10\n  output:\n    steps:\n    - mul10\n\n\ncat ./pipelines/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-add10\nspec:\n  steps:\n    - name: add10\n  output:\n    steps:\n    - add10\n\n\nseldon pipeline load -f ./pipelines/add10.yaml\nseldon pipeline load -f ./pipelines/mul10.yaml\n\n\nseldon pipeline status pipeline-add10 -w PipelineReady\nseldon pipeline status pipeline-mul10 -w PipelineReady\n\n\n{\"pipelineName\":\"pipeline-add10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-add10\", \"uid\":\"cieov47l80lc739juklg\", \"version\":1, \"steps\":[{\"name\":\"add10\"}], \"output\":{\"steps\":[\"add10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.460868091Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"pipeline-mul10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-mul10\", \"uid\":\"cieov47l80lc739jukm0\", \"version\":1, \"steps\":[{\"name\":\"mul10\"}], \"output\":{\"steps\":[\"mul10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.631980330Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer pipeline-add10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-mul10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./experiments/addmul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 50\n  - name: pipeline-mul10\n    weight: 50\n\n\nseldon experiment start -f ./experiments/addmul10.yaml\n\n\nseldon experiment status addmul10 -w | jq -M .\n\n\n{\n  \"experimentName\": \"addmul10\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon pipeline infer pipeline-add10 -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::28 :mul10_1::22 :pipeline-add10.pipeline::28 :pipeline-mul10.pipeline::22]\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieov8ofh5ss739277i0]\n< date:[Thu, 29 Jun 2023 14:05:23 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-envoy-upstream-service-time:[6]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< x-forwarded-proto:[http]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :pipeline-add10.pipeline:]\n< x-request-id:[cieov90fh5ss739277ig]\n< x-envoy-upstream-service-time:[7]\n< date:[Thu, 29 Jun 2023 14:05:24 GMT]\n< server:[envoy]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::50 :pipeline-add10.pipeline::150]\n\n\n\ncat ./models/add20.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add20\nspec:\n  storageUri: \"gs://seldon-models/triton/add20\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add20.yaml\n\n\n{}\n\n\nseldon model status add20 -w ModelAvailable\n\n\n{}\n\n\ncat ./experiments/add1020.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: add1020\nspec:\n  default: add10\n  candidates:\n  - name: add10\n    weight: 50\n  - name: add20\n    weight: 50\n\n\nseldon experiment start -f ./experiments/add1020.yaml\n\n\nseldon experiment status add1020 -w | jq -M .\n\n\n{\n  \"experimentName\": \"add1020\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon model infer add10 -i 50  --inference-mode grpc \\\n  '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::22 :add20_1::28]\n\n\n\nseldon pipeline infer pipeline-add10 -i 100 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::24 :add20_1::32 :mul10_1::44 :pipeline-add10.pipeline::56 :pipeline-mul10.pipeline::44]\n\n\n\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-request-id:[cieovf0fh5ss739279u0]\n< x-envoy-upstream-service-time:[5]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< date:[Thu, 29 Jun 2023 14:05:48 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieovf8fh5ss739279ug]\n< x-envoy-upstream-service-time:[6]\n< date:[Thu, 29 Jun 2023 14:05:49 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :add20_1: :pipeline-add10.pipeline:]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[21, 22, 23, 24]}}]}\n\n\n\nseldon experiment stop addmul10\nseldon experiment stop add1020\nseldon pipeline unload pipeline-add10\nseldon pipeline unload pipeline-mul10\nseldon model unload add10\nseldon model unload add20\nseldon model unload mul10\n\n\n\n\nModel Mirror Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate a model with a mirror.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nCreate an experiment that modifies in which we mirror traffic to iris also to iris2.\ncat ./experiments/sklearn-mirror.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: sklearn-mirror\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 100\n  mirror:\n    name: iris2\n    percent: 100\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/sklearn-mirror.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status sklearn-mirror -w | jq -M .\n\n\n{\n  \"experimentName\": \"sklearn-mirror\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nWe get responses from iris but all requests would also have been mirrored to iris2\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nWe can check the local prometheus port from the agent to validate requests went to iris2\ncurl -s 0.0.0:9006/metrics | grep seldon_model_infer_total | grep iris2_1\n\n\nseldon_model_infer_total{code=\"200\",method_type=\"rest\",model=\"iris\",model_internal=\"iris2_1\",server=\"mlserver\",server_replica=\"0\"} 50\n\n\n\nStop the experiment\nseldon experiment stop sklearn-mirror\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n\n", "model-experiment": "\nModel Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate experiments.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nCreate an experiment that modifies the iris model to add a second model splitting traffic 50/50 between the two.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::27 :iris_1::23]\n\n\n\nShow sticky session header x-seldon-route that is returned\nseldon model infer iris --show-headers \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\n> POST /v2/models/iris/infer HTTP/1.1\n> Host: localhost:9000\n> Content-Type:[application/json]\n> Seldon-Model:[iris]\n\n< X-Seldon-Route:[:iris_1:]\n< Ce-Id:[463e96ad-645f-4442-8890-4c340b58820b]\n< Traceparent:[00-fe9e87fcbe4be98ed82fb76166e15ceb-d35e7ac96bd8b718-01]\n< X-Envoy-Upstream-Service-Time:[3]\n< Ce-Specversion:[0.3]\n< Date:[Thu, 29 Jun 2023 14:03:03 GMT]\n< Ce-Source:[io.seldon.serving.deployment.mlserver]\n< Content-Type:[application/json]\n< Server:[envoy]\n< X-Request-Id:[cieou5ofh5ss73fbjdu0]\n< Ce-Endpoint:[iris_1]\n< Ce-Modelid:[iris_1]\n< Ce-Type:[io.seldon.serving.inference.response]\n< Content-Length:[213]\n< Ce-Inferenceservicename:[mlserver]\n< Ce-Requestid:[463e96ad-645f-4442-8890-4c340b58820b]\n\n{\n\t\"model_name\": \"iris_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"463e96ad-645f-4442-8890-4c340b58820b\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t2\n\t\t\t]\n\t\t}\n\t]\n}\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon model infer iris -s -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris --inference-mode grpc -s -i 50\\\n   '{\"model_name\":\"iris\",\"inputs\":[{\"name\":\"input\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[1,4]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nStop the experiment\nseldon experiment stop experiment-sample\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n", "pipeline-experiment": "\nPipeline Experiment\u00b6\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n\n\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n\n\ncat ./pipelines/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-mul10\nspec:\n  steps:\n    - name: mul10\n  output:\n    steps:\n    - mul10\n\n\ncat ./pipelines/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-add10\nspec:\n  steps:\n    - name: add10\n  output:\n    steps:\n    - add10\n\n\nseldon pipeline load -f ./pipelines/add10.yaml\nseldon pipeline load -f ./pipelines/mul10.yaml\n\n\nseldon pipeline status pipeline-add10 -w PipelineReady\nseldon pipeline status pipeline-mul10 -w PipelineReady\n\n\n{\"pipelineName\":\"pipeline-add10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-add10\", \"uid\":\"cieov47l80lc739juklg\", \"version\":1, \"steps\":[{\"name\":\"add10\"}], \"output\":{\"steps\":[\"add10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.460868091Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"pipeline-mul10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-mul10\", \"uid\":\"cieov47l80lc739jukm0\", \"version\":1, \"steps\":[{\"name\":\"mul10\"}], \"output\":{\"steps\":[\"mul10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:05:04.631980330Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer pipeline-add10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          11,\n          12,\n          13,\n          14\n        ]\n      }\n    }\n  ]\n}\n\n\nseldon pipeline infer pipeline-mul10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}' | jq -M .\n\n\n{\n  \"outputs\": [\n    {\n      \"name\": \"OUTPUT\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        \"4\"\n      ],\n      \"contents\": {\n        \"fp32Contents\": [\n          10,\n          20,\n          30,\n          40\n        ]\n      }\n    }\n  ]\n}\n\n\ncat ./experiments/addmul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 50\n  - name: pipeline-mul10\n    weight: 50\n\n\nseldon experiment start -f ./experiments/addmul10.yaml\n\n\nseldon experiment status addmul10 -w | jq -M .\n\n\n{\n  \"experimentName\": \"addmul10\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon pipeline infer pipeline-add10 -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::28 :mul10_1::22 :pipeline-add10.pipeline::28 :pipeline-mul10.pipeline::22]\n\n\n\nUse sticky session key passed by last infer request to ensure same route is taken each time.\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieov8ofh5ss739277i0]\n< date:[Thu, 29 Jun 2023 14:05:23 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-envoy-upstream-service-time:[6]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< x-forwarded-proto:[http]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :pipeline-add10.pipeline:]\n< x-request-id:[cieov90fh5ss739277ig]\n< x-envoy-upstream-service-time:[7]\n< date:[Thu, 29 Jun 2023 14:05:24 GMT]\n< server:[envoy]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s -i 50 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::50 :pipeline-add10.pipeline::150]\n\n\n\ncat ./models/add20.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add20\nspec:\n  storageUri: \"gs://seldon-models/triton/add20\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add20.yaml\n\n\n{}\n\n\nseldon model status add20 -w ModelAvailable\n\n\n{}\n\n\ncat ./experiments/add1020.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: add1020\nspec:\n  default: add10\n  candidates:\n  - name: add10\n    weight: 50\n  - name: add20\n    weight: 50\n\n\nseldon experiment start -f ./experiments/add1020.yaml\n\n\nseldon experiment status add1020 -w | jq -M .\n\n\n{\n  \"experimentName\": \"add1020\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon model infer add10 -i 50  --inference-mode grpc \\\n  '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::22 :add20_1::28]\n\n\n\nseldon pipeline infer pipeline-add10 -i 100 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\nSuccess: map[:add10_1::24 :add20_1::32 :mul10_1::44 :pipeline-add10.pipeline::56 :pipeline-mul10.pipeline::44]\n\n\n\nseldon pipeline infer pipeline-add10 --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-request-id:[cieovf0fh5ss739279u0]\n< x-envoy-upstream-service-time:[5]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n< date:[Thu, 29 Jun 2023 14:05:48 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\n\nseldon pipeline infer pipeline-add10 -s --show-headers --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n> /inference.GRPCInferenceService/ModelInfer HTTP/2\n> Host: localhost:9000\n> x-seldon-route:[:add10_1: :pipeline-add10.pipeline:]\n> seldon-model:[pipeline-add10.pipeline]\n\n< x-forwarded-proto:[http]\n< x-envoy-expected-rq-timeout-ms:[60000]\n< x-request-id:[cieovf8fh5ss739279ug]\n< x-envoy-upstream-service-time:[6]\n< date:[Thu, 29 Jun 2023 14:05:49 GMT]\n< server:[envoy]\n< content-type:[application/grpc]\n< x-seldon-route:[:add10_1: :pipeline-add10.pipeline: :add20_1: :pipeline-add10.pipeline:]\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[21, 22, 23, 24]}}]}\n\n\n\nseldon experiment stop addmul10\nseldon experiment stop add1020\nseldon pipeline unload pipeline-add10\nseldon pipeline unload pipeline-mul10\nseldon model unload add10\nseldon model unload add20\nseldon model unload mul10\n\n\n", "model-mirror-experiment": "\nModel Mirror Experiment\u00b6\nWe will use two SKlearn Iris classification models to illustrate a model with a mirror.\ncat ./models/sklearn1.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\ncat ./models/sklearn2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris2\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  requirements:\n  - sklearn\n\n\nLoad both models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\n\n\n{}\n{}\n\n\nWait for both models to be ready.\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\n\n\n{}\n{}\n\n\nCreate an experiment that modifies in which we mirror traffic to iris also to iris2.\ncat ./experiments/sklearn-mirror.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: sklearn-mirror\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 100\n  mirror:\n    name: iris2\n    percent: 100\n\n\nStart the experiment.\nseldon experiment start -f ./experiments/sklearn-mirror.yaml\n\n\nWait for the experiment to be ready.\nseldon experiment status sklearn-mirror -w | jq -M .\n\n\n{\n  \"experimentName\": \"sklearn-mirror\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nWe get responses from iris but all requests would also have been mirrored to iris2\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nWe can check the local prometheus port from the agent to validate requests went to iris2\ncurl -s 0.0.0:9006/metrics | grep seldon_model_infer_total | grep iris2_1\n\n\nseldon_model_infer_total{code=\"200\",method_type=\"rest\",model=\"iris\",model_internal=\"iris2_1\",server=\"mlserver\",server_replica=\"0\"} 50\n\n\n\nStop the experiment\nseldon experiment stop sklearn-mirror\n\n\nUnload both models.\nseldon model unload iris\nseldon model unload iris2\n\n\n", "pipeline-mirror-experiment": "\nPipeline Mirror Experiment\u00b6\ncat ./models/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: add10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/add10\"\n  requirements:\n  - triton\n  - python\n\n\ncat ./models/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: mul10\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/triton_23-03/mul10\"\n  requirements:\n  - triton\n  - python\n\n\nseldon model load -f ./models/add10.yaml\nseldon model load -f ./models/mul10.yaml\n\n\n{}\n{}\n\n\nseldon model status add10 -w ModelAvailable\nseldon model status mul10 -w ModelAvailable\n\n\n{}\n{}\n\n\ncat ./pipelines/mul10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-mul10\nspec:\n  steps:\n    - name: mul10\n  output:\n    steps:\n    - mul10\n\n\ncat ./pipelines/add10.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: pipeline-add10\nspec:\n  steps:\n    - name: add10\n  output:\n    steps:\n    - add10\n\n\nseldon pipeline load -f ./pipelines/add10.yaml\nseldon pipeline load -f ./pipelines/mul10.yaml\n\n\nseldon pipeline status pipeline-add10 -w PipelineReady\nseldon pipeline status pipeline-mul10 -w PipelineReady\n\n\n{\"pipelineName\":\"pipeline-add10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-add10\", \"uid\":\"ciep072i8ufs73flaipg\", \"version\":1, \"steps\":[{\"name\":\"add10\"}], \"output\":{\"steps\":[\"add10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:07:24.903503109Z\", \"modelsReady\":true}}]}\n{\"pipelineName\":\"pipeline-mul10\", \"versions\":[{\"pipeline\":{\"name\":\"pipeline-mul10\", \"uid\":\"ciep072i8ufs73flaiq0\", \"version\":1, \"steps\":[{\"name\":\"mul10\"}], \"output\":{\"steps\":[\"mul10.outputs\"]}, \"kubernetesMeta\":{}}, \"state\":{\"pipelineVersion\":1, \"status\":\"PipelineReady\", \"reason\":\"created pipeline\", \"lastChangeTimestamp\":\"2023-06-29T14:07:25.082642153Z\", \"modelsReady\":true}}]}\n\n\nseldon pipeline infer pipeline-add10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\nseldon pipeline infer pipeline-mul10 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\n\n\ncat ./experiments/addmul10-mirror.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: addmul10-mirror\nspec:\n  default: pipeline-add10\n  resourceType: pipeline\n  candidates:\n  - name: pipeline-add10\n    weight: 100\n  mirror:\n    name: pipeline-mul10\n    percent: 100\n\n\nseldon experiment start -f ./experiments/addmul10-mirror.yaml\n\n\nseldon experiment status addmul10-mirror -w | jq -M .\n\n\n{\n  \"experimentName\": \"addmul10-mirror\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nseldon pipeline infer pipeline-add10 -i 1 --inference-mode grpc \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"contents\":{\"fp32_contents\":[1,2,3,4]},\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[11, 12, 13, 14]}}]}\n\n\nLet\u2019s check that the mul10 model was called.\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep mul10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"mul10\",model_internal=\"mul10_1\",server=\"triton\",server_replica=\"0\"} 2\n\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep add10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"add10\",model_internal=\"add10_1\",server=\"triton\",server_replica=\"0\"} 2\n\n\n\nLet\u2019s do an http call and check agaib the two models\nseldon pipeline infer pipeline-add10 -i 1 \\\n '{\"model_name\":\"add10\",\"inputs\":[{\"name\":\"INPUT\",\"data\":[1,2,3,4],\"datatype\":\"FP32\",\"shape\":[4]}]}'\n\n\n{\n\t\"model_name\": \"\",\n\t\"outputs\": [\n\t\t{\n\t\t\t\"data\": [\n\t\t\t\t11,\n\t\t\t\t12,\n\t\t\t\t13,\n\t\t\t\t14\n\t\t\t],\n\t\t\t\"name\": \"OUTPUT\",\n\t\t\t\"shape\": [\n\t\t\t\t4\n\t\t\t],\n\t\t\t\"datatype\": \"FP32\"\n\t\t}\n\t]\n}\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep mul10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"mul10\",model_internal=\"mul10_1\",server=\"triton\",server_replica=\"0\"} 3\n\n\n\ncurl -s 0.0.0:9007/metrics | grep seldon_model_infer_total | grep add10_1\n\n\nseldon_model_infer_total{code=\"OK\",method_type=\"grpc\",model=\"add10\",model_internal=\"add10_1\",server=\"triton\",server_replica=\"0\"} 3\n\n\n\nseldon pipeline inspect pipeline-mul10\n\n\nseldon.default.model.mul10.inputs\tciep0bofh5ss73dpdiq0\t{\"inputs\":[{\"name\":\"INPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[1, 2, 3, 4]}}]}\nseldon.default.model.mul10.outputs\tciep0bofh5ss73dpdiq0\t{\"modelName\":\"mul10_1\", \"modelVersion\":\"1\", \"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\nseldon.default.pipeline.pipeline-mul10.inputs\tciep0bofh5ss73dpdiq0\t{\"inputs\":[{\"name\":\"INPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[1, 2, 3, 4]}}]}\nseldon.default.pipeline.pipeline-mul10.outputs\tciep0bofh5ss73dpdiq0\t{\"outputs\":[{\"name\":\"OUTPUT\", \"datatype\":\"FP32\", \"shape\":[\"4\"], \"contents\":{\"fp32Contents\":[10, 20, 30, 40]}}]}\n\n\n\nseldon experiment stop addmul10-mirror\nseldon pipeline unload pipeline-add10\nseldon pipeline unload pipeline-mul10\nseldon model unload add10\nseldon model unload mul10\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/local-experiments.html", "key": "examples/local-experiments"}}, "examples/income": {"sections": {"production-income-classifier-with-drift-outlier-and-explanations": "\nProduction Income Classifier with Drift, Outlier and Explanations\u00b6\nRun these examples from the samples/examples/income_classifier folder.\n\nTabular Income Classifier Production Deployment\u00b6\nTo run this notebook you need the inference data. This can be acquired in two ways:\n\nRun make train or,\ngsutil cp -R gs://seldon-models/scv2/examples/income/infer-data .\n\nimport numpy as np\nimport json\nimport requests\n\n\nwith open('./infer-data/test.npy', 'rb') as f:\n    x_ref = np.load(f)\n    x_h1 = np.load(f)\n    y_ref = np.load(f)\n    x_outlier = np.load(f)\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\n\ndef infer(resourceName: str, batchSz: int, requestType: str):\n    if requestType == \"outlier\":\n        rows = x_outlier[0:0+batchSz]\n    elif requestType == \"drift\":\n        rows = x_h1[0:0+batchSz]\n    else:\n        rows = x_ref[0:0+batchSz]\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [batchSz, rows.shape[1]]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    print(response_raw)\n    print(response_raw.json())\n\n\n\nPipeline with model, drift detector and outlier detector\u00b6\ncat ../../models/income-preprocess.yaml\necho \"---\"\ncat ../../models/income.yaml\necho \"---\"\ncat ../../models/income-drift.yaml\necho \"---\"\ncat ../../models/income-outlier.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-preprocess\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/preprocessor\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/income-preprocess.yaml\nseldon model load -f ../../models/income.yaml\nseldon model load -f ../../models/income-drift.yaml\nseldon model load -f ../../models/income-outlier.yaml\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model status income-preprocess -w ModelAvailable | jq .\nseldon model status income -w ModelAvailable | jq .\nseldon model status income-drift -w ModelAvailable | jq .\nseldon model status income-outlier -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n{}\n\n\ncat ../../pipelines/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: income-production\nspec:\n  steps:\n    - name: income\n    - name: income-preprocess\n    - name: income-outlier\n      inputs:\n      - income-preprocess\n    - name: income-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - income\n    - income-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/income.yaml\n\n\nseldon pipeline status income-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"income-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"income-production\",\n        \"uid\": \"cifej8iufmbc73e5int0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"income\"\n          },\n          {\n            \"name\": \"income-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"income-outlier\",\n            \"inputs\": [\n              \"income-preprocess.outputs\"\n            ]\n          },\n          {\n            \"name\": \"income-preprocess\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"income.outputs\",\n            \"income-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:41:38.343754921Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nShow predictions from reference set. Should not be drift or outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"normal\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifej9gfh5ss738i5br0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\nShow predictions from drift data. Should be drift and probably not outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"drift\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejaofh5ss738i5brg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\nShow predictions from outlier data. Should be outliers and probably not drift.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"outlier\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejb8fh5ss738i5bs0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\n\n\nExplanations\u00b6\ncat ../../models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nseldon model load -f ../../models/income-explainer.yaml\n\n\n{}\n\n\nseldon model status income-explainer -w ModelAvailable | jq .\n\n\n{}\n\n\nbatchSz=1\nprint(y_ref[0:batchSz])\ninfer(\"income-explainer\",batchSz,\"normal\")\n\n\n[0]\n<Response [200]>\n{'model_name': 'income-explainer_1', 'model_version': '1', 'id': 'cdd68ba5-c569-4930-886f-fbdc26e24866', 'parameters': {}, 'outputs': [{'name': 'explanation', 'shape': [1, 1], 'datatype': 'BYTES', 'parameters': {'content_type': 'str'}, 'data': ['{\"meta\": {\"name\": \"AnchorTabular\", \"type\": [\"blackbox\"], \"explanations\": [\"local\"], \"params\": {\"seed\": 1, \"disc_perc\": [25, 50, 75], \"threshold\": 0.95, \"delta\": 0.1, \"tau\": 0.15, \"batch_size\": 100, \"coverage_samples\": 10000, \"beam_size\": 1, \"stop_on_first\": false, \"max_anchor_size\": null, \"min_samples_start\": 100, \"n_covered_ex\": 10, \"binary_cache_size\": 10000, \"cache_margin\": 1000, \"verbose\": false, \"verbose_every\": 1, \"kwargs\": {}}, \"version\": \"0.9.1\"}, \"data\": {\"anchor\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"precision\": 0.9942028985507246, \"coverage\": 0.0657, \"raw\": {\"feature\": [3, 5, 8], \"mean\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"precision\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"coverage\": [0.3043, 0.069, 0.0657], \"examples\": [{\"covered_true\": [[30, 0, 1, 1, 0, 1, 1, 0, 0, 0, 50, 2], [49, 4, 2, 1, 6, 0, 4, 1, 0, 0, 60, 9], [39, 2, 5, 1, 5, 0, 4, 1, 0, 0, 40, 9], [33, 4, 2, 1, 5, 0, 4, 1, 0, 0, 40, 9], [63, 4, 1, 1, 8, 1, 4, 0, 0, 0, 40, 9], [23, 4, 1, 1, 7, 1, 4, 1, 0, 0, 66, 8], [45, 4, 1, 1, 8, 0, 1, 1, 0, 0, 40, 1], [54, 4, 1, 1, 8, 4, 4, 1, 0, 0, 45, 9], [32, 6, 1, 1, 8, 4, 2, 0, 0, 0, 30, 9], [40, 5, 1, 1, 2, 0, 4, 1, 0, 0, 40, 9]], \"covered_false\": [[57, 4, 5, 1, 5, 0, 4, 1, 0, 1977, 45, 9], [53, 0, 5, 1, 0, 1, 4, 0, 8614, 0, 35, 9], [37, 4, 1, 1, 5, 0, 4, 1, 0, 0, 45, 9], [53, 4, 5, 1, 8, 0, 4, 1, 0, 1977, 55, 9], [35, 4, 1, 1, 8, 0, 4, 1, 7688, 0, 50, 9], [32, 4, 1, 1, 5, 1, 4, 1, 0, 0, 40, 9], [42, 4, 1, 1, 5, 0, 4, 1, 99999, 0, 40, 9], [32, 4, 1, 1, 8, 0, 4, 1, 15024, 0, 50, 9], [53, 7, 5, 1, 8, 0, 4, 1, 0, 0, 42, 9], [52, 1, 1, 1, 8, 0, 4, 1, 0, 0, 45, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[52, 7, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [27, 4, 1, 1, 8, 3, 4, 1, 0, 0, 40, 9], [28, 4, 1, 1, 6, 3, 4, 1, 0, 0, 60, 9], [46, 6, 5, 1, 2, 3, 4, 1, 0, 0, 50, 9], [53, 2, 5, 1, 5, 3, 2, 0, 0, 1669, 35, 9], [27, 4, 5, 1, 8, 3, 4, 0, 0, 0, 40, 9], [25, 4, 1, 1, 8, 3, 4, 0, 0, 0, 40, 9], [29, 6, 5, 1, 2, 3, 4, 1, 0, 0, 30, 9], [64, 0, 1, 1, 0, 3, 4, 1, 0, 0, 50, 9], [63, 0, 5, 1, 0, 3, 4, 1, 0, 0, 30, 9]], \"covered_false\": [[50, 5, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [45, 6, 1, 1, 6, 3, 4, 1, 14084, 0, 45, 9], [37, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 40, 9], [33, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [41, 6, 5, 1, 8, 3, 4, 1, 7298, 0, 70, 9], [42, 6, 1, 1, 2, 3, 4, 1, 15024, 0, 60, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[41, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [55, 2, 5, 1, 8, 3, 4, 1, 0, 0, 50, 9], [35, 4, 5, 1, 5, 3, 4, 0, 0, 0, 32, 9], [31, 4, 1, 1, 2, 3, 4, 1, 0, 0, 40, 9], [47, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [33, 4, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [58, 0, 1, 1, 0, 3, 4, 0, 0, 0, 50, 9], [44, 6, 1, 1, 2, 3, 4, 1, 0, 0, 90, 9], [30, 4, 1, 1, 6, 3, 4, 1, 0, 0, 40, 9], [25, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9]], \"covered_false\": [], \"uncovered_true\": [], \"uncovered_false\": []}], \"all_precision\": 0, \"num_preds\": 1000000, \"success\": true, \"names\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"prediction\": [0], \"instance\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \"instances\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}']}]}\n\n\n\n\n\nCleanup\u00b6\nseldon pipeline unload income-production\nseldon model unload income-preprocess\nseldon model unload income\nseldon model unload income-drift\nseldon model unload income-outlier\nseldon model unload income-explainer\n\n\n\n\n\n\n\n", "tabular-income-classifier-production-deployment": "\nTabular Income Classifier Production Deployment\u00b6\nTo run this notebook you need the inference data. This can be acquired in two ways:\n\nRun make train or,\ngsutil cp -R gs://seldon-models/scv2/examples/income/infer-data .\n\nimport numpy as np\nimport json\nimport requests\n\n\nwith open('./infer-data/test.npy', 'rb') as f:\n    x_ref = np.load(f)\n    x_h1 = np.load(f)\n    y_ref = np.load(f)\n    x_outlier = np.load(f)\n\n\nreqJson = json.loads('{\"inputs\":[{\"name\":\"input_1\",\"data\":[],\"datatype\":\"FP32\",\"shape\":[]}]}')\nurl = \"http://0.0.0.0:9000/v2/models/model/infer\"\n\n\ndef infer(resourceName: str, batchSz: int, requestType: str):\n    if requestType == \"outlier\":\n        rows = x_outlier[0:0+batchSz]\n    elif requestType == \"drift\":\n        rows = x_h1[0:0+batchSz]\n    else:\n        rows = x_ref[0:0+batchSz]\n    reqJson[\"inputs\"][0][\"data\"] = rows.flatten().tolist()\n    reqJson[\"inputs\"][0][\"shape\"] = [batchSz, rows.shape[1]]\n    headers = {\"Content-Type\": \"application/json\", \"seldon-model\":resourceName}\n    response_raw = requests.post(url, json=reqJson, headers=headers)\n    print(response_raw)\n    print(response_raw.json())\n\n\n\nPipeline with model, drift detector and outlier detector\u00b6\ncat ../../models/income-preprocess.yaml\necho \"---\"\ncat ../../models/income.yaml\necho \"---\"\ncat ../../models/income-drift.yaml\necho \"---\"\ncat ../../models/income-outlier.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-preprocess\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/preprocessor\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/income-preprocess.yaml\nseldon model load -f ../../models/income.yaml\nseldon model load -f ../../models/income-drift.yaml\nseldon model load -f ../../models/income-outlier.yaml\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model status income-preprocess -w ModelAvailable | jq .\nseldon model status income -w ModelAvailable | jq .\nseldon model status income-drift -w ModelAvailable | jq .\nseldon model status income-outlier -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n{}\n\n\ncat ../../pipelines/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: income-production\nspec:\n  steps:\n    - name: income\n    - name: income-preprocess\n    - name: income-outlier\n      inputs:\n      - income-preprocess\n    - name: income-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - income\n    - income-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/income.yaml\n\n\nseldon pipeline status income-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"income-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"income-production\",\n        \"uid\": \"cifej8iufmbc73e5int0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"income\"\n          },\n          {\n            \"name\": \"income-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"income-outlier\",\n            \"inputs\": [\n              \"income-preprocess.outputs\"\n            ]\n          },\n          {\n            \"name\": \"income-preprocess\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"income.outputs\",\n            \"income-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:41:38.343754921Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nShow predictions from reference set. Should not be drift or outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"normal\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifej9gfh5ss738i5br0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\nShow predictions from drift data. Should be drift and probably not outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"drift\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejaofh5ss738i5brg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\nShow predictions from outlier data. Should be outliers and probably not drift.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"outlier\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejb8fh5ss738i5bs0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\n\n\nExplanations\u00b6\ncat ../../models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nseldon model load -f ../../models/income-explainer.yaml\n\n\n{}\n\n\nseldon model status income-explainer -w ModelAvailable | jq .\n\n\n{}\n\n\nbatchSz=1\nprint(y_ref[0:batchSz])\ninfer(\"income-explainer\",batchSz,\"normal\")\n\n\n[0]\n<Response [200]>\n{'model_name': 'income-explainer_1', 'model_version': '1', 'id': 'cdd68ba5-c569-4930-886f-fbdc26e24866', 'parameters': {}, 'outputs': [{'name': 'explanation', 'shape': [1, 1], 'datatype': 'BYTES', 'parameters': {'content_type': 'str'}, 'data': ['{\"meta\": {\"name\": \"AnchorTabular\", \"type\": [\"blackbox\"], \"explanations\": [\"local\"], \"params\": {\"seed\": 1, \"disc_perc\": [25, 50, 75], \"threshold\": 0.95, \"delta\": 0.1, \"tau\": 0.15, \"batch_size\": 100, \"coverage_samples\": 10000, \"beam_size\": 1, \"stop_on_first\": false, \"max_anchor_size\": null, \"min_samples_start\": 100, \"n_covered_ex\": 10, \"binary_cache_size\": 10000, \"cache_margin\": 1000, \"verbose\": false, \"verbose_every\": 1, \"kwargs\": {}}, \"version\": \"0.9.1\"}, \"data\": {\"anchor\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"precision\": 0.9942028985507246, \"coverage\": 0.0657, \"raw\": {\"feature\": [3, 5, 8], \"mean\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"precision\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"coverage\": [0.3043, 0.069, 0.0657], \"examples\": [{\"covered_true\": [[30, 0, 1, 1, 0, 1, 1, 0, 0, 0, 50, 2], [49, 4, 2, 1, 6, 0, 4, 1, 0, 0, 60, 9], [39, 2, 5, 1, 5, 0, 4, 1, 0, 0, 40, 9], [33, 4, 2, 1, 5, 0, 4, 1, 0, 0, 40, 9], [63, 4, 1, 1, 8, 1, 4, 0, 0, 0, 40, 9], [23, 4, 1, 1, 7, 1, 4, 1, 0, 0, 66, 8], [45, 4, 1, 1, 8, 0, 1, 1, 0, 0, 40, 1], [54, 4, 1, 1, 8, 4, 4, 1, 0, 0, 45, 9], [32, 6, 1, 1, 8, 4, 2, 0, 0, 0, 30, 9], [40, 5, 1, 1, 2, 0, 4, 1, 0, 0, 40, 9]], \"covered_false\": [[57, 4, 5, 1, 5, 0, 4, 1, 0, 1977, 45, 9], [53, 0, 5, 1, 0, 1, 4, 0, 8614, 0, 35, 9], [37, 4, 1, 1, 5, 0, 4, 1, 0, 0, 45, 9], [53, 4, 5, 1, 8, 0, 4, 1, 0, 1977, 55, 9], [35, 4, 1, 1, 8, 0, 4, 1, 7688, 0, 50, 9], [32, 4, 1, 1, 5, 1, 4, 1, 0, 0, 40, 9], [42, 4, 1, 1, 5, 0, 4, 1, 99999, 0, 40, 9], [32, 4, 1, 1, 8, 0, 4, 1, 15024, 0, 50, 9], [53, 7, 5, 1, 8, 0, 4, 1, 0, 0, 42, 9], [52, 1, 1, 1, 8, 0, 4, 1, 0, 0, 45, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[52, 7, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [27, 4, 1, 1, 8, 3, 4, 1, 0, 0, 40, 9], [28, 4, 1, 1, 6, 3, 4, 1, 0, 0, 60, 9], [46, 6, 5, 1, 2, 3, 4, 1, 0, 0, 50, 9], [53, 2, 5, 1, 5, 3, 2, 0, 0, 1669, 35, 9], [27, 4, 5, 1, 8, 3, 4, 0, 0, 0, 40, 9], [25, 4, 1, 1, 8, 3, 4, 0, 0, 0, 40, 9], [29, 6, 5, 1, 2, 3, 4, 1, 0, 0, 30, 9], [64, 0, 1, 1, 0, 3, 4, 1, 0, 0, 50, 9], [63, 0, 5, 1, 0, 3, 4, 1, 0, 0, 30, 9]], \"covered_false\": [[50, 5, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [45, 6, 1, 1, 6, 3, 4, 1, 14084, 0, 45, 9], [37, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 40, 9], [33, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [41, 6, 5, 1, 8, 3, 4, 1, 7298, 0, 70, 9], [42, 6, 1, 1, 2, 3, 4, 1, 15024, 0, 60, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[41, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [55, 2, 5, 1, 8, 3, 4, 1, 0, 0, 50, 9], [35, 4, 5, 1, 5, 3, 4, 0, 0, 0, 32, 9], [31, 4, 1, 1, 2, 3, 4, 1, 0, 0, 40, 9], [47, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [33, 4, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [58, 0, 1, 1, 0, 3, 4, 0, 0, 0, 50, 9], [44, 6, 1, 1, 2, 3, 4, 1, 0, 0, 90, 9], [30, 4, 1, 1, 6, 3, 4, 1, 0, 0, 40, 9], [25, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9]], \"covered_false\": [], \"uncovered_true\": [], \"uncovered_false\": []}], \"all_precision\": 0, \"num_preds\": 1000000, \"success\": true, \"names\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"prediction\": [0], \"instance\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \"instances\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}']}]}\n\n\n\n\n\nCleanup\u00b6\nseldon pipeline unload income-production\nseldon model unload income-preprocess\nseldon model unload income\nseldon model unload income-drift\nseldon model unload income-outlier\nseldon model unload income-explainer\n\n\n\n\n\n\n", "pipeline-with-model-drift-detector-and-outlier-detector": "\nPipeline with model, drift detector and outlier detector\u00b6\ncat ../../models/income-preprocess.yaml\necho \"---\"\ncat ../../models/income.yaml\necho \"---\"\ncat ../../models/income-drift.yaml\necho \"---\"\ncat ../../models/income-outlier.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-preprocess\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/preprocessor\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-drift\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/drift-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\nseldon model load -f ../../models/income-preprocess.yaml\nseldon model load -f ../../models/income.yaml\nseldon model load -f ../../models/income-drift.yaml\nseldon model load -f ../../models/income-outlier.yaml\n\n\n{}\n{}\n{}\n{}\n\n\nseldon model status income-preprocess -w ModelAvailable | jq .\nseldon model status income -w ModelAvailable | jq .\nseldon model status income-drift -w ModelAvailable | jq .\nseldon model status income-outlier -w ModelAvailable | jq .\n\n\n{}\n{}\n{}\n{}\n\n\ncat ../../pipelines/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: income-production\nspec:\n  steps:\n    - name: income\n    - name: income-preprocess\n    - name: income-outlier\n      inputs:\n      - income-preprocess\n    - name: income-drift\n      batch:\n        size: 20\n  output:\n    steps:\n    - income\n    - income-outlier.outputs.is_outlier\n\n\nseldon pipeline load -f ../../pipelines/income.yaml\n\n\nseldon pipeline status income-production -w PipelineReady | jq -M .\n\n\n{\n  \"pipelineName\": \"income-production\",\n  \"versions\": [\n    {\n      \"pipeline\": {\n        \"name\": \"income-production\",\n        \"uid\": \"cifej8iufmbc73e5int0\",\n        \"version\": 1,\n        \"steps\": [\n          {\n            \"name\": \"income\"\n          },\n          {\n            \"name\": \"income-drift\",\n            \"batch\": {\n              \"size\": 20\n            }\n          },\n          {\n            \"name\": \"income-outlier\",\n            \"inputs\": [\n              \"income-preprocess.outputs\"\n            ]\n          },\n          {\n            \"name\": \"income-preprocess\"\n          }\n        ],\n        \"output\": {\n          \"steps\": [\n            \"income.outputs\",\n            \"income-outlier.outputs.is_outlier\"\n          ]\n        },\n        \"kubernetesMeta\": {}\n      },\n      \"state\": {\n        \"pipelineVersion\": 1,\n        \"status\": \"PipelineReady\",\n        \"reason\": \"created pipeline\",\n        \"lastChangeTimestamp\": \"2023-06-30T14:41:38.343754921Z\",\n        \"modelsReady\": true\n      }\n    }\n  ]\n}\n\n\nShow predictions from reference set. Should not be drift or outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"normal\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifej9gfh5ss738i5br0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\nShow predictions from drift data. Should be drift and probably not outliers.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"drift\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejaofh5ss738i5brg\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"1\"]}}\n\n\n\nShow predictions from outlier data. Should be outliers and probably not drift.\nbatchSz=20\nprint(y_ref[0:batchSz])\ninfer(\"income-production.pipeline\",batchSz,\"outlier\")\n\n\n[0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1]\n<Response [200]>\n{'model_name': '', 'outputs': [{'data': [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'name': 'predict', 'shape': [20, 1], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}, {'data': [1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], 'name': 'is_outlier', 'shape': [1, 20], 'datatype': 'INT64', 'parameters': {'content_type': 'np'}}]}\n\n\n\nseldon pipeline inspect income-production.income-drift.outputs.is_drift\n\n\nseldon.default.model.income-drift.outputs\tcifejb8fh5ss738i5bs0\t{\"name\":\"is_drift\", \"datatype\":\"INT64\", \"shape\":[\"1\", \"1\"], \"parameters\":{\"content_type\":{\"stringParam\":\"np\"}}, \"contents\":{\"int64Contents\":[\"0\"]}}\n\n\n\n", "explanations": "\nExplanations\u00b6\ncat ../../models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nseldon model load -f ../../models/income-explainer.yaml\n\n\n{}\n\n\nseldon model status income-explainer -w ModelAvailable | jq .\n\n\n{}\n\n\nbatchSz=1\nprint(y_ref[0:batchSz])\ninfer(\"income-explainer\",batchSz,\"normal\")\n\n\n[0]\n<Response [200]>\n{'model_name': 'income-explainer_1', 'model_version': '1', 'id': 'cdd68ba5-c569-4930-886f-fbdc26e24866', 'parameters': {}, 'outputs': [{'name': 'explanation', 'shape': [1, 1], 'datatype': 'BYTES', 'parameters': {'content_type': 'str'}, 'data': ['{\"meta\": {\"name\": \"AnchorTabular\", \"type\": [\"blackbox\"], \"explanations\": [\"local\"], \"params\": {\"seed\": 1, \"disc_perc\": [25, 50, 75], \"threshold\": 0.95, \"delta\": 0.1, \"tau\": 0.15, \"batch_size\": 100, \"coverage_samples\": 10000, \"beam_size\": 1, \"stop_on_first\": false, \"max_anchor_size\": null, \"min_samples_start\": 100, \"n_covered_ex\": 10, \"binary_cache_size\": 10000, \"cache_margin\": 1000, \"verbose\": false, \"verbose_every\": 1, \"kwargs\": {}}, \"version\": \"0.9.1\"}, \"data\": {\"anchor\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"precision\": 0.9942028985507246, \"coverage\": 0.0657, \"raw\": {\"feature\": [3, 5, 8], \"mean\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"precision\": [0.7914951989026063, 0.9400749063670412, 0.9942028985507246], \"coverage\": [0.3043, 0.069, 0.0657], \"examples\": [{\"covered_true\": [[30, 0, 1, 1, 0, 1, 1, 0, 0, 0, 50, 2], [49, 4, 2, 1, 6, 0, 4, 1, 0, 0, 60, 9], [39, 2, 5, 1, 5, 0, 4, 1, 0, 0, 40, 9], [33, 4, 2, 1, 5, 0, 4, 1, 0, 0, 40, 9], [63, 4, 1, 1, 8, 1, 4, 0, 0, 0, 40, 9], [23, 4, 1, 1, 7, 1, 4, 1, 0, 0, 66, 8], [45, 4, 1, 1, 8, 0, 1, 1, 0, 0, 40, 1], [54, 4, 1, 1, 8, 4, 4, 1, 0, 0, 45, 9], [32, 6, 1, 1, 8, 4, 2, 0, 0, 0, 30, 9], [40, 5, 1, 1, 2, 0, 4, 1, 0, 0, 40, 9]], \"covered_false\": [[57, 4, 5, 1, 5, 0, 4, 1, 0, 1977, 45, 9], [53, 0, 5, 1, 0, 1, 4, 0, 8614, 0, 35, 9], [37, 4, 1, 1, 5, 0, 4, 1, 0, 0, 45, 9], [53, 4, 5, 1, 8, 0, 4, 1, 0, 1977, 55, 9], [35, 4, 1, 1, 8, 0, 4, 1, 7688, 0, 50, 9], [32, 4, 1, 1, 5, 1, 4, 1, 0, 0, 40, 9], [42, 4, 1, 1, 5, 0, 4, 1, 99999, 0, 40, 9], [32, 4, 1, 1, 8, 0, 4, 1, 15024, 0, 50, 9], [53, 7, 5, 1, 8, 0, 4, 1, 0, 0, 42, 9], [52, 1, 1, 1, 8, 0, 4, 1, 0, 0, 45, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[52, 7, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [27, 4, 1, 1, 8, 3, 4, 1, 0, 0, 40, 9], [28, 4, 1, 1, 6, 3, 4, 1, 0, 0, 60, 9], [46, 6, 5, 1, 2, 3, 4, 1, 0, 0, 50, 9], [53, 2, 5, 1, 5, 3, 2, 0, 0, 1669, 35, 9], [27, 4, 5, 1, 8, 3, 4, 0, 0, 0, 40, 9], [25, 4, 1, 1, 8, 3, 4, 0, 0, 0, 40, 9], [29, 6, 5, 1, 2, 3, 4, 1, 0, 0, 30, 9], [64, 0, 1, 1, 0, 3, 4, 1, 0, 0, 50, 9], [63, 0, 5, 1, 0, 3, 4, 1, 0, 0, 30, 9]], \"covered_false\": [[50, 5, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [45, 6, 1, 1, 6, 3, 4, 1, 14084, 0, 45, 9], [37, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 40, 9], [33, 4, 1, 1, 8, 3, 4, 1, 15024, 0, 60, 9], [41, 6, 5, 1, 8, 3, 4, 1, 7298, 0, 70, 9], [42, 6, 1, 1, 2, 3, 4, 1, 15024, 0, 60, 9]], \"uncovered_true\": [], \"uncovered_false\": []}, {\"covered_true\": [[41, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [55, 2, 5, 1, 8, 3, 4, 1, 0, 0, 50, 9], [35, 4, 5, 1, 5, 3, 4, 0, 0, 0, 32, 9], [31, 4, 1, 1, 2, 3, 4, 1, 0, 0, 40, 9], [47, 4, 1, 1, 1, 3, 4, 1, 0, 0, 40, 9], [33, 4, 5, 1, 5, 3, 4, 1, 0, 0, 40, 9], [58, 0, 1, 1, 0, 3, 4, 0, 0, 0, 50, 9], [44, 6, 1, 1, 2, 3, 4, 1, 0, 0, 90, 9], [30, 4, 1, 1, 6, 3, 4, 1, 0, 0, 40, 9], [25, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9]], \"covered_false\": [], \"uncovered_true\": [], \"uncovered_false\": []}], \"all_precision\": 0, \"num_preds\": 1000000, \"success\": true, \"names\": [\"Marital Status = Never-Married\", \"Relationship = Own-child\", \"Capital Gain <= 0.00\"], \"prediction\": [0], \"instance\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \"instances\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}']}]}\n\n\n\n", "cleanup": "\nCleanup\u00b6\nseldon pipeline unload income-production\nseldon model unload income-preprocess\nseldon model unload income\nseldon model unload income-drift\nseldon model unload income-outlier\nseldon model unload income-explainer\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/income.html", "key": "examples/income"}}, "kubernetes/service-meshes": {"sections": {"service-meshes": "\nService Meshes\u00b6\nThe Seldon models and pipelines are exposed via a single service endpoint in the install namespace called seldon-mesh. All models, pipelines and experiments can be reached via this single Service endpoint by setting appropriate headers on the inference REST/gRPC request. By this means Seldon is agnostic to any service mesh you may wish to use in your organisation. We provide some example integrations for some example service meshes below (alphabetical order):\n\nAmbassador\nIstio\nTraefik\n\nWe welcome help to extend these to other service meshes.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/service-meshes/index.html", "key": "kubernetes/service-meshes"}}, "tutorials": {"sections": {"workflow-and-tutorials": "\nWorkflow and Tutorials\u00b6\n\nWorkflow\u00b6\nSeldon inference is built from atomic Model components. Models as shown here cover a wide range of artifacts including:\n\nCore machine learning models, e.g. a Tensorflow model.\nFeature transformations that might be built with custom python code.\nDrift detectors.\nOutlier detectors.\nExplainers\nAdversarial detectors.\n\nA typical workflow for a production machine learning setup might be as follows:\n\nYou create a Tensorflow model for your core application use case and test this model in isolation to validate.\nYou create SKLearn feature transformation component before your model to convert the input into the correct form for your model. You also create Drift and Outlier detectors using Seldon\u2019s open source Alibi-detect library and test these in isolation.\nYou join these components together into a Pipeline for the final production setup.\n\nThese steps are shown in the diagram below:\n\n\n\nWorked Examples\u00b6\nSee our selection of examples.\n\n", "workflow": "\nWorkflow\u00b6\nSeldon inference is built from atomic Model components. Models as shown here cover a wide range of artifacts including:\n\nCore machine learning models, e.g. a Tensorflow model.\nFeature transformations that might be built with custom python code.\nDrift detectors.\nOutlier detectors.\nExplainers\nAdversarial detectors.\n\nA typical workflow for a production machine learning setup might be as follows:\n\nYou create a Tensorflow model for your core application use case and test this model in isolation to validate.\nYou create SKLearn feature transformation component before your model to convert the input into the correct form for your model. You also create Drift and Outlier detectors using Seldon\u2019s open source Alibi-detect library and test these in isolation.\nYou join these components together into a Pipeline for the final production setup.\n\nThese steps are shown in the diagram below:\n\n", "worked-examples": "\nWorked Examples\u00b6\nSee our selection of examples.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/tutorials/index.html", "key": "tutorials"}}, "kubernetes/resources/serverconfig": {"sections": {"server-config": "\nServer Config\u00b6\n\nNote\nThis section is for advanced usage where you want to define new types of inference servers.\n\nServer configurations define how to create an inference server. By default one is provided for Seldon MLServer and one for NVIDIA Triton Inference Server. Both these servers support the V2 inference protocol which is a requirement for all inference servers. They define how the Kubernetes ReplicaSet is defined which includes the Seldon Agent reverse proxy as well as an Rclone server for downloading artifacts for the server. The Kustomize ServerConfig for MlServer is shown below:\n---\napiVersion: mlops.seldon.io/v1alpha1\nkind: ServerConfig\nmetadata:\n  name: mlserver\nspec:\n  podSpec:\n    terminationGracePeriodSeconds: 120\n    serviceAccountName: seldon-server\n    containers:\n    - image: rclone:latest\n      imagePullPolicy: IfNotPresent\n      name: rclone\n      ports:\n      - containerPort: 5572\n        name: rclone\n        protocol: TCP\n      lifecycle:\n        preStop:\n          httpGet:\n            port: 9007\n            path: terminate\n      resources:\n        requests:\n          cpu: \"200m\"\n          memory: '100M'\n      readinessProbe:\n        failureThreshold: 3\n        initialDelaySeconds: 5\n        periodSeconds: 5\n        successThreshold: 1\n        tcpSocket:\n          port: 5572\n        timeoutSeconds: 1\n      volumeMounts:\n      - mountPath: /mnt/agent\n        name: mlserver-models\n    - image: agent:latest\n      imagePullPolicy: IfNotPresent\n      command:\n        - /bin/agent\n      args:\n        - --tracing-config-path=/mnt/tracing/tracing.json\n      name: agent\n      env:\n      - name: SELDON_SERVER_CAPABILITIES\n        value: \"mlserver,alibi-detect,alibi-explain,huggingface,lightgbm,mlflow,python,sklearn,spark-mlib,xgboost\"\n      - name: SELDON_OVERCOMMIT_PERCENTAGE\n        value: \"10\"\n      - name: SELDON_MODEL_INFERENCE_LAG_THRESHOLD\n        value: \"30\"\n      - name: SELDON_MODEL_INACTIVE_SECONDS_THRESHOLD\n        value: \"600\"\n      - name: SELDON_SCALING_STATS_PERIOD_SECONDS\n        value: \"20\"\n      - name: SELDON_SERVER_HTTP_PORT\n        value: \"9000\"\n      - name: SELDON_SERVER_GRPC_PORT\n        value: \"9500\"\n      - name: SELDON_REVERSE_PROXY_HTTP_PORT\n        value: \"9001\"\n      - name: SELDON_REVERSE_PROXY_GRPC_PORT\n        value: \"9501\"\n      - name: SELDON_SCHEDULER_HOST\n        value: \"seldon-scheduler\"\n      - name: SELDON_SCHEDULER_PORT\n        value: \"9005\"\n      - name: SELDON_SCHEDULER_TLS_PORT\n        value: \"9055\"\n      - name: SELDON_METRICS_PORT\n        value: \"9006\"\n      - name: SELDON_DRAINER_PORT\n        value: \"9007\"\n      - name: AGENT_TLS_SECRET_NAME\n        value: \"\"\n      - name: AGENT_TLS_FOLDER_PATH\n        value: \"\"\n      - name: SELDON_SERVER_TYPE\n        value: \"mlserver\"\n      - name: SELDON_ENVOY_HOST\n        value: \"seldon-mesh\"\n      - name: SELDON_ENVOY_PORT\n        value: \"80\"\n      - name: POD_NAME\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.name\n      - name: POD_NAMESPACE\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.namespace\n      - name: MEMORY_REQUEST\n        valueFrom:\n          resourceFieldRef:\n            containerName: mlserver\n            resource: requests.memory\n      ports:\n      - containerPort: 9501\n        name: grpc\n        protocol: TCP\n      - containerPort: 9001\n        name: http\n        protocol: TCP\n      - containerPort: 9006\n        name: metrics\n        protocol: TCP\n      lifecycle:\n        preStop:\n          httpGet:\n            port: 9007\n            path: terminate\n      resources:\n        requests:\n          cpu: \"500m\"\n          memory: '500M'\n      volumeMounts:\n      - mountPath: /mnt/agent\n        name: mlserver-models\n      - name: config-volume\n        mountPath: /mnt/config\n      - name: tracing-config-volume\n        mountPath: /mnt/tracing\n    - image: mlserver:latest\n      imagePullPolicy: IfNotPresent\n      env:\n      - name: MLSERVER_HTTP_PORT\n        value: \"9000\"\n      - name: MLSERVER_GRPC_PORT\n        value: \"9500\"\n      - name: MLSERVER_MODELS_DIR\n        value: \"/mnt/agent/models\"\n      - name: MLSERVER_MODEL_PARALLEL_WORKERS\n        value: \"1\"\n      - name: MLSERVER_LOAD_MODELS_AT_STARTUP\n        value: \"false\"\n      - name: MLSERVER_GRPC_MAX_MESSAGE_LENGTH\n        value: \"1048576000\" # 100MB (100 * 1024 * 1024)\n      resources:\n        requests:\n          cpu: 1\n          memory: '1G'\n      lifecycle:\n        preStop:\n          httpGet:\n            port: 9007\n            path: terminate\n      livenessProbe:\n        httpGet:\n          path: /v2/health/live\n          port: server-http\n      readinessProbe:\n        httpGet:\n          path: /v2/health/live\n          port: server-http\n        initialDelaySeconds: 5\n        periodSeconds: 5\n      startupProbe:\n        httpGet:\n          path: /v2/health/live\n          port: server-http\n        failureThreshold: 10\n        periodSeconds: 10\n      name: mlserver\n      ports:\n      - containerPort: 9500\n        name: server-grpc\n        protocol: TCP\n      - containerPort: 9000\n        name: server-http\n        protocol: TCP\n      - containerPort: 8082\n        name: server-metrics\n      volumeMounts:\n      - mountPath: /mnt/agent\n        name: mlserver-models\n        readOnly: true\n      - mountPath: /mnt/certs\n        name: downstream-ca-certs\n        readOnly: true\n    securityContext:\n      fsGroup: 2000\n      runAsUser: 1000\n      runAsNonRoot: true\n    volumes:\n    - name: config-volume\n      configMap:\n        name: seldon-agent\n    - name: tracing-config-volume\n      configMap:\n        name: seldon-tracing\n    - name: downstream-ca-certs\n      secret:\n        secretName: seldon-downstream-server\n        optional: true\n  volumeClaimTemplates:\n  - name: mlserver-models\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/serverconfig/index.html", "key": "kubernetes/resources/serverconfig"}}, "apis": {"sections": {"apis": "\nAPIs\u00b6\nSeldon provides APIs for management and inference.\n\nAPI for inference\nScheduler API for management (Advanced)\nInternal APIs (Reference)\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/index.html", "key": "apis"}}, "outlier": {"sections": {"outlier-detection": "\nOutlier Detection\u00b6\nOutlier detection models are treated as any other Model. You can run any saved Alibi-Detect outlier detection model by adding the requirement alibi-detect.\nAn example outlier detection model from the CIFAR10 image classification example is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: cifar10-outlier\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/cifar10/outlier-detector\"\n  requirements:\n    - mlserver\n    - alibi-detect\n\n\n\nExamples\u00b6\n\nCIFAR10 image classification with outlier detector\nTabular income classification model with outlier detector\n\n\n", "examples": "\nExamples\u00b6\n\nCIFAR10 image classification with outlier detector\nTabular income classification model with outlier detector\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/outlier/index.html", "key": "outlier"}}, "kubernetes/resources/seldonruntime": {"sections": {"seldon-runtime": "\nSeldon Runtime\u00b6\nThe SeldonRuntime resource is used to create an instance of Seldon installed in a particular namespace.\ntype SeldonRuntimeSpec struct {\n\tSeldonConfig string              `json:\"seldonConfig\"`\n\tOverrides    []*OverrideSpec     `json:\"overrides,omitempty\"`\n\tConfig       SeldonConfiguration `json:\"config,omitempty\"`\n\t// +Optional\n\t// If set then when the referenced SeldonConfig changes we will NOT update the SeldonRuntime immediately.\n\t// Explicit changes to the SeldonRuntime itself will force a reconcile though\n\tDisableAutoUpdate bool `json:\"disableAutoUpdate,omitempty\"`\n}\n\ntype OverrideSpec struct {\n\tName        string         `json:\"name\"`\n\tDisable     bool           `json:\"disable,omitempty\"`\n\tReplicas    *int32         `json:\"replicas,omitempty\"`\n\tServiceType v1.ServiceType `json:\"serviceType,omitempty\"`\n\tPodSpec     *PodSpec       `json:\"podSpec,omitempty\"`\n}\n\n\nFor the definition of SeldonConfiguration above see the SeldonConfig resource.\nThe specification above contains overrides for the chosen SeldonConfig.\nTo override the PodSpec for a given component, the overrides field needs to specify the component name and the PodSpec needs to specify the container name, along with fields to override.\nFor instance, the following overrides the resource limits for cpu and memory in the hodometer component in the seldon-mesh namespace, while using values specified in the seldonConfig elsewhere (e.g. default).\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonRuntime\nmetadata:\n  name: seldon\n  namespace: seldon-mesh\nspec:\n  overrides:\n  - name: hodometer\n    podSpec:\n      containers:\n      - name: hodometer\n        resources:\n          limits:\n            memory: 64Mi\n            cpu: 20m\n  seldonConfig: default\n\n\nAs a minimal use you should just define the SeldonConfig to use as a base for this install, for example to install in the seldon-mesh namespace with the SeldonConfig named default:\napiVersion: mlops.seldon.io/v1alpha1\nkind: SeldonRuntime\nmetadata:\n  name: seldon\n  namespace: seldon-mesh  \nspec:\n  seldonConfig: default\n\n\nThe helm chart seldon-core-v2-runtime allows easy creation of this resource and associated default Servers for an installation of Seldon in a particular namespace.\n\nSeldonConfig Update Propagation\u00b6\nWhen a SeldonConfig resource changes any SeldonRuntime resources that reference the changed SeldonConfig will also be updated immediately. If this behaviour is not desired you can set spec.disableAutoUpdate in the SeldonRuntime resource for it not be be updated immediately but only when it changes or any owned resource changes.\n\n", "seldonconfig-update-propagation": "\nSeldonConfig Update Propagation\u00b6\nWhen a SeldonConfig resource changes any SeldonRuntime resources that reference the changed SeldonConfig will also be updated immediately. If this behaviour is not desired you can set spec.disableAutoUpdate in the SeldonRuntime resource for it not be be updated immediately but only when it changes or any owned resource changes.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/seldonruntime/index.html", "key": "kubernetes/resources/seldonruntime"}}, "examples/experiment-versions": {"sections": {"experiments-versions": "\nExperiments Versions\u00b6\nRun these examples from the samples folder.\n\nSeldon V2 Experiment Version Tests\u00b6\nThis notebook will show how we can update running experiments.\n\nTest change candidate for a model\u00b6\nWe will use three SKlearn Iris classification models to illustrate experiment updates.\nLoad all models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models individually first.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nWe will start an experiment to change the iris endpoint to split traffic with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow when we call the iris model we should see a roughly 50/50 split between the two models.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::48 :iris_1::52]\n\n\n\nNow we update the experiment to change to a split with the iris3 model.\ncat ./experiments/ab-default-model2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris3\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model2.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow we should see a split with the iris3 model.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::42 :iris_1::58]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nNow the experiment has been stopped we check everything as before.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n\n\nTest change default model in an experiment\u00b6\nHere we test changing the model we want to split traffic on. We will use three SKlearn Iris classification models to illustrate.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models to verify initial conditions.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nNow we start an experiment to change calls to the iris model to split with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::51 :iris_1::49]\n\n\n\nNow let\u2019s change the model we want to experiment to modify to the iris3 model. Splitting between that and iris2.\ncat ./experiments/ab-default-model3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris3\n  candidates:\n  - name: iris3\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model3.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nLet\u2019s check the iris model is now as before but the iris3 model has traffic split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::25 :iris3_1::25]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nFinally, let\u2019s check now the experiment has stopped as is as at the start.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n\n\n\n\n\n", "seldon-v2-experiment-version-tests": "\nSeldon V2 Experiment Version Tests\u00b6\nThis notebook will show how we can update running experiments.\n\nTest change candidate for a model\u00b6\nWe will use three SKlearn Iris classification models to illustrate experiment updates.\nLoad all models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models individually first.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nWe will start an experiment to change the iris endpoint to split traffic with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow when we call the iris model we should see a roughly 50/50 split between the two models.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::48 :iris_1::52]\n\n\n\nNow we update the experiment to change to a split with the iris3 model.\ncat ./experiments/ab-default-model2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris3\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model2.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow we should see a split with the iris3 model.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::42 :iris_1::58]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nNow the experiment has been stopped we check everything as before.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n\n\nTest change default model in an experiment\u00b6\nHere we test changing the model we want to split traffic on. We will use three SKlearn Iris classification models to illustrate.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models to verify initial conditions.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nNow we start an experiment to change calls to the iris model to split with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::51 :iris_1::49]\n\n\n\nNow let\u2019s change the model we want to experiment to modify to the iris3 model. Splitting between that and iris2.\ncat ./experiments/ab-default-model3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris3\n  candidates:\n  - name: iris3\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model3.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nLet\u2019s check the iris model is now as before but the iris3 model has traffic split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::25 :iris3_1::25]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nFinally, let\u2019s check now the experiment has stopped as is as at the start.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n\n\n\n\n", "test-change-candidate-for-a-model": "\nTest change candidate for a model\u00b6\nWe will use three SKlearn Iris classification models to illustrate experiment updates.\nLoad all models.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models individually first.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nWe will start an experiment to change the iris endpoint to split traffic with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow when we call the iris model we should see a roughly 50/50 split between the two models.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::48 :iris_1::52]\n\n\n\nNow we update the experiment to change to a split with the iris3 model.\ncat ./experiments/ab-default-model2.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris3\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model2.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nNow we should see a split with the iris3 model.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::42 :iris_1::58]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nNow the experiment has been stopped we check everything as before.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n", "test-change-default-model-in-an-experiment": "\nTest change default model in an experiment\u00b6\nHere we test changing the model we want to split traffic on. We will use three SKlearn Iris classification models to illustrate.\nseldon model load -f ./models/sklearn1.yaml\nseldon model load -f ./models/sklearn2.yaml\nseldon model load -f ./models/sklearn3.yaml\n\n\n{}\n{}\n{}\n\n\nseldon model status iris -w ModelAvailable\nseldon model status iris2 -w ModelAvailable\nseldon model status iris3 -w ModelAvailable\n\n\n{}\n{}\n{}\n\n\nLet\u2019s call all three models to verify initial conditions.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nNow we start an experiment to change calls to the iris model to split with the iris2 model.\ncat ./experiments/ab-default-model.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris\n  candidates:\n  - name: iris\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nRun a set of calls and record which route the traffic took. There should be roughly a 50/50 split.\nseldon model infer iris -i 100 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::51 :iris_1::49]\n\n\n\nNow let\u2019s change the model we want to experiment to modify to the iris3 model. Splitting between that and iris2.\ncat ./experiments/ab-default-model3.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: experiment-sample\nspec:\n  default: iris3\n  candidates:\n  - name: iris3\n    weight: 50\n  - name: iris2\n    weight: 50\n\n\nseldon experiment start -f ./experiments/ab-default-model3.yaml\n\n\n{}\n\n\nseldon experiment status experiment-sample -w | jq -M .\n\n\n{\n  \"experimentName\": \"experiment-sample\",\n  \"active\": true,\n  \"candidatesReady\": true,\n  \"mirrorReady\": true,\n  \"statusDescription\": \"experiment active\",\n  \"kubernetesMeta\": {}\n}\n\n\nLet\u2019s check the iris model is now as before but the iris3 model has traffic split.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::25 :iris3_1::25]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon experiment stop experiment-sample\n\n\n{}\n\n\nFinally, let\u2019s check now the experiment has stopped as is as at the start.\nseldon model infer iris -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris_1::50]\n\n\n\nseldon model infer iris2 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris2_1::50]\n\n\n\nseldon model infer iris3 -i 50 \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[1, 2, 3, 4]]}]}'\n\n\nSuccess: map[:iris3_1::50]\n\n\n\nseldon model unload iris\nseldon model unload iris2\nseldon model unload iris3\n\n\n{}\n{}\n{}\n\n\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/experiment-versions.html", "key": "examples/experiment-versions"}}, "apis/inference": {"sections": {"inference-api": "\nInference API\u00b6\nSeldon inference servers must respect the following API specification.\n\nSeldon, KServe, NVIDIA V2 Inference API Spec\n\nIn future, Seldon may provide extensions for use with Pipelines, Experiments and Explainers.\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/apis/inference/index.html", "key": "apis/inference"}}, "cli/docs/seldon_model_infer": {"sections": {"seldon-model-infer": "\nseldon model infer\u00b6\nrun inference on a model\n\nSynopsis\u00b6\ncall a model with a given input and get a prediction\nseldon model infer <modelName> (data) [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        inference payload file\n      --header stringArray      add a header, e.g. key=value; use the flag multiple times to add more than one header\n  -h, --help                    help for infer\n      --inference-host string   seldon inference host (default \"0.0.0.0:9000\")\n      --inference-mode string   inference mode (rest or grpc) (default \"rest\")\n  -i, --iterations int          how many times to run inference (default 1)\n  -t, --seconds int             number of secs to run inference\n      --show-headers            show request and response headers\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -s, --sticky-session          use sticky session from last inference (only works with experiments)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n\n", "synopsis": "\nSynopsis\u00b6\ncall a model with a given input and get a prediction\nseldon model infer <modelName> (data) [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        inference payload file\n      --header stringArray      add a header, e.g. key=value; use the flag multiple times to add more than one header\n  -h, --help                    help for infer\n      --inference-host string   seldon inference host (default \"0.0.0.0:9000\")\n      --inference-mode string   inference mode (rest or grpc) (default \"rest\")\n  -i, --iterations int          how many times to run inference (default 1)\n  -t, --seconds int             number of secs to run inference\n      --show-headers            show request and response headers\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n  -s, --sticky-session          use sticky session from last inference (only works with experiments)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon model\t - manage models\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_model_infer.html", "key": "cli/docs/seldon_model_infer"}}, "getting-started/configuration": {"sections": {"configuration": "\nConfiguration\u00b6\nSeldon can be configured via various config files.\n\nKafka Configuration\u00b6\nWe allow configuration of the Kafka integration. In general this configuration looks like:\n{\n    \"topicPrefix\": \"seldon\",\n    \"bootstrap.servers\":\"kafka:9093\",\n    \"consumer\":{\n\t\"session.timeout.ms\":6000,\n\t\"auto.offset.reset\":\"earliest\",\n\t\"topic.metadata.propagation.max.ms\": 300000,\n\t\"message.max.bytes\":1000000000\n    },\n    \"producer\":{\n\t\"linger.ms\":0,\n\t\"message.max.bytes\":1000000000\n    },\n    \"streams\":{\n    }\n}\n\n\nThe top level keys are:\n\ntopicPrefix : the prefix to add to kafka topics created by Seldon\nconsumerGroupIdPrefix : the prefix to add to Kafka consumer group IDs created by Seldon\nbootstrap.servers : the global bootstrap kafka servers to use\nconsumer : consumer settings\nproducer : producer settings\nstreams : KStreams settings\n\nFor topicPrefix you can use any acceptable kafka topic characters which are a-z, A-Z, 0-9, . (dot), _ (underscore), and - (dash). We use . (dot) internally as topic naming separator so we would suggest you don\u2019t end your topic prefix with a dot for clarity. For illustration, an example topic could be seldon.default.model.mymodel.inputs where seldon is the topic prefix.\nThe consumerGroupIdPrefix will ensure that all consumer groups created have a given prefix.\n\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap called seldon-kafka whose default values are defined in the SeldonConfig custom resource.\n      bootstrap.servers: 'seldon-kafka-bootstrap.seldon-mesh:9092'\n      consumer:\n        auto.offset.reset: 'earliest'\n        message.max.bytes: '1000000000'\n        session.timeout.ms: '6000'\n        topic.metadata.propagation.max.ms: '300000'\n      consumerGroupIdPrefix: ''\n      debug: ''\n      producer:\n        linger.ms: '0'\n        message.max.bytes: '1000000000'\n      topicPrefix: 'seldon'\n\n\nWhen the SeldonRuntime is installed in a namespace a configMap will be created with these settings for Kafka configuration.\nTo customize the settings you can add and modify the Kafka configuration via Helm, for example below is a custom Helm values file that add compression for producers:\nconfig:\n  kafkaConfig:\n    producer:\n      compression.type: gzip\n\n\nTo use this with the SeldonRuntime Helm chart:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n    --namespace seldon-mesh \\\n    --values k8s/samples/values-runtime-kafka-compression.yaml\n\n\n\n\nTopic and consumer isolation\u00b6\nIf you use a shared Kafka cluster with other applications you may want to isolate the topic names and consumer group IDs from other users of the cluster to ensure there is no name clash. For this we provide two settings:\n\ntopicPrefix: set a prefix for all topics\nconsumerGroupIdPrefix: set a prefix for all consumer groups\n\nAn example to set this in the configuration when using the helm installation is showm below for creating the default SeldonConfig:\nhelm upgrade --install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n    --set controller.clusterwide=true \\\n    --set kafka.topicPrefix=myorg \\\n    --set kafka.consumerGroupIdPrefix=myorg\n\n\nYou can find a worked example here.\nYou can create alternate SeldonConfigs with different values or override values for particular SeldonRuntime installs.\n\n\n\nTracing Configuration\u00b6\nWe allow configuration of tracing. This file looks like:\n{\n  \"disable\": false,\n  \"otelExporterEndpoint\": \"otel-collector:4317\",\n  \"otelExporterProtocol\": \"grpc\",\n  \"ratio\": \"1\"\n}\n\n\nThe top level keys are:\n\nenable : whether to enable tracing\notelExporterEndpoint : The host and port for the OTEL exporter\notelExporterProtocol : The protocol for the OTEL exporter. Currently used for\njvm-based components only (such as dataflow-engine), because opentelemetry-java-instrumentation\nrequires a http(s) URI for the endpoint but defaults to http/protobuf as a protocol.\nBecause of this, gRPC connections (over http) can only be set up by setting this option to grpc\nratio : The ratio of requests to trace. Takes values between 0 and 1 inclusive.\n\n\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap call seldon-tracing whose default value is shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tracing\ndata:\n  tracing.json: |-\n   {\n     \"enable\": true,\n     \"otelExporterEndpoint\": \"seldon-collector:4317\",\n     \"otelExporterProtocol\": \"grpc\",\n     \"ratio\": \"1\"\n   }\n  OTEL_JAVAAGENT_ENABLED: \"true\"\n  OTEL_EXPORTER_OTLP_ENDPOINT: \"http://seldon-collector:4317\"\n  OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n\n\nNote, this ConfigMap is created via our Helm charts and there is usually no need to modify it manually.\nAt present Java instrumentation (for the dataflow engine) is duplicated via separate keys.\n\n\n", "kafka-configuration": "\nKafka Configuration\u00b6\nWe allow configuration of the Kafka integration. In general this configuration looks like:\n{\n    \"topicPrefix\": \"seldon\",\n    \"bootstrap.servers\":\"kafka:9093\",\n    \"consumer\":{\n\t\"session.timeout.ms\":6000,\n\t\"auto.offset.reset\":\"earliest\",\n\t\"topic.metadata.propagation.max.ms\": 300000,\n\t\"message.max.bytes\":1000000000\n    },\n    \"producer\":{\n\t\"linger.ms\":0,\n\t\"message.max.bytes\":1000000000\n    },\n    \"streams\":{\n    }\n}\n\n\nThe top level keys are:\n\ntopicPrefix : the prefix to add to kafka topics created by Seldon\nconsumerGroupIdPrefix : the prefix to add to Kafka consumer group IDs created by Seldon\nbootstrap.servers : the global bootstrap kafka servers to use\nconsumer : consumer settings\nproducer : producer settings\nstreams : KStreams settings\n\nFor topicPrefix you can use any acceptable kafka topic characters which are a-z, A-Z, 0-9, . (dot), _ (underscore), and - (dash). We use . (dot) internally as topic naming separator so we would suggest you don\u2019t end your topic prefix with a dot for clarity. For illustration, an example topic could be seldon.default.model.mymodel.inputs where seldon is the topic prefix.\nThe consumerGroupIdPrefix will ensure that all consumer groups created have a given prefix.\n\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap called seldon-kafka whose default values are defined in the SeldonConfig custom resource.\n      bootstrap.servers: 'seldon-kafka-bootstrap.seldon-mesh:9092'\n      consumer:\n        auto.offset.reset: 'earliest'\n        message.max.bytes: '1000000000'\n        session.timeout.ms: '6000'\n        topic.metadata.propagation.max.ms: '300000'\n      consumerGroupIdPrefix: ''\n      debug: ''\n      producer:\n        linger.ms: '0'\n        message.max.bytes: '1000000000'\n      topicPrefix: 'seldon'\n\n\nWhen the SeldonRuntime is installed in a namespace a configMap will be created with these settings for Kafka configuration.\nTo customize the settings you can add and modify the Kafka configuration via Helm, for example below is a custom Helm values file that add compression for producers:\nconfig:\n  kafkaConfig:\n    producer:\n      compression.type: gzip\n\n\nTo use this with the SeldonRuntime Helm chart:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n    --namespace seldon-mesh \\\n    --values k8s/samples/values-runtime-kafka-compression.yaml\n\n\n\n\nTopic and consumer isolation\u00b6\nIf you use a shared Kafka cluster with other applications you may want to isolate the topic names and consumer group IDs from other users of the cluster to ensure there is no name clash. For this we provide two settings:\n\ntopicPrefix: set a prefix for all topics\nconsumerGroupIdPrefix: set a prefix for all consumer groups\n\nAn example to set this in the configuration when using the helm installation is showm below for creating the default SeldonConfig:\nhelm upgrade --install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n    --set controller.clusterwide=true \\\n    --set kafka.topicPrefix=myorg \\\n    --set kafka.consumerGroupIdPrefix=myorg\n\n\nYou can find a worked example here.\nYou can create alternate SeldonConfigs with different values or override values for particular SeldonRuntime installs.\n\n", "kubernetes": "\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap called seldon-kafka whose default values are defined in the SeldonConfig custom resource.\n      bootstrap.servers: 'seldon-kafka-bootstrap.seldon-mesh:9092'\n      consumer:\n        auto.offset.reset: 'earliest'\n        message.max.bytes: '1000000000'\n        session.timeout.ms: '6000'\n        topic.metadata.propagation.max.ms: '300000'\n      consumerGroupIdPrefix: ''\n      debug: ''\n      producer:\n        linger.ms: '0'\n        message.max.bytes: '1000000000'\n      topicPrefix: 'seldon'\n\n\nWhen the SeldonRuntime is installed in a namespace a configMap will be created with these settings for Kafka configuration.\nTo customize the settings you can add and modify the Kafka configuration via Helm, for example below is a custom Helm values file that add compression for producers:\nconfig:\n  kafkaConfig:\n    producer:\n      compression.type: gzip\n\n\nTo use this with the SeldonRuntime Helm chart:\nhelm install seldon-v2-runtime k8s/helm-charts/seldon-core-v2-runtime \\\n    --namespace seldon-mesh \\\n    --values k8s/samples/values-runtime-kafka-compression.yaml\n\n\n", "topic-and-consumer-isolation": "\nTopic and consumer isolation\u00b6\nIf you use a shared Kafka cluster with other applications you may want to isolate the topic names and consumer group IDs from other users of the cluster to ensure there is no name clash. For this we provide two settings:\n\ntopicPrefix: set a prefix for all topics\nconsumerGroupIdPrefix: set a prefix for all consumer groups\n\nAn example to set this in the configuration when using the helm installation is showm below for creating the default SeldonConfig:\nhelm upgrade --install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh \\\n    --set controller.clusterwide=true \\\n    --set kafka.topicPrefix=myorg \\\n    --set kafka.consumerGroupIdPrefix=myorg\n\n\nYou can find a worked example here.\nYou can create alternate SeldonConfigs with different values or override values for particular SeldonRuntime installs.\n", "tracing-configuration": "\nTracing Configuration\u00b6\nWe allow configuration of tracing. This file looks like:\n{\n  \"disable\": false,\n  \"otelExporterEndpoint\": \"otel-collector:4317\",\n  \"otelExporterProtocol\": \"grpc\",\n  \"ratio\": \"1\"\n}\n\n\nThe top level keys are:\n\nenable : whether to enable tracing\notelExporterEndpoint : The host and port for the OTEL exporter\notelExporterProtocol : The protocol for the OTEL exporter. Currently used for\njvm-based components only (such as dataflow-engine), because opentelemetry-java-instrumentation\nrequires a http(s) URI for the endpoint but defaults to http/protobuf as a protocol.\nBecause of this, gRPC connections (over http) can only be set up by setting this option to grpc\nratio : The ratio of requests to trace. Takes values between 0 and 1 inclusive.\n\n\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap call seldon-tracing whose default value is shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tracing\ndata:\n  tracing.json: |-\n   {\n     \"enable\": true,\n     \"otelExporterEndpoint\": \"seldon-collector:4317\",\n     \"otelExporterProtocol\": \"grpc\",\n     \"ratio\": \"1\"\n   }\n  OTEL_JAVAAGENT_ENABLED: \"true\"\n  OTEL_EXPORTER_OTLP_ENDPOINT: \"http://seldon-collector:4317\"\n  OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n\n\nNote, this ConfigMap is created via our Helm charts and there is usually no need to modify it manually.\nAt present Java instrumentation (for the dataflow engine) is duplicated via separate keys.\n\n", "id1": "\nKubernetes\u00b6\nFor Kubernetes this is controlled via a ConfigMap call seldon-tracing whose default value is shown below:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tracing\ndata:\n  tracing.json: |-\n   {\n     \"enable\": true,\n     \"otelExporterEndpoint\": \"seldon-collector:4317\",\n     \"otelExporterProtocol\": \"grpc\",\n     \"ratio\": \"1\"\n   }\n  OTEL_JAVAAGENT_ENABLED: \"true\"\n  OTEL_EXPORTER_OTLP_ENDPOINT: \"http://seldon-collector:4317\"\n  OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n\n\nNote, this ConfigMap is created via our Helm charts and there is usually no need to modify it manually.\nAt present Java instrumentation (for the dataflow engine) is duplicated via separate keys.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/configuration/index.html", "key": "getting-started/configuration"}}, "examples/explainer-examples": {"sections": {"explainer-examples": "\nExplainer Examples\u00b6\nRun these examples from the samples folder.\n\nAnchor Tabular Explainer for SKLearn Income Model\u00b6\ncat ./models/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n\n\nseldon model load -f ./models/income.yaml\n\n\n{}\n\n\nseldon model status income -w ModelAvailable\n\n\n{}\n\n\nseldon model infer income \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"c65b8302-85af-4bac-aac5-91e3bedebee8\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nseldon model load -f ./models/income-explainer.yaml\n\n\n{}\n\n\nseldon model status income-explainer -w ModelAvailable\n\n\n{}\n\n\nseldon model infer income-explainer \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income-explainer_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"a22c3785-ff3b-4504-9b3c-199aa48a62d6\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"explanation\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"str\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"meta\\\": {\\\"name\\\": \\\"AnchorTabular\\\", \\\"type\\\": [\\\"blackbox\\\"], \\\"explanations\\\": [\\\"local\\\"], \\\"params\\\": {\\\"seed\\\": 1, \\\"disc_perc\\\": [25, 50, 75], \\\"threshold\\\": 0.95, \\\"delta\\\": 0.1, \\\"tau\\\": 0.15, \\\"batch_size\\\": 100, \\\"coverage_samples\\\": 10000, \\\"beam_size\\\": 1, \\\"stop_on_first\\\": false, \\\"max_anchor_size\\\": null, \\\"min_samples_start\\\": 100, \\\"n_covered_ex\\\": 10, \\\"binary_cache_size\\\": 10000, \\\"cache_margin\\\": 1000, \\\"verbose\\\": false, \\\"verbose_every\\\": 1, \\\"kwargs\\\": {}}, \\\"version\\\": \\\"0.9.0\\\"}, \\\"data\\\": {\\\"anchor\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"precision\\\": 0.9518716577540107, \\\"coverage\\\": 0.07165109034267912, \\\"raw\\\": {\\\"feature\\\": [3, 5], \\\"mean\\\": [0.7959381044487428, 0.9518716577540107], \\\"precision\\\": [0.7959381044487428, 0.9518716577540107], \\\"coverage\\\": [0.3037383177570093, 0.07165109034267912], \\\"examples\\\": [{\\\"covered_true\\\": [[52, 5, 5, 1, 8, 1, 2, 0, 0, 0, 50, 9], [49, 4, 1, 1, 4, 4, 1, 0, 0, 0, 40, 1], [23, 4, 1, 1, 6, 1, 4, 1, 0, 0, 40, 9], [55, 2, 1, 1, 5, 1, 4, 0, 0, 0, 48, 9], [22, 4, 1, 1, 2, 3, 4, 0, 0, 0, 15, 9], [51, 4, 2, 1, 5, 0, 1, 1, 0, 0, 99, 4], [40, 4, 1, 1, 5, 1, 4, 0, 0, 0, 40, 9], [40, 6, 1, 1, 2, 0, 4, 1, 0, 0, 50, 9], [50, 5, 5, 1, 6, 0, 4, 1, 0, 0, 55, 9], [41, 4, 1, 1, 6, 0, 4, 1, 0, 0, 40, 9]], \\\"covered_false\\\": [[42, 4, 1, 1, 8, 0, 4, 1, 0, 2415, 60, 9], [48, 6, 2, 1, 5, 4, 4, 0, 0, 0, 60, 9], [37, 4, 1, 1, 5, 0, 4, 1, 0, 0, 45, 9], [57, 4, 5, 1, 8, 0, 4, 1, 0, 0, 50, 9], [63, 7, 2, 1, 8, 0, 4, 1, 0, 1902, 50, 9], [51, 4, 5, 1, 8, 0, 4, 1, 0, 1887, 47, 9], [51, 2, 2, 1, 8, 1, 4, 0, 0, 0, 45, 9], [68, 7, 5, 1, 5, 0, 4, 1, 0, 2377, 42, 0], [45, 4, 1, 1, 8, 0, 4, 1, 15024, 0, 40, 9], [45, 4, 1, 1, 8, 0, 4, 1, 0, 1977, 60, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}, {\\\"covered_true\\\": [[44, 6, 5, 1, 8, 3, 4, 0, 0, 1902, 60, 9], [58, 7, 2, 1, 5, 3, 1, 1, 4064, 0, 40, 1], [50, 7, 1, 1, 1, 3, 2, 0, 0, 0, 37, 9], [34, 4, 2, 1, 5, 3, 4, 1, 0, 0, 45, 9], [45, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9], [33, 7, 5, 1, 5, 3, 1, 1, 0, 0, 30, 6], [61, 7, 2, 1, 5, 3, 4, 1, 0, 0, 40, 0], [35, 4, 5, 1, 1, 3, 4, 1, 0, 0, 40, 9], [71, 2, 1, 1, 5, 3, 4, 0, 0, 0, 6, 9], [44, 4, 1, 1, 8, 3, 2, 1, 0, 0, 35, 9]], \\\"covered_false\\\": [[30, 4, 5, 1, 5, 3, 4, 1, 10520, 0, 40, 9], [54, 7, 2, 1, 8, 3, 4, 1, 0, 1902, 50, 9], [66, 6, 2, 1, 6, 3, 4, 1, 0, 2377, 25, 9], [35, 4, 2, 1, 5, 3, 4, 1, 7298, 0, 40, 9], [44, 4, 1, 1, 8, 3, 4, 1, 7298, 0, 48, 9], [31, 4, 1, 1, 8, 3, 4, 0, 13550, 0, 50, 9], [35, 4, 1, 1, 8, 3, 4, 1, 8614, 0, 45, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}], \\\"all_precision\\\": 0, \\\"num_preds\\\": 1000000, \\\"success\\\": true, \\\"names\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"prediction\\\": [0], \\\"instance\\\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \\\"instances\\\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-explainer\n\n\n{}\n\n\nseldon model unload income\n\n\n{}\n\n\n\n\nAnchor Text Explainer for SKLearn Movies Sentiment Model\u00b6\ncat ./models/moviesentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/moviesentiment-sklearn\"\n  requirements:\n  - sklearn\n\n\nseldon model load -f ./models/moviesentiment.yaml\n\n\n{}\n\n\nseldon model status sentiment -w ModelAvailable\n\n\n{}\n\n\nseldon model infer sentiment \\\n  '{\"parameters\": {\"content_type\": \"str\"}, \"inputs\": [{\"name\": \"foo\", \"data\": [\"I am good\"], \"datatype\": \"BYTES\",\"shape\": [1]}]}'\n\n\n{\n\t\"model_name\": \"sentiment_2\",\n\t\"model_version\": \"1\",\n\t\"id\": \"f5c07363-7e9d-4f09-aa30-228c81fdf4a4\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/moviesentiment-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/moviesentiment-sklearn-explainer\"\n  explainer:\n    type: anchor_text\n    modelRef: sentiment\n\n\nseldon model load -f ./models/moviesentiment-explainer.yaml\n\n\n{}\n\n\nseldon model status sentiment-explainer -w ModelAvailable\n\n\n{}\n\n\nseldon model infer sentiment-explainer \\\n  '{\"parameters\": {\"content_type\": \"str\"}, \"inputs\": [{\"name\": \"foo\", \"data\": [\"I am good\"], \"datatype\": \"BYTES\",\"shape\": [1]}]}'\n\n\nError: V2 server error: 500 Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette_exporter/middleware.py\", line 307, in __call__\n    await self.app(scope, receive, wrapped_send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\n    await responder(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\n    await self.app(scope, receive, self.send_with_gzip)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/rest/app.py\", line 42, in custom_route_handler\n    return await original_route_handler(request)\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/routing.py\", line 237, in app\n    raw_response = await run_endpoint_function(\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/rest/endpoints.py\", line 99, in infer\n    inference_response = await self._data_plane.infer(\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/handlers/dataplane.py\", line 103, in infer\n    prediction = await model.predict(payload)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/runtime.py\", line 86, in predict\n    output_data = await self._async_explain_impl(input_data, payload.parameters)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/runtime.py\", line 119, in _async_explain_impl\n    explanation = await loop.run_in_executor(self._executor, explain_call)\n  File \"/opt/conda/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/explainers/black_box_runtime.py\", line 62, in _explain_impl\n    input_data = input_data[0]\nKeyError: 0\n\n\n\nseldon model unload sentiment-explainer\n\n\n{}\n\n\nseldon model unload sentiment\n\n\n{}\n\n\n\n", "anchor-tabular-explainer-for-sklearn-income-model": "\nAnchor Tabular Explainer for SKLearn Income Model\u00b6\ncat ./models/income.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/classifier\"\n  requirements:\n  - sklearn\n\n\nseldon model load -f ./models/income.yaml\n\n\n{}\n\n\nseldon model status income -w ModelAvailable\n\n\n{}\n\n\nseldon model infer income \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"c65b8302-85af-4bac-aac5-91e3bedebee8\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/income-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: income-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/examples/mlserver_1.3.5/income/explainer\"\n  explainer:\n    type: anchor_tabular\n    modelRef: income\n\n\nseldon model load -f ./models/income-explainer.yaml\n\n\n{}\n\n\nseldon model status income-explainer -w ModelAvailable\n\n\n{}\n\n\nseldon model infer income-explainer \\\n  '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 12], \"datatype\": \"FP32\", \"data\": [[47,4,1,1,1,3,4,1,0,0,40,9]]}]}'\n\n\n{\n\t\"model_name\": \"income-explainer_1\",\n\t\"model_version\": \"1\",\n\t\"id\": \"a22c3785-ff3b-4504-9b3c-199aa48a62d6\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"explanation\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"BYTES\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"str\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t\"{\\\"meta\\\": {\\\"name\\\": \\\"AnchorTabular\\\", \\\"type\\\": [\\\"blackbox\\\"], \\\"explanations\\\": [\\\"local\\\"], \\\"params\\\": {\\\"seed\\\": 1, \\\"disc_perc\\\": [25, 50, 75], \\\"threshold\\\": 0.95, \\\"delta\\\": 0.1, \\\"tau\\\": 0.15, \\\"batch_size\\\": 100, \\\"coverage_samples\\\": 10000, \\\"beam_size\\\": 1, \\\"stop_on_first\\\": false, \\\"max_anchor_size\\\": null, \\\"min_samples_start\\\": 100, \\\"n_covered_ex\\\": 10, \\\"binary_cache_size\\\": 10000, \\\"cache_margin\\\": 1000, \\\"verbose\\\": false, \\\"verbose_every\\\": 1, \\\"kwargs\\\": {}}, \\\"version\\\": \\\"0.9.0\\\"}, \\\"data\\\": {\\\"anchor\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"precision\\\": 0.9518716577540107, \\\"coverage\\\": 0.07165109034267912, \\\"raw\\\": {\\\"feature\\\": [3, 5], \\\"mean\\\": [0.7959381044487428, 0.9518716577540107], \\\"precision\\\": [0.7959381044487428, 0.9518716577540107], \\\"coverage\\\": [0.3037383177570093, 0.07165109034267912], \\\"examples\\\": [{\\\"covered_true\\\": [[52, 5, 5, 1, 8, 1, 2, 0, 0, 0, 50, 9], [49, 4, 1, 1, 4, 4, 1, 0, 0, 0, 40, 1], [23, 4, 1, 1, 6, 1, 4, 1, 0, 0, 40, 9], [55, 2, 1, 1, 5, 1, 4, 0, 0, 0, 48, 9], [22, 4, 1, 1, 2, 3, 4, 0, 0, 0, 15, 9], [51, 4, 2, 1, 5, 0, 1, 1, 0, 0, 99, 4], [40, 4, 1, 1, 5, 1, 4, 0, 0, 0, 40, 9], [40, 6, 1, 1, 2, 0, 4, 1, 0, 0, 50, 9], [50, 5, 5, 1, 6, 0, 4, 1, 0, 0, 55, 9], [41, 4, 1, 1, 6, 0, 4, 1, 0, 0, 40, 9]], \\\"covered_false\\\": [[42, 4, 1, 1, 8, 0, 4, 1, 0, 2415, 60, 9], [48, 6, 2, 1, 5, 4, 4, 0, 0, 0, 60, 9], [37, 4, 1, 1, 5, 0, 4, 1, 0, 0, 45, 9], [57, 4, 5, 1, 8, 0, 4, 1, 0, 0, 50, 9], [63, 7, 2, 1, 8, 0, 4, 1, 0, 1902, 50, 9], [51, 4, 5, 1, 8, 0, 4, 1, 0, 1887, 47, 9], [51, 2, 2, 1, 8, 1, 4, 0, 0, 0, 45, 9], [68, 7, 5, 1, 5, 0, 4, 1, 0, 2377, 42, 0], [45, 4, 1, 1, 8, 0, 4, 1, 15024, 0, 40, 9], [45, 4, 1, 1, 8, 0, 4, 1, 0, 1977, 60, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}, {\\\"covered_true\\\": [[44, 6, 5, 1, 8, 3, 4, 0, 0, 1902, 60, 9], [58, 7, 2, 1, 5, 3, 1, 1, 4064, 0, 40, 1], [50, 7, 1, 1, 1, 3, 2, 0, 0, 0, 37, 9], [34, 4, 2, 1, 5, 3, 4, 1, 0, 0, 45, 9], [45, 4, 1, 1, 5, 3, 4, 1, 0, 0, 40, 9], [33, 7, 5, 1, 5, 3, 1, 1, 0, 0, 30, 6], [61, 7, 2, 1, 5, 3, 4, 1, 0, 0, 40, 0], [35, 4, 5, 1, 1, 3, 4, 1, 0, 0, 40, 9], [71, 2, 1, 1, 5, 3, 4, 0, 0, 0, 6, 9], [44, 4, 1, 1, 8, 3, 2, 1, 0, 0, 35, 9]], \\\"covered_false\\\": [[30, 4, 5, 1, 5, 3, 4, 1, 10520, 0, 40, 9], [54, 7, 2, 1, 8, 3, 4, 1, 0, 1902, 50, 9], [66, 6, 2, 1, 6, 3, 4, 1, 0, 2377, 25, 9], [35, 4, 2, 1, 5, 3, 4, 1, 7298, 0, 40, 9], [44, 4, 1, 1, 8, 3, 4, 1, 7298, 0, 48, 9], [31, 4, 1, 1, 8, 3, 4, 0, 13550, 0, 50, 9], [35, 4, 1, 1, 8, 3, 4, 1, 8614, 0, 45, 9]], \\\"uncovered_true\\\": [], \\\"uncovered_false\\\": []}], \\\"all_precision\\\": 0, \\\"num_preds\\\": 1000000, \\\"success\\\": true, \\\"names\\\": [\\\"Marital Status = Never-Married\\\", \\\"Relationship = Own-child\\\"], \\\"prediction\\\": [0], \\\"instance\\\": [47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0], \\\"instances\\\": [[47.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 40.0, 9.0]]}}}\"\n\t\t\t]\n\t\t}\n\t]\n}\n\n\nseldon model unload income-explainer\n\n\n{}\n\n\nseldon model unload income\n\n\n{}\n\n\n", "anchor-text-explainer-for-sklearn-movies-sentiment-model": "\nAnchor Text Explainer for SKLearn Movies Sentiment Model\u00b6\ncat ./models/moviesentiment.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/moviesentiment-sklearn\"\n  requirements:\n  - sklearn\n\n\nseldon model load -f ./models/moviesentiment.yaml\n\n\n{}\n\n\nseldon model status sentiment -w ModelAvailable\n\n\n{}\n\n\nseldon model infer sentiment \\\n  '{\"parameters\": {\"content_type\": \"str\"}, \"inputs\": [{\"name\": \"foo\", \"data\": [\"I am good\"], \"datatype\": \"BYTES\",\"shape\": [1]}]}'\n\n\n{\n\t\"model_name\": \"sentiment_2\",\n\t\"model_version\": \"1\",\n\t\"id\": \"f5c07363-7e9d-4f09-aa30-228c81fdf4a4\",\n\t\"parameters\": {},\n\t\"outputs\": [\n\t\t{\n\t\t\t\"name\": \"predict\",\n\t\t\t\"shape\": [\n\t\t\t\t1,\n\t\t\t\t1\n\t\t\t],\n\t\t\t\"datatype\": \"INT64\",\n\t\t\t\"parameters\": {\n\t\t\t\t\"content_type\": \"np\"\n\t\t\t},\n\t\t\t\"data\": [\n\t\t\t\t0\n\t\t\t]\n\t\t}\n\t]\n}\n\n\ncat ./models/moviesentiment-explainer.yaml\n\n\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: sentiment-explainer\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/moviesentiment-sklearn-explainer\"\n  explainer:\n    type: anchor_text\n    modelRef: sentiment\n\n\nseldon model load -f ./models/moviesentiment-explainer.yaml\n\n\n{}\n\n\nseldon model status sentiment-explainer -w ModelAvailable\n\n\n{}\n\n\nseldon model infer sentiment-explainer \\\n  '{\"parameters\": {\"content_type\": \"str\"}, \"inputs\": [{\"name\": \"foo\", \"data\": [\"I am good\"], \"datatype\": \"BYTES\",\"shape\": [1]}]}'\n\n\nError: V2 server error: 500 Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette_exporter/middleware.py\", line 307, in __call__\n    await self.app(scope, receive, wrapped_send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\n    await responder(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\n    await self.app(scope, receive, self.send_with_gzip)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.8/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/rest/app.py\", line 42, in custom_route_handler\n    return await original_route_handler(request)\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/routing.py\", line 237, in app\n    raw_response = await run_endpoint_function(\n  File \"/opt/conda/lib/python3.8/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/rest/endpoints.py\", line 99, in infer\n    inference_response = await self._data_plane.infer(\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver/handlers/dataplane.py\", line 103, in infer\n    prediction = await model.predict(payload)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/runtime.py\", line 86, in predict\n    output_data = await self._async_explain_impl(input_data, payload.parameters)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/runtime.py\", line 119, in _async_explain_impl\n    explanation = await loop.run_in_executor(self._executor, explain_call)\n  File \"/opt/conda/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.8/site-packages/mlserver_alibi_explain/explainers/black_box_runtime.py\", line 62, in _explain_impl\n    input_data = input_data[0]\nKeyError: 0\n\n\n\nseldon model unload sentiment-explainer\n\n\n{}\n\n\nseldon model unload sentiment\n\n\n{}\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/examples/explainer-examples.html", "key": "examples/explainer-examples"}}, "getting-started/kubernetes-installation": {"sections": {"kubernetes-installation": "\nKubernetes Installation\u00b6\n\nPrerequisites\u00b6\n\nA running Kubernetes Cluster\u00b6\nYou will need a running Kubernetes cluster or can create a local KinD one for testing via Ansible.\n\n\nInstall Ecosystem Components\u00b6\nYou will also need to install our ecosystem components. For this we provide directions for Ansible to install these.\n\n\nComponent\nSummary\n\n\n\nKafka\nRequired for inference Pipeline usage.\n\nPrometheus\n(Optional) Exposes metrics.\n\nGrafana\n(Optional) UI for metrics.\n\nOpenTelemetry\n(Optional) Exposes tracing.\n\nJaeger\n(Optional) UI for traces.\n\n\n\n\n\nInstall\u00b6\nTo install Seldon Core V2 itself you can choose from the following. At present, all require a clone of the source repository.\n\nHelm Installation (recommended for production systems)\nAnsible (recommended for test / dev / trial purposes)\n\nThe Kubernetes operator that is installed runs in namespaced mode so any resources you create need to be in the same namespace as you installed into.\n\n\nKustomize\u00b6\nOur recommended and supported way to install is via Helm or Ansible. If you wish to use Kustomize then you can base your configuration on the raw yaml we create in the folder k8s/yaml or follow the steps in k8s/Makefile which illustrate how we build this yaml from our own Kustomize bases.\n\n\n\nOperations\u00b6\n\nSecurity\n\n\n\n\n", "prerequisites": "\nPrerequisites\u00b6\n\nA running Kubernetes Cluster\u00b6\nYou will need a running Kubernetes cluster or can create a local KinD one for testing via Ansible.\n\n\nInstall Ecosystem Components\u00b6\nYou will also need to install our ecosystem components. For this we provide directions for Ansible to install these.\n\n\nComponent\nSummary\n\n\n\nKafka\nRequired for inference Pipeline usage.\n\nPrometheus\n(Optional) Exposes metrics.\n\nGrafana\n(Optional) UI for metrics.\n\nOpenTelemetry\n(Optional) Exposes tracing.\n\nJaeger\n(Optional) UI for traces.\n\n\n\n\n\nInstall\u00b6\nTo install Seldon Core V2 itself you can choose from the following. At present, all require a clone of the source repository.\n\nHelm Installation (recommended for production systems)\nAnsible (recommended for test / dev / trial purposes)\n\nThe Kubernetes operator that is installed runs in namespaced mode so any resources you create need to be in the same namespace as you installed into.\n\n\nKustomize\u00b6\nOur recommended and supported way to install is via Helm or Ansible. If you wish to use Kustomize then you can base your configuration on the raw yaml we create in the folder k8s/yaml or follow the steps in k8s/Makefile which illustrate how we build this yaml from our own Kustomize bases.\n\n", "a-running-kubernetes-cluster": "\nA running Kubernetes Cluster\u00b6\nYou will need a running Kubernetes cluster or can create a local KinD one for testing via Ansible.\n", "install-ecosystem-components": "\nInstall Ecosystem Components\u00b6\nYou will also need to install our ecosystem components. For this we provide directions for Ansible to install these.\n\n\nComponent\nSummary\n\n\n\nKafka\nRequired for inference Pipeline usage.\n\nPrometheus\n(Optional) Exposes metrics.\n\nGrafana\n(Optional) UI for metrics.\n\nOpenTelemetry\n(Optional) Exposes tracing.\n\nJaeger\n(Optional) UI for traces.\n\n\n\n", "install": "\nInstall\u00b6\nTo install Seldon Core V2 itself you can choose from the following. At present, all require a clone of the source repository.\n\nHelm Installation (recommended for production systems)\nAnsible (recommended for test / dev / trial purposes)\n\nThe Kubernetes operator that is installed runs in namespaced mode so any resources you create need to be in the same namespace as you installed into.\n", "kustomize": "\nKustomize\u00b6\nOur recommended and supported way to install is via Helm or Ansible. If you wish to use Kustomize then you can base your configuration on the raw yaml we create in the folder k8s/yaml or follow the steps in k8s/Makefile which illustrate how we build this yaml from our own Kustomize bases.\n", "operations": "\nOperations\u00b6\n\nSecurity\n\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/index.html", "key": "getting-started/kubernetes-installation"}}, "kubernetes/resources/pipeline": {"sections": {"pipeline": "\nPipeline\u00b6\nPipelines allow one to connect flows of inference data transformed by Model components. A directed acyclic graph (DAG) of steps can be defined to join Models together. Each Model will need to be capable of receiving a V2 inference request and respond with a V2 inference response. An example Pipeline is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: join\nspec:\n  steps:\n    - name: tfsimple1\n    - name: tfsimple2\n    - name: tfsimple3      \n      inputs:\n      - tfsimple1.outputs.OUTPUT0\n      - tfsimple2.outputs.OUTPUT1\n      tensorMap:\n        tfsimple1.outputs.OUTPUT0: INPUT0\n        tfsimple2.outputs.OUTPUT1: INPUT1\n  output:\n    steps:\n    - tfsimple3\n\n\nThe steps list shows three models: tfsimple1, tfsimple2 and tfsimple3. These three models each take two tensors called INPUT0 and INPUT1 of integers. The models produce two outputs OUTPUT0 (the sum of the inputs) and OUTPUT1 (subtraction of the second input from the first).\ntfsimple1 and tfsimple2 take as inputs the input to the Pipeline: the default assumption when no explicit inputs are defined. tfsimple3 takes one V2 tensor input from each of the outputs of tfsimple1 and tfsimple2. As the outputs of tfsimple1 and tfsimple2 have tensors named OUTPUT0 and OUTPUT1 their names need to be changed to respect the expected input tensors and this is done with a tensorMap component providing this tensor renaming. This is only required if your models can not be directly chained together.\nThe output of the Pipeline is the output from the tfsimple3 model.\n\nDetailed Specification\u00b6\nThe full GoLang specification for a Pipeline is shown below:\ntype PipelineSpec struct {\n\t// External inputs to this pipeline, optional\n\tInput *PipelineInput `json:\"input,omitempty\"`\n\n\t// The steps of this inference graph pipeline\n\tSteps []PipelineStep `json:\"steps\"`\n\n\t// Synchronous output from this pipeline, optional\n\tOutput *PipelineOutput `json:\"output,omitempty\"`\n}\n\n// +kubebuilder:validation:Enum=inner;outer;any\ntype JoinType string\n\nconst (\n\t// data must be available from all inputs\n\tJoinTypeInner JoinType = \"inner\"\n\t// data will include any data from any inputs at end of window\n\tJoinTypeOuter JoinType = \"outer\"\n\t// first data input that arrives will be forwarded\n\tJoinTypeAny JoinType = \"any\"\n)\n\ntype PipelineStep struct {\n\t// Name of the step\n\tName string `json:\"name\"`\n\n\t// Previous step to receive data from\n\tInputs []string `json:\"inputs,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs *uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n\n\t// Triggers required to activate step\n\tTriggers []string `json:\"triggers,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tInputsJoinType *JoinType `json:\"inputsJoinType,omitempty\"`\n\n\tTriggersJoinType *JoinType `json:\"triggersJoinType,omitempty\"`\n\n\t// Batch size of request required before data will be sent to this step\n\tBatch *PipelineBatch `json:\"batch,omitempty\"`\n}\n\ntype PipelineBatch struct {\n\tSize     *uint32 `json:\"size,omitempty\"`\n\tWindowMs *uint32 `json:\"windowMs,omitempty\"`\n\tRolling  bool    `json:\"rolling,omitempty\"`\n}\n\ntype PipelineInput struct {\n\t// Previous external pipeline steps to receive data from\n\tExternalInputs []string `json:\"externalInputs,omitempty\"`\n\n\t// Triggers required to activate inputs\n\tExternalTriggers []string `json:\"externalTriggers,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs *uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tJoinType *JoinType `json:\"joinType,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tTriggersJoinType *JoinType `json:\"triggersJoinType,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n}\n\ntype PipelineOutput struct {\n\t// Previous step to receive data from\n\tSteps []string `json:\"steps,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tStepsJoin *JoinType `json:\"stepsJoin,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n}\n\n\n\n", "detailed-specification": "\nDetailed Specification\u00b6\nThe full GoLang specification for a Pipeline is shown below:\ntype PipelineSpec struct {\n\t// External inputs to this pipeline, optional\n\tInput *PipelineInput `json:\"input,omitempty\"`\n\n\t// The steps of this inference graph pipeline\n\tSteps []PipelineStep `json:\"steps\"`\n\n\t// Synchronous output from this pipeline, optional\n\tOutput *PipelineOutput `json:\"output,omitempty\"`\n}\n\n// +kubebuilder:validation:Enum=inner;outer;any\ntype JoinType string\n\nconst (\n\t// data must be available from all inputs\n\tJoinTypeInner JoinType = \"inner\"\n\t// data will include any data from any inputs at end of window\n\tJoinTypeOuter JoinType = \"outer\"\n\t// first data input that arrives will be forwarded\n\tJoinTypeAny JoinType = \"any\"\n)\n\ntype PipelineStep struct {\n\t// Name of the step\n\tName string `json:\"name\"`\n\n\t// Previous step to receive data from\n\tInputs []string `json:\"inputs,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs *uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n\n\t// Triggers required to activate step\n\tTriggers []string `json:\"triggers,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tInputsJoinType *JoinType `json:\"inputsJoinType,omitempty\"`\n\n\tTriggersJoinType *JoinType `json:\"triggersJoinType,omitempty\"`\n\n\t// Batch size of request required before data will be sent to this step\n\tBatch *PipelineBatch `json:\"batch,omitempty\"`\n}\n\ntype PipelineBatch struct {\n\tSize     *uint32 `json:\"size,omitempty\"`\n\tWindowMs *uint32 `json:\"windowMs,omitempty\"`\n\tRolling  bool    `json:\"rolling,omitempty\"`\n}\n\ntype PipelineInput struct {\n\t// Previous external pipeline steps to receive data from\n\tExternalInputs []string `json:\"externalInputs,omitempty\"`\n\n\t// Triggers required to activate inputs\n\tExternalTriggers []string `json:\"externalTriggers,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs *uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tJoinType *JoinType `json:\"joinType,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tTriggersJoinType *JoinType `json:\"triggersJoinType,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n}\n\ntype PipelineOutput struct {\n\t// Previous step to receive data from\n\tSteps []string `json:\"steps,omitempty\"`\n\n\t// msecs to wait for messages from multiple inputs to arrive before joining the inputs\n\tJoinWindowMs uint32 `json:\"joinWindowMs,omitempty\"`\n\n\t// +kubebuilder:default=inner\n\tStepsJoin *JoinType `json:\"stepsJoin,omitempty\"`\n\n\t// Map of tensor name conversions to use e.g. output1 -> input1\n\tTensorMap map[string]string `json:\"tensorMap,omitempty\"`\n}\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/pipeline/index.html", "key": "kubernetes/resources/pipeline"}}, "models": {"sections": {"models": "\nModels\u00b6\nModels provide the atomic building blocks of Seldon. They represents machine learning models, drift detectors, outlier detectors, explainers, feature transformations, and more complex routing models such as multi-armed bandits.\n\nSeldon can handle a wide range of inference artifacts\nArtifacts can be stored on any of the 40 or more cloud storage technologies as well as from local (mounted) folder as discussed here.\n\n\nKubernetes Example\u00b6\nA Kubernetes yaml example is shown below for a SKLearn model for iris classification:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nIts Kubernetes spec has two core requirements\n\nA storageUri specifying the location of the artifact. This can be any rclone URI specification.\nA requirements list which provides tags that need to be matched by the Server that can run this artifact type. By default when you install Seldon we provide a set of Servers that cover a range of artifact types.\n\n\n\nGRPC Example\u00b6\nYou can also load models directly over the scheduler grpc service. An example is shown below use grpcurl tool:\n!grpcurl -d '{\"model\":{ \\\n              \"meta\":{\"name\":\"iris\"},\\\n              \"modelSpec\":{\"uri\":\"gs://seldon-models/mlserver/iris\",\\\n                           \"requirements\":[\"sklearn\"],\\\n                           \"memoryBytes\":500},\\\n              \"deploymentSpec\":{\"replicas\":1}}}' \\\n         -plaintext \\\n         -import-path ../../apis \\\n         -proto apis/mlops/scheduler/scheduler.proto  0.0.0.0:9004 seldon.mlops.scheduler.Scheduler/LoadModel\n\n\nThe proto buffer definitions for the scheduler are outlined here.\n\n\n\n\nMulti-model Serving with Overcommit\u00b6\nMulti-model serving is an architecture pattern where one ML inference server hosts multiple models at the same time. It is a feature provided out of the box by Nvidia Triton and Seldon MLServer. Multi-model serving reduces infrastructure hardware requirements (e.g. expensive GPUs) which enables the deployment of a large number of models while making it efficient to operate the system at scale.\nSeldon Core v2 leverages multi-model serving by design and it is the default option for deploying models. The system will find an appropriate server to load the model onto based on requirements that the user defines in the Model deployment definition.\nMoreover, in many cases demand patterns allow for further Overcommit of resources. Seldon Core v2 is able to register more models than what can be served by the provisioned (memory) infrastructure and will swap models dynamically according to least used without adding significant latency overheads to inference workload.\nSee Multi-model serving for more information.\n\n\nAutoscaling of Models\u00b6\nSee here for discussion of autoscaling of models.\n\n", "kubernetes-example": "\nKubernetes Example\u00b6\nA Kubernetes yaml example is shown below for a SKLearn model for iris classification:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\"\n  requirements:\n  - sklearn\n  memory: 100Ki\n\n\nIts Kubernetes spec has two core requirements\n\nA storageUri specifying the location of the artifact. This can be any rclone URI specification.\nA requirements list which provides tags that need to be matched by the Server that can run this artifact type. By default when you install Seldon we provide a set of Servers that cover a range of artifact types.\n\n", "grpc-example": "\nGRPC Example\u00b6\nYou can also load models directly over the scheduler grpc service. An example is shown below use grpcurl tool:\n!grpcurl -d '{\"model\":{ \\\n              \"meta\":{\"name\":\"iris\"},\\\n              \"modelSpec\":{\"uri\":\"gs://seldon-models/mlserver/iris\",\\\n                           \"requirements\":[\"sklearn\"],\\\n                           \"memoryBytes\":500},\\\n              \"deploymentSpec\":{\"replicas\":1}}}' \\\n         -plaintext \\\n         -import-path ../../apis \\\n         -proto apis/mlops/scheduler/scheduler.proto  0.0.0.0:9004 seldon.mlops.scheduler.Scheduler/LoadModel\n\n\nThe proto buffer definitions for the scheduler are outlined here.\n\n\n", "multi-model-serving-with-overcommit": "\nMulti-model Serving with Overcommit\u00b6\nMulti-model serving is an architecture pattern where one ML inference server hosts multiple models at the same time. It is a feature provided out of the box by Nvidia Triton and Seldon MLServer. Multi-model serving reduces infrastructure hardware requirements (e.g. expensive GPUs) which enables the deployment of a large number of models while making it efficient to operate the system at scale.\nSeldon Core v2 leverages multi-model serving by design and it is the default option for deploying models. The system will find an appropriate server to load the model onto based on requirements that the user defines in the Model deployment definition.\nMoreover, in many cases demand patterns allow for further Overcommit of resources. Seldon Core v2 is able to register more models than what can be served by the provisioned (memory) infrastructure and will swap models dynamically according to least used without adding significant latency overheads to inference workload.\nSee Multi-model serving for more information.\n", "autoscaling-of-models": "\nAutoscaling of Models\u00b6\nSee here for discussion of autoscaling of models.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/models/index.html", "key": "models"}}, "cli/docs/seldon": {"sections": {"seldon": "\nseldon\u00b6\n\nOptions\u00b6\n  -h, --help   help for seldon\n\n\n\n\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\nseldon experiment\t - manage experiments\nseldon model\t - manage models\nseldon pipeline\t - manage pipelines\nseldon server\t - manage servers\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for seldon\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon config\t - manage configs\nseldon experiment\t - manage experiments\nseldon model\t - manage models\nseldon pipeline\t - manage pipelines\nseldon server\t - manage servers\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon.html", "key": "cli/docs/seldon"}}, "cli/docs/seldon_experiment": {"sections": {"seldon-experiment": "\nseldon experiment\u00b6\nmanage experiments\n\nSynopsis\u00b6\nexperiments allow you to test models by splitting traffic between candidates.\nseldon experiment <subcomand> [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for experiment\n\n\n\n\nSEE ALSO\u00b6\n\nseldon\t -\nseldon experiment list\t - get list of experiments\nseldon experiment start\t - start an experiment\nseldon experiment status\t - get status for experiment\nseldon experiment stop\t - stop an experiment\n\n\n", "synopsis": "\nSynopsis\u00b6\nexperiments allow you to test models by splitting traffic between candidates.\nseldon experiment <subcomand> [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for experiment\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon\t -\nseldon experiment list\t - get list of experiments\nseldon experiment start\t - start an experiment\nseldon experiment status\t - get status for experiment\nseldon experiment stop\t - stop an experiment\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_experiment.html", "key": "cli/docs/seldon_experiment"}}, "cli/docs/seldon_pipeline": {"sections": {"seldon-pipeline": "\nseldon pipeline\u00b6\nmanage pipelines\n\nSynopsis\u00b6\npipelines allow you to join models together into inference graphs.\nseldon pipeline <subcomand> [flags]\n\n\n\n\nOptions\u00b6\n  -h, --help   help for pipeline\n\n\n\n\nSEE ALSO\u00b6\n\nseldon\t -\nseldon pipeline infer\t - run inference on a pipeline\nseldon pipeline inspect\t - inspect data in a pipeline\nseldon pipeline list\t - list pipelines\nseldon pipeline load\t - load a pipeline\nseldon pipeline status\t - status of a pipeline\nseldon pipeline unload\t - unload a pipeline\n\n\n", "synopsis": "\nSynopsis\u00b6\npipelines allow you to join models together into inference graphs.\nseldon pipeline <subcomand> [flags]\n\n\n", "options": "\nOptions\u00b6\n  -h, --help   help for pipeline\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon\t -\nseldon pipeline infer\t - run inference on a pipeline\nseldon pipeline inspect\t - inspect data in a pipeline\nseldon pipeline list\t - list pipelines\nseldon pipeline load\t - load a pipeline\nseldon pipeline status\t - status of a pipeline\nseldon pipeline unload\t - unload a pipeline\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline.html", "key": "cli/docs/seldon_pipeline"}}, "cli/docs/seldon_pipeline_load": {"sections": {"seldon-pipeline-load": "\nseldon pipeline load\u00b6\nload a pipeline\n\nSynopsis\u00b6\nload a pipeline\nseldon pipeline load [flags]\n\n\n\n\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        pipeline manifest file (YAML)\n  -h, --help                    help for load\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n\n\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n\n", "synopsis": "\nSynopsis\u00b6\nload a pipeline\nseldon pipeline load [flags]\n\n\n", "options": "\nOptions\u00b6\n      --authority string        authority (HTTP/2) or virtual host (HTTP/1)\n  -f, --file-path string        pipeline manifest file (YAML)\n  -h, --help                    help for load\n      --scheduler-host string   seldon scheduler host (default \"0.0.0.0:9004\")\n  -r, --show-request            show request\n  -o, --show-response           show response (default true)\n\n\n", "see-also": "\nSEE ALSO\u00b6\n\nseldon pipeline\t - manage pipelines\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/cli/docs/seldon_pipeline_load.html", "key": "cli/docs/seldon_pipeline_load"}}, "kubernetes/resources/server": {"sections": {"server": "\nServer\u00b6\n\nNote\nThe default installation will provide two initial servers: one MLServer and one Triton. You only need to define additional servers for advanced use cases.\n\nA Server defines an inference server onto which models will be placed for inference. By default on installation two server StatefulSets will be deployed one MlServer and one Triton. An example Server definition is shown below:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver\nspec:\n  serverConfig: mlserver\n  replicas: 1\n\n\nThe main requirement is a reference to a ServerConfig resource in this case mlserver.\n\nDetailed Specs\u00b6\ntype ServerSpec struct {\n\t// Server definition\n\tServerConfig string `json:\"serverConfig\"`\n\t// The extra capabilities this server will advertise\n\t// These are added to the capabilities exposed by the referenced ServerConfig\n\tExtraCapabilities []string `json:\"extraCapabilities,omitempty\"`\n\t// The capabilities this server will advertise\n\t// This will override any from the referenced ServerConfig\n\tCapabilities []string `json:\"capabilities,omitempty\"`\n\t// Image overrides\n\tImageOverrides *ContainerOverrideSpec `json:\"imageOverrides,omitempty\"`\n\t// PodSpec overrides\n\t// Slices such as containers would be appended not overridden\n\tPodSpec *PodSpec `json:\"podSpec,omitempty\"`\n\t// Scaling spec\n\tScalingSpec `json:\",inline\"`\n\t// +Optional\n\t// If set then when the referenced ServerConfig changes we will NOT update the Server immediately.\n\t// Explicit changes to the Server itself will force a reconcile though\n\tDisableAutoUpdate bool `json:\"disableAutoUpdate,omitempty\"`\n}\n\ntype ContainerOverrideSpec struct {\n\t// The Agent overrides\n\tAgent *v1.Container `json:\"agent,omitempty\"`\n\t// The RClone server overrides\n\tRClone *v1.Container `json:\"rclone,omitempty\"`\n}\n\ntype ServerDefn struct {\n\t// Server config name to match\n\t// Required\n\tConfig string `json:\"config\"`\n}\n\n\n\n\nCustom Servers\u00b6\nOne can easily utilize a custom image with the existing ServerConfigs. For example, the following defines an MLServer server with a custom image:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-134\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - mlserver-1.3.4\n  podSpec:\n    containers:\n    - image: seldonio/mlserver:1.3.4\n      name: mlserver\n\n\nThis server can then be targeted by a particular model by specifying this server name when creating the model, for example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  server: mlserver-134\n\n\n\nServer with PVC\u00b6\nOne can also create a Server definition to add a persistent volume to your server. This can be used to allow models to be loaded directly from the persistent volume.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\nThe server can be targeted by a model whose artifact is on the persistent volume as shown below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\nA fully worked example for this can be found here.\nAn alternative would be to create your own ServerConfig for more complex use cases or you want to standardise the Server definition in one place.\n\n\n", "detailed-specs": "\nDetailed Specs\u00b6\ntype ServerSpec struct {\n\t// Server definition\n\tServerConfig string `json:\"serverConfig\"`\n\t// The extra capabilities this server will advertise\n\t// These are added to the capabilities exposed by the referenced ServerConfig\n\tExtraCapabilities []string `json:\"extraCapabilities,omitempty\"`\n\t// The capabilities this server will advertise\n\t// This will override any from the referenced ServerConfig\n\tCapabilities []string `json:\"capabilities,omitempty\"`\n\t// Image overrides\n\tImageOverrides *ContainerOverrideSpec `json:\"imageOverrides,omitempty\"`\n\t// PodSpec overrides\n\t// Slices such as containers would be appended not overridden\n\tPodSpec *PodSpec `json:\"podSpec,omitempty\"`\n\t// Scaling spec\n\tScalingSpec `json:\",inline\"`\n\t// +Optional\n\t// If set then when the referenced ServerConfig changes we will NOT update the Server immediately.\n\t// Explicit changes to the Server itself will force a reconcile though\n\tDisableAutoUpdate bool `json:\"disableAutoUpdate,omitempty\"`\n}\n\ntype ContainerOverrideSpec struct {\n\t// The Agent overrides\n\tAgent *v1.Container `json:\"agent,omitempty\"`\n\t// The RClone server overrides\n\tRClone *v1.Container `json:\"rclone,omitempty\"`\n}\n\ntype ServerDefn struct {\n\t// Server config name to match\n\t// Required\n\tConfig string `json:\"config\"`\n}\n\n\n", "custom-servers": "\nCustom Servers\u00b6\nOne can easily utilize a custom image with the existing ServerConfigs. For example, the following defines an MLServer server with a custom image:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-134\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - mlserver-1.3.4\n  podSpec:\n    containers:\n    - image: seldonio/mlserver:1.3.4\n      name: mlserver\n\n\nThis server can then be targeted by a particular model by specifying this server name when creating the model, for example:\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"gs://seldon-models/mlserver/iris\"\n  server: mlserver-134\n\n\n\nServer with PVC\u00b6\nOne can also create a Server definition to add a persistent volume to your server. This can be used to allow models to be loaded directly from the persistent volume.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\nThe server can be targeted by a model whose artifact is on the persistent volume as shown below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\nA fully worked example for this can be found here.\nAn alternative would be to create your own ServerConfig for more complex use cases or you want to standardise the Server definition in one place.\n\n", "server-with-pvc": "\nServer with PVC\u00b6\nOne can also create a Server definition to add a persistent volume to your server. This can be used to allow models to be loaded directly from the persistent volume.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver-pvc\nspec:\n  serverConfig: mlserver\n  extraCapabilities:\n  - \"pvc\"  \n  podSpec:\n    volumes:\n    - name: models-pvc\n      persistentVolumeClaim:\n        claimName: ml-models-pvc\n    containers:\n    - name: rclone\n      volumeMounts:\n      - name: models-pvc\n        mountPath: /var/models\n\n\nThe server can be targeted by a model whose artifact is on the persistent volume as shown below.\napiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: iris\nspec:\n  storageUri: \"/var/models/iris\"\n  requirements:\n  - sklearn\n  - pvc\n\n\nA fully worked example for this can be found here.\nAn alternative would be to create your own ServerConfig for more complex use cases or you want to standardise the Server definition in one place.\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/resources/server/index.html", "key": "kubernetes/resources/server"}}, "getting-started/kubernetes-installation/security/strimzi-sasl": {"sections": {"strimzi-sasl-example": "\nStrimzi SASL Example\u00b6\nCreate a Strimzi Kafka cluster with SASL_SSL enabled.\nThis can be done with our Ansible scripts by running the following from the ansible/ folder:\nansible-playbook playbooks/setup-ecosystem.yaml -e @../k8s/samples/ansible-strimzi-kafka-sasl-scram.yaml -e strimzi_kafka_operator_feature_gates=\"\"\n\n\nThe referenced SASL/SCRAM YAML file looks like the below:\nseldon_kafka_cluster_values:\n  broker:\n    tls:\n      authentication:\n        type: scram-sha-512\n\n\nThis will use the Strimzi Helm chart provided in Core v2.\nThis will call the Strimzi cluster Helm chart provided by the project with overrides for the cluster authentication type and will also create a user seldon with password credentials in a Kubernetes Secret.\nInstall Core v2 with SASL settings using a custom values file.\nThis sets the secret created by Strimzi for the user created above (seldon) and targets the server certificate authority secret from the name of the cluster created on install of the Kafka cluster (seldon-cluster-ca-cert).\nConfigure Seldon Core v2 by setting following Helm values:\n---\nkafka:\n  bootstrap: seldon-kafka-bootstrap.seldon-mesh.svc.cluster.local:9093\n\nsecurity:\n  kafka:\n    protocol: SASL_SSL\n    sasl:\n      mechanism: SCRAM-SHA-512\n      client:\n        username: seldon\n        secret: seldon\n        passwordPath: password\n    ssl:\n      client:\n        brokerValidationSecret: seldon-cluster-ca-cert\n        brokerCaPath: /tmp/certs/kafka/broker/ca.crt\n\n\nhelm install seldon-v2 k8s/helm-charts/seldon-core-v2-setup/ -n seldon-mesh -f k8s/samples/values-strimzi-kafka-sasl-scram.yaml\n\n\n"}, "meta": {"url": "https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/kubernetes-installation/security/strimzi-sasl.html", "key": "getting-started/kubernetes-installation/security/strimzi-sasl"}}}