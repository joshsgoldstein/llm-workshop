{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Workshop Building a Chat App With Memory using Seldon\n",
    "\n",
    "This demo implements a seldon-core-v2 pipeline with integrated state using the memory rt and a choice of two LLM backends: the OpenAI RT or the LocalLLM RT. This is a demo of the following seldon products:\n",
    "\n",
    "1. MLserver memory runtime\n",
    "2. MLserver openai runtime\n",
    "3. MLserver local runtime\n",
    "\n",
    "\n",
    "## Being Deplyed today:\n",
    "    - Two memory components\n",
    "    - A local chat RT component\n",
    "    - A chat pipeline app use the LocalLLM RT\n",
    "\n",
    "In order to run a terminal interface with the app use:\n",
    "\n",
    "```sh\n",
    "python chat.py --target=<target> --memory_id=<memory-id>\n",
    "```\n",
    "\n",
    "where memory-id is the id of a converstation and is optional (Not sepcifying will result in a new memory_id and converstation). And target is one of local or openai and specifies which RT to talk to.\n",
    "\n",
    "\n",
    "To remove use:\n",
    "\n",
    "```\n",
    "make undeploy\n",
    "```\n",
    "\n",
    "## SCV2 flow\n",
    "\n",
    "### Chat App Flow\n",
    "\n",
    "The chat app pipeline looks like:\n",
    "\n",
    "```mermaid\n",
    "\n",
    "flowchart LR\n",
    "    input([input])\n",
    "    output([output])\n",
    "    filesys[(FILE SYSTEM)]\n",
    "    memory_1\n",
    "    memory_2\n",
    "    OAI[\"MLSERVER OAI\"]\n",
    "\n",
    "    input --> memory_1 --> OAI --> output\n",
    "    filesys <--> memory_1\n",
    "    memory_2 --> filesys\n",
    "    OAI --> memory_2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating our directorys in which we will put our model configurations inb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./models\n",
    "!mkdir ./models/memory\n",
    "!mkdir ./models/local-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Memory Component Model\n",
    "\n",
    "We will be creating a `model-settings.json` for our memory component to be used within our chat pipeline.\n",
    "\n",
    "The memory component is used to store the chat history and give the flexibility throughout a Seldon Core v2 pipeline to access the memory of that chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/memory/model-settings.json\n",
    "{\n",
    "    \"name\": \"conversational_memory\",\n",
    "    \"implementation\": \"mlserver_memory.ConversationalMemory\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"database_config\": {\n",
    "                \"database\": \"filesys\"\n",
    "            },\n",
    "            \"memory_config\": {\n",
    "                \"window_size\": 10,\n",
    "                \"tensor_names\": [\"content\", \"role\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to confgiure the LLM model itself.  \n",
    "\n",
    "The Local LLM runtime can support three libraries  \n",
    "\n",
    "- Transformers\n",
    "- vLLM\n",
    "- Deepspeed\n",
    "\n",
    "We will be using the transformers library for this example and include a prompt template.\n",
    "For the model confiugration there will be two files being created `model-settings.json` and `prompt.jinja`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/local-models/model-settings.json\n",
    "{\n",
    "  \"name\": \"gpt2\",\n",
    "  \"implementation\": \"mlserver_llm_local.runtime.Local\",\n",
    "  \"parameters\": {\n",
    "    \"uri\": \"gpt2\",\n",
    "    \"extra\": {\n",
    "      \"backend\": \"transformers\",\n",
    "      \"model\": {\n",
    "        \"enable_profile\": \"False\",\n",
    "        \"device\": \"cpu\"\n",
    "      },\n",
    "      \"prompt\": {\n",
    "        \"uri\": \"./prompt.jinja\",\n",
    "        \"enable\": \"True\",\n",
    "        \"tokens\": {\n",
    "          \"eos_token\": \"<|endoftext|>\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the prompt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/local-models/prompt.jinja\n",
    "{% for message in messages %}\n",
    "  {{ message['content'].strip() }}\n",
    "{% endfor %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model settings are ready to go we will upload them to a google storage bucket to be used Model Deployment configurations in the LLM runtimes of Seldon Core v2\n",
    "\n",
    "In the provided `upload_models.py` file please update line 14 with your name\n",
    "\n",
    "```\n",
    "blob = bucket.blob(f'llm/[YOUR NAME]/chat-memory/{directory}/{file}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"josh-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Uploading memory/model-settings.json\n",
      "Uploading local-model/prompt.jinja\n",
      "Uploading local-model/model-settings.json\n",
      "\n",
      "Error: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Running the script with the name variable as an argument\n",
    "result = subprocess.run(['python', 'upload_models.py', NAME], capture_output=True, text=True)\n",
    "\n",
    "# Print output and error, if any\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Error:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Chat Application pipeline into Seldon Core v2 using the LLM Module\n",
    "\n",
    "It is now time to deploy our models to kubernetes and run our chat application. \n",
    "\n",
    "When deploying use cases into production with Seldon Core v2 there is a two pronged workflow in order to leverage the reusability of deploy machine learning models\n",
    "\n",
    "Step 1: Deploy models\n",
    "Step 2: Deploy Pipelines\n",
    "\n",
    "We will setup the `deployments` directory to keep things organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: deployments: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Deploy Models\n",
    "\n",
    "Deploying LLMs in Seldon is similar to deploy traditional models as well. We create a manifest file containg the model configurations.\n",
    "\n",
    "This manifest is composed of Seldon's `model` CRD.\n",
    "\n",
    "For our use case we will be registering 3 seperate models while reusing the same configuration for both the combine-question and combine-answer models registered in Seldon Core \n",
    "\n",
    "The Pipeline that we will configure looks as below:\n",
    "\n",
    "Step 1: Take input query and memory for context of chat\n",
    "Step 2: Take combined content and process with LLM\n",
    "Step 3: Append LLM output to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create the models manifest to deploy the models into kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "models_manifest = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: combine-question\n",
    "spec:\n",
    "  storageUri: \"gs://josh-seldon/llm/chat-memory/{NAME}/memory\"\n",
    "  requirements:\n",
    "  - memory\n",
    "---\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: combine-answer\n",
    "spec:\n",
    "  storageUri: \"gs://josh-seldon/llm/chat-memory/{NAME}/memory\"\n",
    "  requirements:\n",
    "  - memory\n",
    "---\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: localgpt\n",
    "spec:\n",
    "  storageUri: \"gs://josh-seldon/llm/chat-memory/{NAME}/local-model\"\n",
    "  requirements:\n",
    "  - llm-local\n",
    "\"\"\"\n",
    "\n",
    "# Load multiple documents\n",
    "models = list(yaml.safe_load_all(models_manifest))\n",
    "\n",
    "# Save each document separately in the YAML file\n",
    "with open('deployments/models.yaml', 'w') as file:\n",
    "    yaml.safe_dump_all(models, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Now we apply the Models manifest to the Namespace we will be deploying our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/combine-question created\n",
      "model.mlops.seldon.io/combine-answer created\n",
      "model.mlops.seldon.io/localgpt created\n",
      "model.mlops.seldon.io/combine-answer condition met\n",
      "model.mlops.seldon.io/combine-question condition met\n",
      "model.mlops.seldon.io/localgpt condition met\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployments/models.yaml -n seldon\n",
    "!kubectl wait --for condition=ready --timeout=300s model --all -n seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io \"combine-question\" deleted\n",
      "model.mlops.seldon.io \"combine-answer\" deleted\n",
      "model.mlops.seldon.io \"localgpt\" deleted\n",
      "error: no matching resources found\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f deployments/models.yaml -n seldon\n",
    "!kubectl wait --for condition=ready --timeout=300s model --all -n seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add test prediction to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create the Seldon Core Pipeline to tie everything together for our chat application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "pipeline_manifest = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: local-chat-memory\n",
    "spec:\n",
    "  steps:\n",
    "    - name: combine-question\n",
    "      inputs:\n",
    "      - local-chat-memory.inputs.memory_id\n",
    "      - local-chat-memory.inputs.role\n",
    "      - local-chat-memory.inputs.content\n",
    "    - name: localgpt\n",
    "      inputs:\n",
    "      - combine-question.outputs.role\n",
    "      - combine-question.outputs.content\n",
    "    - name: combine-answer\n",
    "      inputs:\n",
    "      - local-chat-memory.inputs.memory_id\n",
    "      - localgpt.outputs.role\n",
    "      - localgpt.outputs.content\n",
    "  output:\n",
    "    steps:\n",
    "    - localgpt\n",
    "\"\"\"\n",
    "\n",
    "# Load multiple documents\n",
    "pipeline = list(yaml.safe_load_all(pipeline_manifest))\n",
    "\n",
    "# Save each document separately in the YAML file\n",
    "with open('deployments/pipeline.yaml', 'w') as file:\n",
    "    yaml.safe_dump_all(pipeline, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.mlops.seldon.io/local-chat-memory created\n",
      "pipeline.mlops.seldon.io/local-chat-memory condition met\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployments/pipeline.yaml -n seldon\n",
    "!kubectl wait --for condition=ready --timeout=300s pipeline --all -n seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delivery-window",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
